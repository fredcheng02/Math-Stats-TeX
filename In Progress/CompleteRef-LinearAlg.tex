\documentclass[10pt]{article}
\usepackage[T1]{fontenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{tikz-cd}
\usepackage{enumitem}
\usepackage{setspace,microtype}
\usepackage{soul,framed}
\usepackage[dvipsnames]{xcolor}
\usepackage[pdfusetitle,bookmarks,bookmarksnumbered,bookmarksopen,bookmarksopenlevel=1,colorlinks,
citecolor=CadetBlue,urlcolor=CadetBlue,linkcolor=RedOrange]{hyperref}
\usepackage[capitalize]{cleveref}
\usepackage[all]{hypcap}

\usepackage{titlesec}
\titleformat*{\section}
{\sffamily\Large}
\titleformat*{\subsection}{\sffamily\large}

% no parentheses
\newtheoremstyle{plain-star}{\topsep}{\topsep}{}{}{\sffamily}{.}{5pt plus 1pt minus 1pt}{\thmnumber{#2 }\thmname{#1}\thmnote{ #3}}

\newtheoremstyle{definition-star}{\topsep}{\topsep}{}{}{\sffamily}{.}{5pt plus 1pt minus 1pt}{\thmnumber{#2 }\thmname{#1}\thmnote{ (#3)}}

\newtheoremstyle{remark-star}{.5\topsep}{.5\topsep}{}{}{\itshape\sffamily}{.}{5pt plus 1pt minus 1pt}{\thmnumber{#2 }\thmname{#1}\thmnote{ (#3)}}

\numberwithin{equation}{section}
\theoremstyle{plain-star}
\newtheorem{thm}[equation]{Theorem}
\newtheorem*{thm*}{Theorem}
\newtheorem{prop}[equation]{Proposition}
% \newtheorem*{prop*}[equation]{Proposition}
\newtheorem{fact}[equation]{Fact}
\newtheorem{lem}[equation]{Lemma}
\newtheorem{cor}[equation]{Corollary}
\theoremstyle{definition-star}
\newtheorem{defn}[equation]{Definition}
\theoremstyle{remark-star}
\newtheorem{rem}[equation]{Remark}
\newtheorem*{rem*}{Remark}

\newenvironment{sketch}[1][Sketch]{\begin{proof}[#1]\renewcommand*{\qedsymbol}{$\triangle$}}{\end{proof}}

\makeatletter
\newcommand\thmsname{Theorem}
\newcommand\nm@thmtype{theorem}
\theoremstyle{plain-star}
\newtheorem{namedtheorem}[equation]{\thmsname}
\newenvironment{namedthm}[1][Undefined Theorem Name]{
    \ifx{#1}{Undefined Theorem Name}
    \renewcommand\nm@thmtype{theorem}
    \else\renewcommand\thmsname{#1}
    \renewcommand\nm@thmtype{namedtheorem}
    \fi
    \begin{\nm@thmtype}\def\@currentlabelname{#1}}
    {\end{\nm@thmtype}}

\newtheorem*{namedtheorem*}{\thmsname}
\newenvironment{namedthm*}[1][Undefined Theorem Name]{
    \ifx{#1}{Undefined Theorem Name} \renewcommand\nm@thmtype{theorem*}
    \else\renewcommand\thmsname{#1}
    \renewcommand\nm@thmtype{namedtheorem*}
    \fi
    \begin{\nm@thmtype}}
    {\end{\nm@thmtype}}
\makeatother

% \renewcommand{\arraystretch}{1.2}

\setlength{\parskip}{0em} % default parskip
\setlist{listparindent=\parindent,parsep=0pt} % indentation and separation between list paragraphs
\setenumerate[1]{label=(\alph*)}

\makeatletter
% \onehalfspacing
\usepackage{xpatch}
\xpatchcmd{\env@cases}{1.2}{1.2}{}{}
\delimiterfactor=801
\xpatchcmd{\proof}{\itshape}{\normalfont\proofnamefont}{}{}
\newcommand{\proofnamefont}{\itshape\sffamily}

\let\@subtitle\@empty % default value
\protected\def\subtitle#1{\gdef\@subtitle{#1}}
\def\@maketitle{%
  \newpage
  \begin{center}%
  \let \footnote \thanks
    {\Large\sffamily \@title \par}% % LARGE
    {\sffamily \@subtitle \par}% % large
    \vskip 0.5em%
    {\lineskip .5em%
      \begin{tabular}[t]{c}%
        \sffamily \@author
      \end{tabular}\par}%
    {\sffamily \@date}%
    \vskip -0.5em%
  \end{center}%
  \par}
\makeatother

\newcommand{\R}{\mathbf{R}}
\newcommand{\C}{\mathbf{C}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\F}{\mathbf{F}}
\newcommand{\Mat}{\operatorname{Mat}}
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}
\newcommand{\s}{\operatorname{span}}
\newcommand{\nul}{\operatorname{null}}
\newcommand{\rng}{\operatorname{range}}
\newcommand{\nullity}{\operatorname{nullity}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\trp}{\mathrm{t}}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\inp}[2]{\langle #1, #2 \rangle}
\newcommand{\nm}[1]{\lVert #1 \rVert}
\newcommand{\abs}[1]{\lvert #1 \rvert}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\LV}{\mathcal{L}(V)}
\newcommand{\LVW}{\mathcal{L}(V,W)}
\newcommand{\M}{\mathcal{M}}
\newcommand{\PF}{\mathcal{P}(\F)}
\newcommand{\blank}{\,\cdot\,}
\renewcommand{\phi}{\varphi}
\renewcommand{\implies}{\Rightarrow}
\renewcommand{\impliedby}{\Leftarrow}

\newcommand{\df}[1]{\textit{#1}}

\usepackage[style=alphabetic,doi=false,url=false,isbn=false]{biblatex}
\addbibresource{CRLA.bib}
\renewbibmacro{in:}{}

% disable automatic page designation
\DeclareFieldFormat{postnote}{#1}


\title{Complete Reference on Linear Algebra}
\subtitle{Comments on \textit{Linear Algebra Done Right, 4th Edition}\thanks{The order of exposition will be based on \cite{axler2024}, but we will incorporate materials from other linear algebra texts, including the famous \cite{friedberg2003linear}. This note assumes familiarity with basic results and definitions in linear algebra. It aims to provide reference for applications mostly in statistics, and sometimes in algebra, topology, functional analysis, and probability.}}
\author{Feng Cheng}
\date{}

\begin{document}
\maketitle

\begin{rem*}
    In Sections 1 to 5 below, we assume by default that $V$ is a finite-dimensional vector space (FDVS) over $\F$ ($\R$ or $\C$).
\end{rem*}

\section{Vector Spaces}
The corresponding Chapter 1 in LADR deals with vector spaces and subspaces. Sums and direct sums of subspaces are introduced.

\section{Finite-Dimensional Vector Spaces}
The corresponding Chapter 2 in LADR discusses spanning lists, linear dependence and independence, bases, and dimensions.

Axler works with lists (i.e., ordered sets) instead of sets throughout. This is mainly for consistency purpose: the matrix representation of a linear map ultimately requires an order on the basis. Moreover it makes proving the relations between spanning sets, linearly independent sets, and bases more straightforward, with the following important lemma: 

\begin{namedthm}[Linear dependence lemma {\cite[2.19]{axler2024}}]
    Suppose $v_1,\dotsc,v_m$ is a linearly dependent list in $V$. Then there exists $k \in [m]$ such that \[
        v_k \in \s(v_1,\dotsc,v_{k-1}).
    \] Furthermore removing the $v_k$ does not change the span of the list.
\end{namedthm}

The next selected result from this chapter shows the existence of a complement for any subspace of a FDVS. 
\begin{thm}[{\cite[2.33]{axler2024}}] \label{thm:exist-comp}
    Let $V$ be a FDVS and $U$ be its subspace, then there exists a subspace $W$ of $V$ such that $U\oplus W = V$.
\end{thm}
Such $W$ is called a \df{complement} for $U$. We can work with finite bases in FDVS, and this is how the theorem above is proved. The analog of finite basis in infinite dimensional vector spaces is the Hamel basis,\footnote{The existence of Hamel basis in an infinite-dimensional vector space is ensured by Zorn's lemma.} which is a purely algebraic concept. In general for a vector space $V$ and its subspace $U$, $W$ is called the \df{algebraic complement} of $V$ if \[
    U + W = V \quad \text{and} \quad U \cap W = \{0\}.
\] We denote this by $U \oplus W = V$.

The generalization of \cref{thm:exist-comp} says that every vector subspace of a general vector space has an algebraic complement. This can be proved by Zorn's lemma: one consider the poset of all subspaces $W_j$ of $V$ such that $U \cap W_j = {0}$, ordered by inclusion. It has a maximal element $W$, which one can show by contradiction that $U + W = V$.

This leads us to a natural digression to infinite-dimensional topological vector spaces, where we have the related notion of \emph{topologically complementary spaces}. In particular, we consider the case where $X$ is a Banach space, and $M$ and $N$ are two \emph{closed} subspaces of $X$. (The topological constraint certainly makes the question more interesting.) We have the following nice result: 
\begin{thm*}[{\cite[Theorem~13.1]{conway2007func}}]
    If $M$ and $N$ are algebraically complementary in $X$, then they are topologically complementary.
\end{thm*}
And hence there is no distinction between the two definitions when the space is Banach.

It turns out that if $M$ is of finite dimension or if $X/M$ is of finite dimension, then $M$ can be complemented. However, not every closed subspace $M$ has a complement.

Moving on to Section 2C, we emphasize two results concerning the dimension of a vector space. The first one is a neat tool sometimes useful in proofs: 
\begin{prop}[{\cite[2.39]{axler2024}}]
    Subspace of the full dimension equals the whole space.
\end{prop}
The second one is about the dimension of the sum of two subspaces: 
\begin{thm}[{\cite[2.43]{axler2024}}] \label{thm:sum-dim-two-spaces}
    Let $V_1$ and $V_2$ be two subspaces of an FDVS, then \[
        \dim(V_1+V_2) = \dim(V_1) + \dim(V_2) - \dim(V_1 \cap V_2).
    \]
\end{thm}
Despite the resemblance between the sum of vector subspaces and the union of two sets, they are ultimately distinct notions. \Cref{thm:sum-dim-two-spaces} does not generalize, in the sense of inclusion-exclusion formula, to a sum of $3$ subspaces.

At last we state a direct sum result from Section 3E, where we introduce product spaces.
\begin{prop}[{\cite[3.94]{axler2024}}]
    A sum of subspaces is a direct sum if and only if their dimensions adds up to the dimension of the whole space.
\end{prop}
\begin{sketch}
    This is a consequence of two results. First, \[
        \dim(V_1 \times \dotsb \times V_m) = \sum_{j=1}^m \dim V_j.
    \] Second, the linear map $\Gamma\colon V_1 \times \dotsb \times V_m \to V_1 + \dotsb + V_m$ that adds up the components is injective if and only if the image of $\Gamma$ is a direct sum.
\end{sketch}

\section{Linear Maps}
\begin{rem*}
    To show a map $T\colon V\to W$ is linear, it suffices to show that for any $c \in \F$ and $v_1,v_2\in V$, \[T(cv_1 + v_2) = cTv_1 + Tv_2.\]
\end{rem*}

\begin{thm}
    A linear map $T \in \LVW$ is uniquely determined by its action on any chosen basis of $V$.
\end{thm}

\begin{namedthm}[Rank-nullity theorem] \label{thm:rank-nullity}
    Let $V$ be finite-dimensional and $T\in \LVW$, then $\rng T$ is finite-dimensional and \[
        \dim V = \nullity T + \rank T.
    \] (Recall $\nullity$ is the dimension of the nullspace, and $\rank$ is the dimension of the range of a linear map.)
\end{namedthm}
\begin{sketch}
    Choose a basis for $\nul T$ and extend it to a basis of $V$. The length of the extension should correspond to $\dim \rng T$.
\end{sketch}

Some key facts about matrix ranks have been omitted from Axler's text. We mention them here.
\begin{defn}
    For a matrix $A$, the \df{column rank} of $A$ is defined to be the number of linearly independent columns of $A$, i.e., the dimension of the column space of $A$. Define the \df{row rank} of $A$ similarly to be the number of linearly independent rows of $A$.
\end{defn}
It is clear that the column rank of $A$ we defined above is exactly the $\rank$ of left-multiplication linear map of the matrix $A$ (from $\F^n$ to $\F^m$). We will show below the important fact that \[
    \text{column rank} = \text{row rank},
\] and hence the rank of a linear map and the rank of a matrix are the same concept.

\begin{prop}[{\cite[3.56]{axler2024}}]
    Let $A \in \Mat_{m\times n}(\F)$ with column rank $c$. Then $A = CR$, where $C\in \Mat_{m\times c}(\F)$ and $R\in \Mat_{c\times n}(\F)$.
\end{prop}
\begin{proof}
    Take out $c$ linearly independent columns from $A$ and we get a $m$-by-$c$ matrix $C$. Each column $k$ of $A$ is a (unique) linear combination of the columns of $C$, and we may define the matrix $R$ such that its $k$\textsuperscript{th} column corresponds to the $c$ coefficients in the linear combination.
\end{proof}
\begin{cor}\label{cor:rank-result}
    The column rank of $A$ is equal to the row rank of $A$. Therefore we may simply refer to both as the \df{rank} of $A$.
\end{cor}
\begin{proof}
    Each row of $A = CR$ is a linear combination of the $c$ rows of $R$. Therefore the row rank of $A$ is $\leq c$.

    To prove the opposite direction, apply the above to the transpose of $A$ :
    \begin{align*}
        c & = \text{row rank of } A^\trp \\
        & \leq \text{column rank of } A^\trp \\
        & = \text{row rank of } A. \qedhere
    \end{align*}
\end{proof}

See \cref{sec:elem-mat-op} for materials on the elementary matrix operations, which gives an alternative proof to \cref{cor:rank-result}. That proof also requires knowledge on invertible matrices, and therefore the proof above by Axler is a bit more elementary. Axler provides a second proof of \cref{cor:rank-result} at the very end of Section 3F (using dual spaces and annihilators), and a third proof in the Exercises of Section 7A (using adjoint operators).

For completeness we state a result from \cref{sec:elem-mat-op} here.

\begin{prop}
    For $A \in \Mat_{m\times n}(\F)$, $P \in \Mat_{m \times m}(\F)$, and $Q \in \Mat_{n \times n}(\F)$, where $P$ and $Q$ are invertible, we have \[
        \rank PA = \rank A = \rank AQ.
    \]
\end{prop}

Now we move onto invertible linear maps.
\begin{thm}
    Let $\dim V = n$. A linear operator $T \in \LV$ is injective if and only if it is surjective if and only if it is bijective. (This is in general true for any linear map with the same dimension for its domain and codomain.)

    If $T$ is the left-multiplication map of a square matrix $A$ in addition, then the above is equivalent to $A$ is of full rank (by the definition of the rank of a matrix).
\end{thm}

This theorem directly implies that when solving a system of $n$ linear equations with $n$ unknowns $Ax = b$, having only the trivial solution when $b = 0$ (injective) is equivalent to having a unique solution for all possible $b$'s.

\begin{prop}
    Two vector spaces are linearly isomorphic if and only if they have the same dimension.
\end{prop}

\begin{thm}
    Consider the space of linear maps $\LVW$, where we fix the basis $v_1,\dotsc,v_n$ of $V$ and $w_1,\dotsc,w_m$ of $W$. Then the matrix representation of $\mathcal{M}\colon \LVW \to \Mat_{m\times n}(\F)$ is a bijection.
\end{thm}

\begin{namedthm}[Change of basis]
    Let $T \in \LV$, and $u_1,\dotsc,u_n$ and $v_1,\dotsc,v_n$ be two bases of $V$. If we let \[
        A = \mathcal{M}\bigl(T,(u_1,\dotsc,u_n)\bigr), \quad
        B = \mathcal{M}\bigl(T,(v_1,\dotsc,v_n)\bigr), \] and
    \[
        C = \mathcal{M}\bigl(T,(v_1,\dotsc,v_n),(u_1,\dotsc,u_n)\bigr),
    \] then $A = CBC^{-1}$.
\end{namedthm}

For $V$ and its subspace $U$, with the \nameref{thm:rank-nullity} on the quotient map, we obtain \[
    \dim V/U = \dim V - \dim U.
\]
And of course we have the first isomorphism theorem for vector spaces. We have $V/\nul T \cong \rng T$ in the diagram below. \[
            \begin{tikzcd}
                V \arrow[rr,"T"] \arrow[dr,"\pi"] && W \\
                & {V/\nul T} \arrow[ur,"\widetilde{T}"]
            \end{tikzcd}
        \]

The dual space (space of linear functionals) of a vector space $V$ is easy to characterize when $V$ is finite-dimensional.

First we have the following result: 
\begin{thm}
    If $X$ is a finite-dimensional normed vector space and $Y$ is a normed vector space, then every linear map $T\colon X\to Y$ is bounded.
\end{thm}
\begin{proof}
    Let $v_1,v_2,\dotsc,v_n$ be a basis of $X$. Then we have when $
        x = a_1v_1+\dotsb+a_nv_n$, \[
        \nm{Tx}_Y\leq \sum_{k=1}^n \abs{a_k} \nm{Tv_k}_Y.
    \] Note $\sum_{k=1}^n \abs{a_k}$ is a norm for every $x \in X$, which we write as $\nm{x}_1$. Let $M = \max_{1\leq k \leq n} \nm{Tv_k}_Y$, then \[
        \nm{Tx}_Y \leq M\nm{x}_1.
    \] Now invoke the fact that all norms are equivalent in a finite-dimensional space.
\end{proof}
\begin{fact}
    $\dim V^* = \dim V \cdot \dim \F = \dim V$.
\end{fact}
\begin{thm}[(dual basis)]
    Fix a basis $v_1,\dotsc,v_n$ of $V$. Consider the list of elements $\phi_1,\dotsc,\phi_n$ in the dual space $V^*$ given by \[
        \phi_j(v_k) = \begin{cases}
            1 & \text{if } k = j, \\
            0 & \text{if } k \neq j.
        \end{cases}
    \]
\end{thm}

\begin{defn}
    For $T \in \LVW$, the \df{adjoint/transpose/dual map} of the $T$ is the map $T'\colon W^* \to V^*$ such that \[
        T'\phi(v) = (\phi \circ T)(v).
    \]
\end{defn}

The adjoint of a linear map is also linear. If $X$ and $Y$ are normed vector spaces and $T$ is bounded, then furthermore $\nm{T'}_{\mathcal{L}(W',V')} = \nm{T}_{\LVW}$.

The adjoint of above is related the (Hermitian) adjoint in inner product spaces via the Riesz representation theorem.

The double dual \emph{hat map} is very important in the study of functional analysis.

\section{Inner Product Spaces}
\begin{thm} Let $P\in \LV$.
    \begin{enumerate}
        \item Then $P$ is an orthogonal projection onto its range if and only if it is self-adjoint and idempotent. (Same argument applies in the Hilbert space setting.)
        \item Let $P$ be idempotent and $\dim V = n$. Then $P$ have exactly $n$ eigenvalues of $0$'s and $1$'s counting multiplicity, where the number of $1$'s corresponds to the rank of $P$. If $P$ is an orthogonal projection onto the subspace $U$, then the number of $1$'s is exactly $\dim U$.
    \end{enumerate}
\end{thm}

\begin{proof}\leavevmode
    \begin{enumerate}
        \item ($\implies$) Say $P=P_U$. For any $v\in V$, we have $Pv = u$ for some $u \in U$. We know $Pu = u$, and thus \[P^2v = P(Pv) = Pu = u = Pv.\] To show $P$ is idempotent, we need to prove that for any $v,w\in V$, \[
            \inp{Pv}{w} = \inp{v}{Pw}.
        \]
        Orthogonal decomposition gives us $v = v_1 + v_2$ and $w = w_1 + w_2$, where $v_1, w_1 \in U$ and $v_2, w_2 \in U^\perp$. Therefore \[
            \inp{Pv}{w} = \inp{v_1}{w_1+w_2} = \inp{v_1}{w_1},
        \] and meanwhile \[
            \inp{v}{Pw} = \inp{v_1+v_2}{w_1} = \inp{v_1}{w_1}.
        \] This finishes the proof in the ($\implies$) direction.

        ($\impliedby$) It suffices to show that $\inp{Pv}{v - Pv} = 0$: \begin{align*}
            \inp{Pv}{v - Pv} & = \inp{Pv}{v} - \inp{Pv}{Pv} \\ 
            & = \inp{Pv}{v} - \inp{P^*Pv}{v} \\
            & = \inp{Pv - Pv}{v} = 0.
        \end{align*}
        \item  For every $v\in V$ we have $P^2 v = Pv$, which means that $Pv \in E(1,P)$. Also we have $P(v - Pv) = 0$, which implies that $v - Pv \in E(0,P)$. Since \[
            v = Pv + v - Pv
        \] for any $v\in V$, we have \[
            V = E(1,P) + E(0,P),
        \] showing that $P$ is diagonalizable with eigenvalues $0$ and $1$. Note $\rng P = E(1,P)$, and hence their dimensions are the same. \qedhere
    \end{enumerate}
\end{proof}

\begin{defn}
    For a self-adjoint operator $T \in \LV$, its \df{Rayleigh quotient} for $v \in V - \{0\}$ is the scalar \[R(v) = \frac{\inp{Tv}{v}}{\nm{v}^2}.\]
\end{defn}

\begin{prop}\label{prop:rayleigh-eigen}
    For a self-adjoint operator $T \in \LV$, where $\dim V = n$, it holds that \[
        \max_{v \neq 0} R(v) = \lambda_1(T)\quad \text{and} \quad \min_{v \neq 0} R(v) = \lambda_n(T).
    \]
\end{prop}

\begin{proof}
    Without loss of generality we may restrict $v$ to be a unit vector (because scaling in norm does not change the Rayleigh quotient). According to the spectral theorems, we may let $e_1,\dotsc,e_n$ be the orthonormal basis of eigenvectors corresponding to the (real) eigenvalues $\lambda_1\geq\dotsb\geq\lambda_n$. Thus \[
        v = a_1 e_1 + \dotsb + a_n e_n
    \] for some scalars $a_1,\dotsc,a_n$. 
    \begin{align} \begin{split} \label{eq:rayleigh-expansion}
        R(v) & = \biggl\la \sum_{j=1}^n a_jTe_j,\sum_{j=1}^n a_je_j\biggr\ra \\
        & \leq \lambda_1 \biggl\la \sum_{j=1}^n a_je_j, \sum_{j=1}^n a_je_j\biggr\ra = \lambda_1 \nm{v}^2 = \lambda_1. \end{split}
    \end{align} 
    The other half is proved in the same way.
\end{proof}

\begin{cor}[{\cite[Exercise~7E.4]{axler2024}}]\label{cor:rayleigh-sing}
    For $T \in \LVW$, where $\dim V = n$, it holds that \[
        \max\{\nm{Tv}:\nm{v} = 1\} = \sigma_1(T)\quad \text{and}\quad \min\{\nm{Tv}:\nm{v} = 1\} = \sigma_n(T).
    \]
\end{cor}
\begin{proof}
    By definition, the singular values of $T$ are the nonnegative square roots of the eigenvalues of $T^*T$ (which is self-adjoint on $V$).
\end{proof}
The first part of the corollary above shows that the operator norm of $T$ is attained and is exactly the singular value of $T$. In particular the maximum is attained when $v$ is a left singular vector corresponding to $\sigma_1$. Alternatively Axler proved this with SVD and Bessel's inequality (see \cite[Section 7F]{axler2024}).

The generalization of \cref{prop:rayleigh-eigen} and \cref{cor:rayleigh-sing} is the theorem below.

\begin{namedthm}[Min-max theorem]\leavevmode
    \begin{enumerate}
        \item \label{enu:min-max-self-adjoint} For a self-adjoint operator $T \in \LV$, where $\dim V = n$, we have 
        \begin{align}
            \lambda_j(T) & = \max_{\dim M = j}\min_{v \in M-\{0\}} R(v) \label{eq:max-min}\\ & = \min_{\dim N = n - j + 1}\max_{v \in N-\{0\}} R(v). \label{eq:min-max}
        \end{align}
        \item \label{enu:min-max-singular} For $T \in \LVW$, where $\dim V = n$, we have \begin{align*}
            \sigma_j(T) & = \max_{\dim M = j}\min_{v \in S(M)}\nm{Tv} \\ & = \min_{\dim N = n - j + 1}\max_{v \in S(N)}\nm{Tv},
        \end{align*}
        where $S(M)$ is the set of unit vectors in the subspace $M$.
    \end{enumerate}
\end{namedthm}
\begin{proof}
    Part~\ref{enu:min-max-singular} is the consequence of part~\ref{enu:min-max-self-adjoint} as we have seen in \cref{cor:rayleigh-sing}. Now we prove \eqref{eq:max-min}. The proof idea is the same as before.
    
    First we show that for any subspace $M$ of dimension $j$, there exists a unit vector $v \in M$ such that $\inp{Tv}{v} \leq \lambda_j$. Again let $e_1, \dotsc, e_n$ be the orthonormal basis of eigenvectors corresponding to the eigenvalues $\lambda_1\geq \dotsb \geq \lambda_n$. Consider $N = \s(e_j,\dotsc,e_n)$, which is of dimension of $n-j+1$. This implies that we can choose a unit vector $v \in M \cap N$. We then have \[
        v = a_j e_j + \dotsb + a_n e_n
    \] for some scalars $a_j,\dotsc,a_n$. We can get $R(v)\leq \lambda_j$, as in \eqref{eq:rayleigh-expansion}.

    Next we need to show that for some subspace $M$ of dimension $j$, the equality in \[
        \lambda_j \geq \max_{\dim M = j}\min_{v \in M-\{0\}} R(v)
    \] can be achieved. Let $M=\s(e_1,\dotsc,e_j)$, and choose $v = e_j$ gives us $R(v) = \inp{Te_j}{e_j} = \lambda_j$. This concludes our proof for \eqref{eq:max-min}.

   Now instead let $N$ be any subspace of dimension $n-j+1$. Since there is a unit vector $v \in N$ such that $\inp{Tv}{v}\leq \lambda_{n-j+1}$, we know that \[\inp{-Tv}{v} \leq -\lambda_j,\] which is now the $(n-j+1)$\textsuperscript{th} largest eigenvalue for the operator $-T$. Therefore \[
        \inp{Tv}{v} \geq \lambda_j,
   \] which implies that \[
        \lambda_j \leq \min_{\dim N = n-j+1} \max_{v \in N-\{0\}} R(v).
   \] Here the equality can be achieved when $N = \s(e_j,\dotsc,e_n)$ and $v = e_j$.
\end{proof}

For a self-adjoint operator, $\sigma_j = \abs{\lambda_j}$. If the operator is furthermore positive semidefinite, then $\sigma_j = \lambda_j$.

\newpage
\appendix
\section{Elementary Matrix Operations}\label{sec:elem-mat-op}
\begin{rem*}
    Most of the material here is adapted from \cite[Chapter 3]{friedberg2003linear}.
\end{rem*}

For an $m$-by-$n$ matrix $A$, an \df{elementary row/column operation} on $A$ if we are performing one of the following three: 
\begin{description}[font=\sffamily]
    \item[Type 1] interchanging any two rows/columns of $A$; 
    \item[Type 2] multiplying any row/column of $A$ by a nonzero scalar; 
    \item[Type 3] adding any scalar multiple of a row/column of A to another row/column.
\end{description}

An elementary matrix (of type 1/2/3) is obtained from the identity matrix by performing the corresponding elementary matrix operation (of type 1/2/3).

It is easy to see that each elementary row/column operation can be represented uniquely by the left/right-multiplication by a matrix. This matrix is obtained by performing the same elementary row/column matrix on the identity matrix. The corollary of this is that every elementary matrix is invertible, and its inverse is an elementary matrix of the same type.

The first goal of this section is to give an alternative proof of \cref{cor:rank-result}. We need to two results. The first result says that the column rank of a matrix is invariant under left/right-multiplication by invertible matrices.
\begin{prop}\label{prop:rank-same}
    For $A \in \Mat_{m\times n}(\F)$, $P \in \Mat_{m \times m}(\F)$, and $Q \in \Mat_{n \times n}(\F)$, where $P$ and $Q$ are invertible, we have \[
        \text{column rank of } PA = \text{column rank of } A = \text{column rank of } AQ.
    \]
\end{prop}
\begin{proof}
    A simple exercise using properties of linear isomorphisms.
\end{proof}

The second result is an important theoretical fact that is a little tedious to prove.
\begin{thm}[{\cite[Theorem 3.6]{friedberg2003linear}}]\label{thm:reduced-form}
    Let $A \in \Mat_{m\times n}(\F)$ be of column rank $r$. Then by means of a finite number of elementary row and column operations, $A$ can be reduced to the form \[
        D = \begin{bmatrix}
            I_r & O_1 \\
            O_2 & O_3
        \end{bmatrix}.
    \]
\end{thm}
It follows easily that \[D = BAC,\] where $B$ and $C$ are two invertible matrices of dimension $m$ and $n$ respectively. We then have \[
    D^\trp = C^\trp A^\trp B^\trp,
\] where $C^\trp$ and $B^\trp$ are still invertible. Therefore by \cref{prop:rank-same}, we have \begin{align*}
    r & = \text{column/row rank of } D \\
    & = \text{column rank of } D^\trp \\
    & = \text{column rank of } A^\trp \\
    & = \text{row rank of } A,
\end{align*}
as desired.

The column rank in \cref{prop:rank-same} and \cref{thm:reduced-form} can now be replaced by rank.

\begin{cor}
    Every invertible matrix is a product of elementary matrices.
\end{cor}
This fact is the key to proving the multivariate change of variables integration formula in real analysis. We reduces $C^1$ diffeomorphisms to invertible linear maps, which is further reduced to elementary matrices. Recall $\det (E_1 E_2 \dotsm E_m) = \prod_{j=1}^m \det E_j$. (Alternatively one may use polar decomposition to prove this result.)

Note that the above corollary also justifies that the Gaussian elimination method is always successful in finding the matrix inverse. For an invertible $n$-by-$n$ matrix $A$, we consider the augmented matrix $[A \,\vert\, I_n]$. Essentially \[
    A^{-1}[A \,\vert\, I_n] = [I_n \,\vert\, A^{-1}],
\] and by the corollary above, $A^{-1}$ can be broken into a product of elementary matrices, each corresponding to an elementary row operation.

Further materials on solving systems of linear equations can be found in \cite[Sections 3.3 and 3.4]{friedberg2003linear}.

\section{Special Matrices and Norms}

\begin{defn}
    On the vector space of matrices $\Mat_{m\times n}(\F)$, we can define a canonical inner product by \[
        \inp{A}{B} = \tr (B^*A) = \sum_{j=1}^m\sum_{k=1}^n a_{jk}\ol{b_{jk}}.
    \] We call its associated norm the \df{Frobenius/Hilbert-Schmidt norm}: \[
        \nm{A}_{\mathrm F} = \sqrt{\tr(A^*A)} = \sqrt{\sum_{j=1}^m\sum_{k=1}^n \abs{a_{jk}}^2}.
    \] Since \[\tr (A^*A) = \sum_{k=1}^n \lambda_j(A^*A) = \sum_{k=1}^n \sigma_k(A)^2,\] we have in addition that \[
        \nm{A}_{\mathrm{F}} = \sqrt{\sum_{k=1}^n \sigma_k(A)^2}.
    \] If we let $s = (\sigma_1,\dotsc,\sigma_n)$, notice that $\nm{s}_2 = \nm{A}_\mathrm{F}$, while $\nm{s}_\infty = \nm{A}_\mathrm{op}$.
\end{defn}

\begin{namedthm}[Eckart-Young-Mirsky theorem (operator norm)]
    
\end{namedthm}

\begin{namedthm}[Eckart-Young-Mirsky theorem (Frobenius norm)]
    
\end{namedthm}

\begin{prop}\label{prop:dual-norm}
    For conjugate exponents $q$ and $p$, where $1 \leq q \leq \infty$, we have for $x \in \F^d$, \[
        \nm{x}_q = \sup\{\inp{x}{y}: \nm{y}_p = 1\}.
    \]
\end{prop}
\begin{proof}
    This is a special $\ell^q$ case of \cite[Proposition 6.13]{folland1999}. Note that $\inp{x}{y} = \sum_{j=1}^n x_j \ol{y_j}$.
\end{proof}

\begin{fact}
    The operator norm of a matrix $A\in \Mat_{m\times n}(\F)$ \[
        \nm{A}_{\mathrm{op}} = \max_{x \in S^{n-1}} \nm{Ax}_2 = \max_{x \in S^{n-1}, y \in S^{m-1}} \inp{Ax}{y}.\footnote{The order of two supremums can be interchanged, and hence the $\max_{x,y}$ makes sense.}
    \]
\end{fact}
\begin{proof}
    The second equality follows from the equality condition for Cauchy-Schwarz inequality (or from \cref{prop:dual-norm}): \[
        \max_{y \in S^{m-1}}\inp{Ax}{y} = \nm{Ax}_2\nm{y}_2 = \nm{Ax}_2.\qedhere
    \]
\end{proof}


\section{Operators on Real Vector Spaces}

\newpage
\printbibliography
\end{document}