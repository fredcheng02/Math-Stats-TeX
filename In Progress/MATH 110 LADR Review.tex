\documentclass[11pt]{article}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,tikz-cd,mathtools}
\usepackage{enumitem}
\usepackage[margin=1in]{geometry}
\usepackage{setspace,microtype,soul}
\usepackage[pdfusetitle,bookmarksnumbered]{hyperref}
\newcommand{\lk}[2]{\hyperlink{subsection.#1.#2}{\S#1.#2}}
\newcommand{\rmk}{\noindent\textit{Remark. }}
\newcommand{\where}{\,|\,}
\newcommand{\df}[1]{\textit{\textsf{#1}}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\C}{\mathbf{C}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\F}{\mathbf{F}}
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}
\newcommand{\s}{\operatorname{span}}
\newcommand{\n}{\operatorname{null}}
\renewcommand{\r}{\operatorname{range}}
\renewcommand{\d}{\dim}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\inp}[2]{\langle #1, #2 \rangle}
\newcommand{\nm}[1]{\lVert #1 \rVert}
\newcommand{\abs}[1]{\lvert #1 \rvert}
\newcommand{\tr}{\operatorname{trace}}
\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\dx}{\,dx}
\newcommand{\LV}{\mathcal{L}(V)}
\newcommand{\LVW}{\mathcal{L}(V,W)}
\newcommand{\M}{\mathcal{M}}
\newcommand{\PF}{\mathcal{P}(\F)}
\newcommand{\bv}{v_1,\dots,v_n}
\newcommand{\bw}{w_1,\dots,w_n}
\renewcommand{\phi}{\varphi}

\makeatletter
\let\@subtitle\@empty % default value
\protected\def\subtitle#1{\gdef\@subtitle{#1}}
\def\@maketitle{%
  \newpage
  \begin{center}%
  \let \footnote \thanks
    {\Large \@title \par}%
    {\@subtitle \par}%
    \vskip 0.5em%
    {\lineskip .5em%
      \begin{tabular}[t]{c}%
        \@author
      \end{tabular}\par}%
    {\large \@date}%
    \vskip -0.5em%
  \end{center}%
  \par}
\makeatother

\title{MATH 110 Notes}
\author{Cheng, Feng}
\subtitle{based on \textit{Linear Algebra Done Right}, third edition}
\date{}

\onehalfspacing
\setlength{\parskip}{0em}
\setlist{listparindent=\parindent,parsep=0pt}

\begin{document}
\maketitle

\tableofcontents
\newpage

\section{Vector Spaces}
\begin{itemize}
    \item A \df{vector space} $V$ over a field $\F$
    \begin{enumerate}
    \item [(1)] is an abelian group under addition;
    \item [(2)] has the multiplicative identity; and
    \item [(3)] has the distributive and associative properties in scalar multiplication.
    \end{enumerate}
    (Addition and scalar multiplication must be closed in $V$.)
    \item $\F^S$ denotes the set of functions from $S$ to $\F$. It is a vector space over $\F$ under the sum and scalar multiplication of functions defined in the usual pointwise way.
    \item A vector space holds the cancellation law and has a unique additive identity and a unique additive inverse because it is a group. $0v = 0$, $a0 = 0$, and $(-1)v = -v$ hold by the distributive property.
    \item A subset $U$ of the whole vector space $V$ that is still a vector space is called a vector subspace. Usually we use the three criteria i) $0_V \in U$ (or $U$ is nonempty), ii) closed under addition in $U$, and iii) closed under scalar multiplication in $U$ to verify. To show the criteria are equivalent to the definition, in one direction we show that $0_V$ is the (unique) identity element of $U$ and in the other direction we only need to check the additive inverse property (satisfied by $(-1)u = -u$).
    \item The sum $U_1 + U_2 +\cdots+ U_m$, where $U_i$'s are subsets of $V$, is the set of all possible sums of the respective elements from each $U_i$.
    \item When the $U_i$'s are subspaces of $V$, then the sum is the smallest vector space containing all $U_i$'s.
    \item When the $U_i$'s are subspaces of $V$, and every element of the sum $U_1 + U_2 +\cdots+ U_m$ can be written in only one way as $u_1+u_2+\cdots+u_m$, we call the sum a \df{direct sum}, denoted by $\oplus$ replacing $+$.
    \item The equivalent criterion for the sum to be a direct sum is that the only way to express $0$ is to write it as the sum of $0$'s from each $U_i$. One direction is trivial, while the other direction employs the common trick of subtracting two different expressions of the same vector $v \in V$.
    \item If we are only discussing two subspaces $U$ and $W$ of $V$, then $U+W$ is a direct sum iff their intersection is $\{0\}$. The proof of this, in both directions, considers $0 = v + (-v)$. The fact $W$ always contains an element and its inverse will give us the answer.
    \begin{itemize}
    	\item Based on this, one can further show that subspaces $U_1,U_2,\dots,U_n$ of $V$ have pairwise intersections are all $\{0\}$ iff $U_1 + U_2 + \dots + U_n$ is a direct sum.
    \end{itemize}
    \item Arbitrary intersection of subspaces of $V$ is a subspace of $V$, and the union of two subspaces is still a subspace iff one is contained in the other.
\end{itemize}


\newpage
\section{Finite-Dimensional Vector Spaces}

\textit{Remark.} Note that this section in the book builds on the idea of a ``list'' (ordered set) of vectors, which is no different than using a ``set'' of vectors because vector addition is commutative. Finite dimension is defined in terms of a spanning list, and the concept of a basis and the dimension of a vector space is pushed backward. In our exposition, we rearrange the progression of these ideas and reconstruct some proofs.

\begin{itemize}
    \item The \df{span} of a list (set) $S$ of vectors in $V$ is defined to be all possible linear combinations of these vectors. In particular, the span of the empty list (set) is defined to be the zero vector space $\{0\}$.
    \item Similar to the fact that the sum $U_1 + U_2 +\cdots+ U_m$ is the smallest vector space containing all $U_i$'s, span($S$) is the smallest subspace of $V$ containing all the vectors in $S$. The proof of the two are almost exactly the same.
    \item A polynomial $p$ in an indeterminate $z$ with coefficients from a field $\F$ is the (not unique) \textbf{expression} $a_0 + a_1 z + \cdots + a_n z^n$ with all the $a_i$'s in $\F$. The \df{degree} of a polynomial is the largest exponent of $z$ with nonzero coefficient, and if all the coefficients are 0, we call the polynomial the \df{zero polynomial}. We customarily define the degree of the zero polynomial as $-\infty$ (as in this book), $-1$, or make it undefined.
    
    P.S. The most general and rigorous definition of a polynomial over a ring is not discussed here, because our focus is vector spaces over a field.
    \begin{itemize}
        \item It is oftentimes useful to define a polynomial over a field $\F$ rather as the \textbf{function} $p: \F \rightarrow \F$ with $p(z) = a_0 + a_1 z + \cdots + a_n z^n$. The reason behind is that a polynomial function with coefficients from an infinite field can be written into only one polynomial expression with its unique coefficients. See Chapter 4 of this book or Appendix E, Theorem 10 of \textit{Linear Algebra} by Friedberg, Insel, and Spence. It is the consequence of the fact that a polynomial of degree $n \geq 1$ has at most $n$ distinct zeros. We will proceed with this definition of polynomials.
        \item We denote the set of all polynomials with coefficients in $\F$ by $\mathcal{P}(\F)$. Under the usual coefficient-wise addition and scalar multiplication, it is obvious that $\mathcal{P}(\F)$ is a vector space over $\F$. In addition, the set of all polynomials with degree at most $m$, denoted by $\mathcal{P}_m(\F)$ is a vector subspace of $\mathcal{P}(\F)$.
    \end{itemize}
    \item Linear independence and dependence can be characterized in multiple ways. A list (set) of vectors is linearly independent iff the only way to write 0 as $a_1v_1 + a_2v_2 + \cdots +a_mv_m$ is the trivial way. (We also sometimes say that some vectors themselves are linearly independent.) It is equivalent to saying that every linear combination has its only representation, or none of the vectors is in the span of the other vectors (can be written as a linear combination of them). Linear dependence is the negation of independence, but in particular we should remember that it means there exists at least one vector $v_i$ that is in the span of the rest of the vectors.
    \begin{itemize}
        \item Obviously removing such vector $v_i$ from $S$ does not change the span.
        \item In particular, for a linearly independent list $s_1, \dots, s_m$ properly extended to a linearly dependent list $s_1, s_2,\dots, s_m, t_1,t_2, \dots, t_n$, one such vector is always in $t_1,t_2,\dots,t_n$. This is because for the nontrivial way of writing $0 = a_1s_1 + \cdots +a_ms_m + b_1 t_1 + \cdots + b_n t_n$, if all $b$'s are 0, then all $a$'s will also be 0. Thus, some $b_i$ must be nonzero, and the corresponding $t_i$ can be removed without changing the span.
        \begin{itemize}
            \item The special case $n=1$ of the contraposition will be useful to us. If one new vector is added to the linearly independent list of $s$'s but does not belong to its span, then the resulting set is still linearly independent.
            \item From this special case we know that for a linearly dependent list $v_1,\dots,v_n$, at some index $k \leq n$ we have $v_1,\dots,v_{k-1}$ being linearly independent, while $v_k \in \s(v_1,\dots,v_{k-1})$. (This is the well-ordering case of the linear dependence lemma 2.21 in book that we will use again in Chapter 5 and Chapter 8.)
        \end{itemize}
        \item Note that a linearly independent list of vectors cannot have the $0$ vector.
    \end{itemize}
    \item We now have the essential tools to prove the following important theorem, that if a vector space $V$ can be spanned by the list $L$ of $w_1, w_2, \dots,w_n$, then any linearly independent list of vectors $u_1,u_2,\dots,u_m$ must have length $m \leq n$. The proof suggests adding a new $u_i$ to the existing list $L$ and then remove another $w_j$ from $L$ in each step. In the end all the $u$'s will be added and the corresponding number of $w$'s are removed from $L$. Therefore, $m \leq n$.
    \begin{itemize}
        \item The direct consequence of this theorem is the definition of dimension. But before that we need to introduce the notion of a basis and its existence.
    \end{itemize}
    \item A \df{basis} of $V$ is a linearly independent spanning list of $V$. A list is a basis iff all vectors in $V$ can be written as a unique linear combination of the basis vectors. This follows directly from the definition.
    \item Most of time we are concerned with vector spaces with finite bases. The vector spaces that have a finite spanning set are called \df{finite-dimensional vector spaces} (FDVS), and we will soon know why they bear this name.
    \begin{itemize}
        \item First of all, we can show that every finite spanning list of a FDVS $V$ can be reduced to a finite basis.
        
        To prove this first claim, we construct our basis $B$ from scratch and try to add all the elements of the spanning list $L$ of $v_1, v_2, \dots, v_n$ one by one into the basis, as long as the element added is not in the span of the existing vectors in $B$. In this way, every time a new vector is added to $B$, $B$ will still be linearly independent, and this $B$ will be linearly independent in the end. Furthermore, $\s(B) = \s(L) = V$ because all the $v$'s left out are in span($B$).
        \begin{itemize}
            \item It is also possible to start $B$ from $L$ and check from $v_1$ to $v_n$ whether each vector is in the span of the rest of the vectors in $B$. If it is, then we remove it from $B$; if it is not, then we keep it. In the end, $\s(B) = \s(L)$ and none of the vectors in $B$ is in the span of the others in $B$. However, this top-down approach is harder in practice to actually find the basis.
            \item This claim shows that every FDVS has a finite basis.
        \end{itemize}
        \item Furthermore, every linearly independent list of a FDVS $V$ can be extended to a basis.
        
        To prove this second claim, similarly construct our basis from the given linearly independent list $L$. We pick a finite basis $B$ of $V$ consisting of $b_1,\dots,b_n$, and check whether the next $b$ may be added to $L$ (same to what we did above). In the end, all the $b$'s are in the span of $L$ and $L$ remains linearly independent.
    \end{itemize}
    
    \item Choose two finite bases of a vector space. The fact that they are both linearly independent and both span $V$ tells us that the two bases should have the same size, which we define as the \df{dimension} of a vector space. Thus, the vector spaces with finite bases are called \df{finite-dimensional}.
    \begin{itemize}
        \item The fact that the bases of $V$ has a fixed size implies every spanning list or linearly independent list of this size $\d V$ is a basis itself (because the reduction/extension here is the trivial one).
    \end{itemize}
    \item The subspace dimension $\d U$ is always $\leq$ the whole space dimension $\d V$. To accomplish this we have to resort to the classic procedure: we add vectors from $U$ that are not in the span of the existing ones in $L$. Thus $L$ always remains linearly independent, and it should always be smaller in size than $B$, a spanning list of $U$. Our procedure will eventually terminate and no more vectors can be added to $L$, i.e., all the vectors in $V$ is in $\s(L)$. $L$ is our desired basis, and $\abs{L} = \d U \leq \d V = \abs{B}$.
    \begin{itemize}
        \item Note it is WRONG to assume that a finite basis of the whole space can always be reduced to a finite basis of the subspace!
        \item Corollary: for FDVS $V$ and its subspace $U$, if the two spaces have the same dimension, then they are the same space. This is an important result useful in many proofs.
    \end{itemize}
    \item There are two more things that deserve mentioning. For FDVS $V$ and its subspace $U$, we can always find subspace $W$ such that $V = U \oplus W$. The idea is to find a basis $B_U$ of $U$ and extend it to the basis $B_V$ of $V$. Using the set notation, $\s(B_V \backslash B_U)$ is our desired $W$. We then only have to show $V = U+W$ and $U \cap W = \{0\}$, which is not difficult.
    \item The dimension of the sum of two vector spaces $U_1$ and $U_2$ can be expressed as follows: $$\d (U_1 + U_2) = \d U_1 + \d U_2 - \d (U_1 \cap U_2).$$
    \begin{itemize}
        \item Despite the fact that it looks like the inclusion-exclusion formula, $+$ and $\cup$ are quite different, thus the formula cannot be similarly generalized to higher orders. To prove the formula, start from a basis $u_1,u_2,\dots,u_m$ of $U_1 \cap U_2$ (intersection is a subspace). $U_1$ and $U_2$ then have their respective bases $B_1$ and $B_2$. We can show $B_1 \cup B_2$ is a basis of $U_1 + U_2$. See page 47 of the book for the full proof.
        \item Note that when the sum is a direct sum, then the formula is just $\d(U_1 \oplus U_2) = \d U_1 + \d U_2$. We will generalize this result in \lk{3}{5}.
    \end{itemize}
\end{itemize}


\newpage
\section{Linear Maps}
\subsection{The Vector Space of Linear Maps}
\begin{itemize}
    \item A \df{linear map(ping)/transformation} is a homomorphism of vector spaces under addition and scalar multiplication. The linear structure is preserved after the transformation $T$. To use symbols, a linear map is a function $T: V \rightarrow W$ such that for all $u,v \in V$ and $\lambda \in \F$ (the field of $V$ and $W$),
    \begin{equation*}
        T(u+v) = Tu + Tv \quad \text{and} \quad
        T (\lambda u) = \lambda T(u).
    \end{equation*}
    \item The zero transformation maps every vector of $V$ to $0_W$. The identity transformation $I_V: V \rightarrow V$ maps each element of $V$ to itself in $V$.
    \item We define the addition and scalar multiplication of linear maps in the vector-wise way: $$(S+T)(v) = Sv + Tv \quad \text{and} \quad (\lambda T)(v) = \lambda T(v),$$ following which we can easily show that the set of linear maps $\mathcal{L}(V,W)$ from $V$ to $W$ is a vector space over $\F$. In addition to the linear properties, we define the \df{product} $ST$ of linear maps same as $S \circ T$, given the appropriate domain and codomain. Why the product is essential will become clear later. The theory of functions tells us this product is associative [$T_1(T_2 T_3) = (T_1 T_2) T_3$] and the identity works as usual ($I_W T = T I_V = T$). The distributive properties [$(S_1 + S_2) T = S_1 T + S_2 T \text{ and } S(T_1 + T_2) = S T_1 + S T_2$] obviously hold as well.
    % \item If $T(x_1,\dots,x_n) = (a_{11}x_1+\cdots+a_{1n}x_n,\dots,a_{m1}x_1+\cdots+a_{mn}x_n)$, then $T$ is obviously a linear map from $\F^n$ to $\F^m$. For a linear map $T \in \mathcal{L}(\F^n,\F^m)$, it must be of this form, and all the $a_{ij}$'s are determined by the mapping of the $n$ canonical basis vectors of $\F^n$.
    \item The uniqueness of the linear map that takes basis vectors $v_1, v_2, \dots, v_n$ to corresponding vectors $w_1, w_2, \dots, w_n$ is the most fundamental theorem in this section. It is our first step toward showing that linear maps and matrices are isomorphic. To prove the theorem, for arbitrary $a_i$'s, let $$T: \sum_{i=1}^n a_i v_i \mapsto \sum_{i=1}^n a_i w_i.$$
    \item Here are a few properties that are useful in linear maps. The proofs are quite routine.
    \begin{itemize}
        \item $T(0_V) = 0_W;$
        \item $T$ is linear iff $T(cx+y) = cT(x) + T(y)$ for all $c,x,y.$
    \end{itemize}
\end{itemize}

\subsection{Null Spaces and Ranges}
\begin{itemize}
    \item The \df{nullspace}/\df{kernel} $\n T$ is the preimage of $0_W$. The \df{range}/\df{image} $\r T$ is defined the same as the range of a function. The nullspace and the range are respectively subspaces of $V$ and $W$ (by checking the three criterion). If the nullspace (resp.\ range) is finite-dimensional, then its dimension is the \df{nullity} (resp.\ \df{rank}) of $T$.
    \begin{itemize}
        \item Since $\d U = \d W$ for subspace $U$ of $W$ implies $U = W$, $\d \r T = \d W$ implies that $T$ is surjective.
        \item An easy theorem to always keep in mind is that the map of the basis vectors of $V$ spans $\r T$. We will use it in the proof of the next theorem.
    \end{itemize}
    \item We are now endowed with all the tools to prove the \textbf{rank-nullity theorem}: for a FDVS $V$ and $T$ from $V$ to $W$, $\r T$ is finite-dimensional with $$\d V = \d \n T + \d \r T.$$
    \begin{itemize}
        \item As always, we need to construct a basis to work with first. Let $u_1, u_2, \dots, u_n$ be a basis of $\n T$, and thus can be extended to $u_1,\dots,u_m,v_1,\dots,v_n$ a basis of $V$. We prove $T(v_1, v_2, \dots, v_n)$ is a basis for $\r T$. The fact that the $u$'s are in the nullspace shows that $Tv_1, \dots,Tv_n$ spans $\r T$ (by the ``easy'' theorem above). To show the elements are linearly independent, consider that $$a_1 T(v_1)+\cdots+a_n T(v_n) = 0$$ implies $a_1v_1+\dots+a_nv_n \in \n T$. Yet we also have $b_1u_1+\dots+b_nu_n \in \n T$, so we can set the two linear combinations equal. Because the $u$'s and $v$'s are altogether linearly independent, the coefficients $a$'s (and $b$'s) are all 0.
    \end{itemize}
    \item The rank-nullity theorem gives rise to a number of equivalent characterizations regarding injection and surjection. First of all, there is a criterion that says $T$ is injective iff $\n T = {0}$ (proof follows straight from the definition of nullspace). The results below follow.
    \begin{itemize}
        \item A map to a smaller dimensional space cannot be injective because $\d (\n T) > 0$.
        \item A map to a large dimensional space cannot be surjective because $\d (\r T) \leq \d V < \d W$.
        \item $V$ and $W$ are FDVS of equal dimension, then $$T \text{ is injective} \Longleftrightarrow T \text{ is surjective} \Longleftrightarrow \d \r T = \d V.$$
    \end{itemize}
    \item Solving systems of linear equations is a direct application of this.
    \begin{itemize}
        \item A homogeneous system of linear equations with more variables than equations has nonzero solutions. Here we can view the system as a linear map $T$ that maps the vector of unknown variables $x \in \F^n$ to $0 \in \F^m$ with $n > m$. The nullity of $T$ is greater than 0 as a result.
        \item Similarly, the $T$ associated with an inhomogeneous equation with more equations than variables is not surjective, and thus some choices of constants on the right side will give no solutions to the system.
    \end{itemize}
\end{itemize}

\subsection{Matrices}
\begin{itemize}
    \item Consider the $n$-dimensional $V$ with basis $\bv$ and $m$-dimensional $W$ with basis $w_1,\dots,w_m$. For $T \in \LVW$, we can find for every $v_k$, a list of $A_{i,k}$'s such that \[Tv_k = A_{1,k}w_1 +\cdots+ A_{m,k}w_m = \sum_{j=1}^m A_{j,k}w_j.\] As a result, we have $n$ lists of scalar $a$'s, each of length $m$, and we can put them into an $m \times n$ matrix $A$, with entries $a_{i,j}$ in the $i$-th row and $j$-th column. $\mathcal{M}(T,(\bv),(\bw))$ uniquely determines the linear map $T \in \LVW$.
    \begin{itemize}
        \item Assume
        \begin{align*}
            Tv_1 & = A'_{1,1} w_1 + \dots + A'_{1,m} w_m, \\
            & \vdotswithin{=} \\
            Tv_n & = A'_{n,1} w_1 + \dots + A'_{n,m} w_m,
        \end{align*}
        from which we get the $m \times n$ matrix $A'$. Its transpose $n \times m$ $A$ is our defined matrix representation of $T$. The reason for taking transpose will become clear later.
    \end{itemize}
    \item We define matrix addition entry-wise, because under this definition, matrix representation of linear maps preserves addition, i.e. $\M(S+T) = \M(S) + \M(T)$ for $S,T \in \LVW$, assuming $S+T$, $S$, and $T$ share the same ordered bases.
    \item Similarly entry-wise scalar multiplication with matrices give us $\M(\lambda T) = \lambda \M(T)$.
    \item Given the definition of matrix addition and scalar multiplication, $\F^{m,n}$, the set of matrices with $m$ rows and $n$ columns over $\F$, is a vector space of dimension $mn$.
    \item The reason why we represent linear maps from $n$-dimensional VS to $m$-dimensional VS is that we want to preserve the ordering of ``$S$'' and ``$T$'' after matrix representation, i.e., $\M(ST) = \M(S)\M(T)$ instead of $\M(T)\M(S)$, under the usual definition of matrix multiplication: $$[AC]_{j,k} = \sum_{r = 1}^n A_{j,r}C_{r,k}.$$
    \begin{itemize}
        \item Remember that matrix multiplication is not commutative, but is associative and distributive.
    \end{itemize}
    \item The book defines $A_{j,\cdot}$ and $A_{\cdot,k}$ as the $j$-th row and the $k$-th column of matrix $A$, and thus $[AC]_{j,k} = A_{j,\cdot}C_{\cdot,k}$. Also, $k$-th column of matrix product equals matrix times the $k$-th column of the second matrix: $$[AC]_{\cdot,k} = AC_{\cdot,k},$$ which is an alternative to understanding matrix multiplication. Obviously there is a row equivalent version that says $j$-th row of the matrix product equals $j$-th row times the second matrix.
    \item When we are considering matrix $A$ times a column vector $c$, we can see $Ac$ as the linear combination $c_1A_{\cdot,1} + \cdots + c_nA_{\cdot,n}$.
\end{itemize}

\subsection{Invertibility and Isomorphic Vector Spaces}
\textit{Remark.} The isomorphism between linear maps and matrices is the most central theorem of linear algebra. With the machinery developed in \lk{3}{3} and \lk{3}{4}, all the nice results we will prove about linear maps can be directly \textbf{translated to matrices}.
\begin{itemize}
    \item $T \in \LVW$ is \df{invertible} if there exists $S$ such that $ST = I_V$ and $TS = I_W$. We denote this inverse $S$ by $T^{-1}$. One can show that the inverse must be linear and thus belongs to $\mathcal{L}(W,V)$. By the associativity of linear maps, the inverse is unique.
    \item Invertibility is equivalent to bijectivity between vector spaces. Linear maps are functions, so beyond showing that the inverse is linear, the proof is exactly the same.
    \item A (linear) \df{isomorphism} is an invertible linear map; if an isomorphism exists between two vector spaces, then the vector spaces are (linearly) \df{isomorphic}.
    \item Two FDVS over $\F$ are isomorphic iff they have the same dimension. Injection gives us $\d \n = 0$ and surjection gives us $\d \r T = \d W$. The rank-nullity theorem tells us $\d V = \d W$. On the other hand to prove isomorphism, we show $T(a_1v_1+\cdots+a_nv_n) = a_1w_1+\cdots+a_nw_n$ is both injective and surjective, which is not hard.
    \begin{itemize}
        \item The theorem above implies vector spaces of dimension $n$ are always isomorphic to $\F^n$.
    \end{itemize}
    \item Now we show the isomorphism between a linear map and its corresponding matrix. Given ordered bases $\bv$ of $V$ and $w_1,w_2,\dots,w_m$ of $W$, then $\M$ is a function from $\LVW$ to $\F^{m,n}$. As we have noted earlier $\M$ is linear. Thus, it suffices to show that $\M$ is bijective.
    \begin{itemize}
        \item For all $A \in \F^{m,n}$, let $$Tv_k = \sum_{j = 1}^m A_{j,k}w_j.$$ Then this is the \emph{unique} (recall the theorem we stressed in \lk{3}{1}) $T$ such that $A = \M(T)$.
        \item A linear map and its matrix representation are homomorphic not merely in regards to linearity, but also in regards to the preservation of the order of the product $\M(ST) = \M(S)\M(T)$.
    \end{itemize}
    \item By the theorems on isomorphism above, $\d \LVW = mn = (\d V) (\d W)$.
    \item We now shift our focus from the matrix representing an entire linear map to the matrix/column vector representing a vector in the same $V$. We fix the basis $\bv$, then for any $v \in V$, we define
    \begin{equation*}
        \M(v) = 
        \begin{bmatrix}
            a_1 \\ \vdots \\ a_n
        \end{bmatrix},
    \end{equation*}
    where $a_1,\dots,a_n$ are given by $v = a_1v_1+\cdots+a_nv_n$. Here $\M$ is an isomorphism from $V$ to $F^{n(, 1)}$.
    \begin{itemize}
        \item Following this definition, $\M(T)_{\cdot,k} = \M(Tv_k)$.
        \item More importantly, the linear map of a vector can now be completely described by matrix-multiplication. Symbolically, we want to prove $\M(Tv) = \M(T)\M(v)$ for all $v \in V$. Since $v = a_1v_1 + \cdots +a_nv_n$ and $\M$ is linear,
        \begin{align*}
            \M(Tv) & = a_1\M(Tv_1) + \cdots + a_n \M(Tv_n) \\ & = a_1\M(T)_{\cdot, 1} + \cdots + a_n\M(T)_{\cdot, n} \\ & = \M(T)\M(v).
        \end{align*}
        \item How do we interpret this? Consider the following diagram under the appropriate bases for $V$ and $W$.
        \begin{equation*}
            \begin{tikzcd}
            V \arrow[r,"T"] \arrow[d,"\simeq"]& W \arrow[d,"\simeq"] \\
            \F^{n,1} \arrow[r,"\M(T)"] \arrow[u,"\M"] & \F^{m,1} \arrow[u,"\M"]
            \end{tikzcd}
        \end{equation*}
        For a vector $v \in V$, its image $\M(v)$ under the isomorphism $\M$ of $V$ belongs to $\F^{n,1}$. Matrix multiplication with $\M(T)$ gives us $\M(Tv) \in \F^{m,1}$, which we identify with $Tv$ under the isomorphism $\M$ of $W$. This is why a linear map can be described entirely by its matrix representation.
        \item If $V = \F^n$ and $W = \F^m$ and we are using the canonical bases, then every linear map is equivalent to a unique matrix multiplication on the left. \emph{Every matrix-left-multiplication is a linear map, and every linear map is a matrix-left-multiplication.} Think about the Jacobian matrix representation of the total derivative in the Euclidean space. Left-multiplication by a matrix corresponds to the linear \emph{transformation} of a vector.
    \end{itemize}
    \item A \df{linear operator} is a linear map from a VS to itself. The notation $\LV$ stands for $\mathcal{L}(V,V)$.
    \item Suppose $V$ is a FDVS and $T \in \LV$, then (a) $T$ is invertible iff (b) $T$ is injective iff (c) $T$ is surjective. (This is not true for infinite-dimensional vector space.) This is true for functions $f:S \to S$ (where $S$ is finite) in general, but for linear maps the proof is likely to be rather simple:
        
    (a)$\implies$(b) we proved earlier.
        
    (b)$\implies$(c) by checking the rank-nullity theorem. $T$ being injective means $\d \n T = 0$, which implies $\d \r T = \d V$.
        
    (c)$\implies$(a) by showing $T$ is injective, as $\d \n T = \d V - \d \r T$, which is 0.
\end{itemize}

\subsection{Products and Quotients of Vector Spaces}
\begin{itemize}
    \item The \df{product} $V_1 \times \dots \times V_n$ of vector spaces $V_1,\dots,V_n$ over $\F$ is the set $$\{(v_1,\dots,v_n) \where v_1 \in V_1,\dots, v_n \in V_n\}.$$ The addition and scalar multiplication on the product is defined in the usual entry-wise way. It is routine to prove that this product is a vector space over $\F$ as well.
    \item $\d (V_1\times \dots \times V_n) = \d V_1 + \dots + \d V_n$. Why? Choose a basis for each $V_i$, then put every basis vector of $V_i$ into the $i$-th entry of $(v_1,\dots,v_n)$ and let other entries be 0. These vectors are linearly independent and span the product and thus is a basis of the product.
    \item For subspaces $U_1,\dots,U_n$ of $V$, define the linear map $\Gamma: U_1 \times \dots \times U_n \to U_1 + \dots + U_n$ by $$\Gamma(u_1,\dots,u_n) = u_1+\dots+u_n.$$ (One can easily check that this map is indeed linear.) Then $U_1+\dots+U_n$ is a direct sum iff $\Gamma$ is injective (or invertible, since $\Gamma$ is surjective by its definition.) To prove this, we just need to show $\n T = {0}$ iff the only way to write $0 = u_1+\dots+u_n$ is to let all $u$'s be 0. This is quite clear.
    \item The previous two theorems lead to the following important one. A sum of subspaces $U_i$'s of $V$ is a direct sum iff dimensions add up. This is because two isomorphic vector spaces must have the same dimension. In both directions we would encounter $$\d (U_1+\dots+U_n) = \d (U_1 \times \dots \times U_n) = \d U_1 + \dots + \d U_n.$$
    Construct the linear map $\Gamma$ above from the product of subspaces makes the proof really clean. Recall that we have proved the special case $n=2$ already.
    \item An \df{affine subset} of $V$ is a subset of $V$ of the form $v+U = \{v+u \where u \in U\}$ for some vector $v \in V$ and some subspace $U$ of $V$. Here the affine subset $v+U$ is said to be \df{parallel} to $U$.
    \begin{itemize}
        \item $v+U$ is a subspace of $V$ iff $v \in U$. If $v \in U$, then $v+U = U$, subspace of $V$. If $v \notin U$, then $0 \notin v+U$, and thus $v+U$ cannot be a subspace of $V$.
    \end{itemize}
    \item Under the same setting, the quotient space $V/U$ is the set of all affine subsets of $V$ parallel to $U$, i.e., $V/U = \{v+U \where v \in V\}$.
    \item For subspace $U$ of $V$ and $v,w \in U$, then
        \begin{equation*}
            \text{(a) } v-w \in U \iff \text{(b) } v+U = w+U \iff \text{(c) } (v+U) \cap (w+U) \not= \emptyset.
        \end{equation*}
    (a)$\implies$(b) as $v+u = w+((v-w)+u) \in w+U$ (and the same for the other direction). (b)$\implies$(c) because $v+U$ is not empty, while (c)$\implies$(a) because $v+u_1=w+u_2$ for some $u_1$ and $u_2$, and we then move $v-w$ to the left side.
    \begin{itemize}
        \item The theorem above tells us that two affine subset to $U$ are either the same or  disjoint.
    \end{itemize}
    \item The addition and scalar multiplication on $V/U$ are defined by  $$(v+U)+(w+U) = (v+w)+U \quad \text{and} \quad \lambda(v+U) = \lambda v + U.$$ We should pay particular attention to this definition because \textbf{an affine subset do not have a unique representation} (for $v \not= v'$, it is possible that $v+U = v'+U$ still)! Therefore, one has to show that  different representations $v+U$ and $w+U$ of the same affine subsets of $U$ still gives us a \textbf{unique} $(v+w)+U$, so that the addition function above is well-defined (and similar for scalar multiplication). Suppose $v+U = v'+U$ and $w+U = w'+U$, then essentially we want to prove $(v+w)+U=(v'+w')+U$. By assumption $v-v'$ and $w-w' \in U$, $(v+w)-(v'+w') \in U$, and we reach the conclusion (and similar for scalar multiplication).
    \item $V/U$ with the two operations above is a vector space. Showing this is routine, but one has to remember that $V/U$ is not a subspace of $V$. Despite all the elements of the quotient space is in the whole space, the additive identity is now $0+U=U$, and the additive inverse of $v+U$ is $-v+U$. (The quotient space is discussed with respect to a fixed subspace.) Therefore, we have to check the 8 VS properties.
    \begin{itemize}
        \item Also, by the previous theorem, for any $v \in U$, $v+U$ equals the 0 vector $(0+)U$ and is thus not a basis vector for $V/U$.
    \end{itemize}
    \item To find the dimension of $V/U$, we appeal to the trick of constructing a linear map again. (One should check its linearity) The \df{quotient map} $\pi: V \to V/U$ is given by $$\pi(v) = v+U.$$
    For FDVS $V$ and its subspace $U$, $\d V/U = \d V - \d U$. $\pi$ is a map onto $V/U$ obviously, and $\n \pi = U$. Therefore the formula follows.
    \begin{itemize}
        \item A basis from $V/U$ combined with a basis from $U$ form a basis of $V$. Suppose $v_1,\dots,v_n$ are the basis vectors of $V/U$ and $u_1,\dots,u_m$ are the basis vectors of $U$. If we write $$a_1v_1+\dots+a_nv_n + b_1u_1+\dots+b_mu_m = 0,$$ then necessarily $(a_1v_1+\dots+a_nv_n) \in U$, which implies that $$0+U = (a_1v_1+\dots+a_nv_n)+U = a_1(v_1+U)+\dots+a_n(v_n+U).$$ Because the $v+U$'s form a basis of $V/U$, the coefficients $a_1,\dots,a_n$ must be 0. Since $u_1,\dots,u_m$ form a basis of $U$, their coefficients $b_1,\dots,b_m$ must also be 0. And because $v_1,\dots,v_n,u_1,\dots,u_m$ is of length $n+m$, by the previous theorem this indeed is a basis of $V$.
    \end{itemize}
    \item Now we define the induced map $\widetilde{T}: V/(\n T) \to W$ of $T: V \to W$ by \[\widetilde{T}(v+\n T) = Tv.\] $u + \n T = v + \n T$ implies $u - v \in \n T$, and thus $Tu-Tv = 0$. Therefore the induced map is well defined. Checking the linearity of $\widetilde{T}$ is routine. Note that $T = \widetilde{T} \circ \pi$.
        \[
            \begin{tikzcd}
                V \arrow[rr,"T"] \arrow[dr,"\pi"] && W \\
                & {V/\n T} \arrow[ur,"\widetilde{T}"]
            \end{tikzcd}
        \]
    \item The definition leads to the isomorphism theorem that exists for different algebraic structures. For $T \in \LVW$, $\widetilde{T}$ is injective and $\r \widetilde{T} = \r T$. The latter simply follows from the definition, while the first requires us to prove $\n \widetilde{T} = \{0+\n T\}$, the identity. $\widetilde{T}(v+\n T) = Tv = 0$ gives us $v-0 \in \n T$. Hence, $v+\n T = 0+\n T$, and the conclusion follows.
        \begin{itemize}
            \item The two claims combined tell us $V/(\n T) \simeq \r T$ under the isomorphism $\widetilde{T}$.
        \end{itemize}
    \end{itemize}
    
\subsection{Duality}
\begin{itemize}
    \item A \df{linear functional} on $V$ is a linear map belonging to $\mathcal{L}(V,\F)$. The \df{dual space} of $V$, denoted usually by $V^*$ or $V'$, is the vector space $\mathcal{L}(V,\F)$.
    \item We proved earlier that $\d \LVW = (\d V)(\d W)$. Therefore, $\d V' = \d V$ for finite-dimensional $V$.
    \item The \df{dual basis} of $v_1,\dots,v_n$, basis of $V$, is a list of vectors $\phi_1,\dots,\phi_n$ in $V'$, where each $\phi_i$ is a linear functional defined by
    \begin{equation*}
        \phi_i(v_j) = \delta_{ij} = \left\{
            \begin{array}{rl}
                1 \quad & \text{if } i = j, \\
                0 \quad & \text{if } i \not= j.
            \end{array}
        \right.
    \end{equation*}
    These $\phi_i$'s are designed to form a basis of $V'$. It suffices to show that $\phi_1,\dots,\phi_n$ is linearly independent in $V'$. Consider $$a_1\phi_1+\dots+a_n\phi_n=0.$$ Since $(a_1\phi_1+\dots+a_n\phi_n)(v_j)=a_j$, all $a_j$'s are 0.
    \begin{itemize}
        \item Remember that the dual basis vector $\phi_i$ of $e_1,\dots,e_n$ in $\F^n$ has $$\phi_i(x_1,\dots,x_n)=\phi_i(x_1e_1+\dots+x_ne_n)=x_i.$$ Namely, the $i$-th dual basis vector projects $(x_1,\dots,x_n)$ to its $i$-th coordinate.
    \end{itemize}
    \item For $v_1,\dots,v_n$ basis of $V$ and its associated dual basis $\phi_1,\dots,\phi_n$ of $V'$, 
    \begin{itemize}
        \item first, every $\psi \in V'$ can be written as $\sum_{i=1}^n \psi(v_i)\phi_i$. Since $(\sum_{i=1}^n \psi(v_i)\phi_i)(v_j) = \psi(v_j)$, the sum and $\psi$ agrees on all basis vectors and are thus equal;
        \item second, every $v \in V$ can be written as $\sum_{i=1}^n \phi_i(v)v_i$, since for $v = a_1v_1 + \dots +a_nv_n$, every $a_j = \phi_j(v)$.
    \end{itemize}
    \item The \df{dual map} $T' \in \mathcal{L}(W',V')$ of $T \in \LVW$ is given by $T'(\phi)=\phi \circ T$ for $\phi \in W'$. We have to be very clear about what this definition means. Here $T'$ maps a function to a function.
    \begin{equation*}
        \begin{tikzcd}
            W \arrow[dd, "\phi"'] &  & V 
            \arrow[dd, "\substack{\phi \circ T \\ \text{through} \\ W}"] \\
            {} \arrow[rr, "T'"', Rightarrow] &  & {} \\
            \F &  & \F
        \end{tikzcd}
    \end{equation*}
    $T'$ is a composition of linear maps and is thus a linear map from $V'$ to $\F$. To show that $T'$ is linear from $W'$ to $V'$, one just follows the definition.
    \begin{itemize}
        \item The following algebraic properties hold for dual maps:
        \begin{itemize}
            \item $(S+T)'=S'+T'$,
            \item $(\lambda T)'=\lambda T'$,
            \item $(ST)'=T'S'$
        \end{itemize}
        under the appropriate $S,T,$ and $\lambda$. The first two are routine, and the third one is manipulation of function compositions.
    \end{itemize}
    \item We define \df{annihilator} $U^0$ of set $U \subseteq V$ as $\{\phi \in V' \where \phi(U)=0\}$, the subset of all linear functionals on $V$ whose image of subset $U$ is 0.
    \item There is a good example (3.104) in the book about annihilators. For the dual basis $\phi_1,\dots,\phi_5$ in $(\F^5)'$ that corresponds to $e_1,\dots,e_5$ in $\F^5$, $(\s(e_1,e_2))^0 = \s(\phi_3,\phi_4,\phi_5)$, since $\phi_i$ takes out the $i$-th coordinate.
    \item As expected $U^0$ is a subspace of $V'$. The zero linear functional takes $U \subseteq V$ to $0$. $(\phi+\psi)(U) = 0$ and $\lambda\phi(U) = 0$ are omitted as usual.
    \item For FDVS $V$ and its subspace $U$, \[\d U + \d U^0 = \d V.\]
    
    The book provides two proofs. The first proof takes the usual ``choose a basis and extend it'' approach. The second proof is more obscure: consider the inclusion map $i \in \mathcal{L}(U,V)$ such that $i(u) = u$ for all $u \in U$, then $i' \in \mathcal{L}(V',U')$, which gives us $$\d \n i' + \d \r i' = \d V' = \d V.$$ Note that $i'(\phi) = \phi \circ i$ for $\phi \in V'$, giving us $\n i' = U^0$. The nullspace of $i'$ are the set of $\phi$'s such that $\phi \circ i = 0$, and therefore $\phi$ should map the entire $U$ to $0$.
    
    Now we show $U = \r i'$. A linear map defined on a subspace can always be extended to the whole space, and thus we may extend any $\phi \in U'$ to $\psi \in V'$. The dual map $i'$ maps $\psi$ back to $\phi$, so $\phi \in U' \implies \phi \in \r i'$, showing us what we desire.
    \item The nullspace and range of the dual map $T'$ can now be characterized in terms of the concepts we have introduced.
    \begin{itemize}
        \item For $T \in \LVW$, $\n T' = (\r T')^0$: $$\phi \in \n T' \Longleftrightarrow (\phi \circ T)(v) = \phi(Tv) = 0 \Longleftrightarrow \phi \in (\r T)^0.$$
        \item If $V,W$ are finite-dimensional, then we have
        \begin{align*}
            \d \n T' & = \d (\r T)^0 \\
            & = \d W - \d \r T \\
            & = \d W - (\d V - \d \n T) \\
            & = \d \n T + \d W - \d V.
        \end{align*}
        \item $\d \r T' = \d \r T$ easily follows from the rank-nullity theorem and the two bullet points above.
        \item Also we can prove $\r T' = (\n T)^0$. Showing $\r T' \subseteq (\n T)^0$ is standard (just assume $T'$ maps $\phi$ to $\psi \in \r T'$), while the other  direction is quite tricky. The book assumes finite-dimension here and showed the two spaces have the same dimension: $$\d \r T' = \d \r T = \d V - \d \n T = \d (\n T)^0.$$
        We have to make a side note here. When we proved the first bullet point, we did not assume finite-dimension of $V$ and $W$, so it is natural to consider if our proof of $\r T' \supseteq (\n T)^0$ also does not require the finite-dimension assumption either.
        
        We may formulate our question this way: for $T \in \LVW$ and $\phi \in V'$ with $\phi(\n T) = 0$, can we always find a $\psi \in W'$ such that $\phi = \psi \circ T$? This turns out to be true if we assume the axiom of choice from set theory, which has important corollaries in the theory of infinite-dimensional vector spaces. We will not get into details here, but it is interesting to know about this.
    \end{itemize}
    \item The corollary of the previous claims are the following:
    $$T \text{ is surjective} \iff T' \text{ is injective} \quad \text{and} \quad T \text{ is injective} \iff T' \text{ is surjective}.$$ Proof of this is quite standard, and we show the first one here only: 
    \begin{align*}
        T \in \LVW \text{ surjective} & \iff \r T = W \\ & \iff (\r T)^0 = {0} \iff \n T' = {0} \iff T' \text{ injective}.
    \end{align*}
    \item The \df{row rank} and \df{column rank} of matrix $A$ are defined to be the dimension of the span of the rows of $A$ in $\F^{1,n}$ and of the columns of $A$ in $\F^{m,1}$, respectively. As we know from lower-division matrix algebra, they are the same and represent the number of pivots in a reduced row echelon form of a matrix. We now prove this fact with the tools we have.
    
    \item $\M(T') = (\M(T))^t$. Consider the dual basis $\psi_j$ ($1 \leq j \leq m$) of $W'$ and $\phi_j$ ($1 \leq k \leq n$) of $V'$. Then \[T'(\psi_j) = \sum_{r=1}^n C_{r,j}\phi_r,\] which we evaluate at $v_k$: \[(\psi_j \circ T)(v_k) = \sum_{r=1}^n C_{r,j}\phi_r(v_k) = C_{k,j}.\] On the other hand, we have \[(\psi_j)(Tv_k) = \psi_j\biggl(\sum_{r=1}^mA_{r,k}w_r\biggr) = \sum_{r=1}^m A_{r,k}\psi_j(w_r) = A_{j,k}.\]
    
    \item First, $\d \r T = \text{column rank of } \M(T)$. From bases $\bv$ and $w_1,\dots,w_m$ of $V$ and $W$, we have $\s(Tv_1,\dots,Tv_n)$ is isomorphic to $\s(\M(Tv_1),\dots,\M(Tv_n))$ under the isomorphism $\M$. Thus, the two VS have the same dimensions, with \[\d \s(\M(Tv_1),\dots,\M(Tv_n)) = \text{column rank of } \M(T).\] Recall that $\s(Tv_1,\dots,Tv_n) = \r T$, thus $\d \r T = $ column rank of $\M(T)$, as desired.
    
    \item For $A \in \F^{m,n}$, define $T \in \LVW$ as the left-multiplication by $A$ map (i.e., $Tx = Ax$). Then $A = \M(T)$ with respect to the canonical bases. It follows that
    \begin{align*}
        \text{column rank of } A & = \text{column rank of } \M(T) \\
        & = \d \r T = \d \r T' \text{ (which we proved a moment ago)} \\
        & = \text{column rank of } \M(T') = \text{column rank of } A^t
        = \text{row rank of } A.
    \end{align*}
    Therefore, \df{rank} is a property of a finite matrix.
\end{itemize}


\newpage
\section{Polynomials}
\textit{Remark.} We used facts about polynomials in high school algebra. These facts were intuitively ``understood'' back then, but we never had the chance to rigorously prove them. An abstract algebra course delves deep into some materials on polynomials, which we still take for granted in this book. For example, note that when evaluating a polynomial $p(z)q(z)$ at $z = a$, although we still directly plug $a$ in and compute $p(a)q(a)$ to get the result, this is formally not rigorous. This ``plugging-in'' step requires the machinery of \emph{evaluation homomorphism} to be justified. As another example, one should show that for polynomials with coefficients from a field (more generally from an integral domain), $\deg p + \deg q = \deg pq$ holds, before using it.

\begin{itemize}
	\item The product $pq \in \PF$ of polynomials $p(z)=\sum_{i=0}^m a_iz^i$ and $q(z)=\sum_{j=0}^m b_jz^j$ in $\PF$ is defined by $$(pq)(z) = p(z)q(z) = \sum_{i=0}^m\sum_{j=0}^n a_i b_j z^{i+j}$$ for all $z \in \F$ (``expanding out and collecting like terms''). We can interchange the Sigma notation and see that $p(z)q(z) = q(z)p(z)$. Two polynomials commute under multiplication.
    \item The division algorithm for polynomials is similar to the one in number theory. For $p,s \in \mathcal{P}(\F)$ with $s \not= 0$, then there exist a pair of unique polynomials $q,r \in \mathcal{P}(\F)$ such that $p=sq+r$ and $\deg r < \deg s$.
    \begin{itemize}
        \item For $\deg p < \deg s$, then $q = 0$ and $r = p$. If $\deg p = n \geq \deg s = m$, we define $T: \mathcal{P}_{n-m}(\F) \times \mathcal{P}_{m-1}(\F) \to \mathcal{P}_n(\F)$ by $$T(q,r) = sq+r.$$
        Clearly $T$ is a linear map (by the definition of polynomial addition and multiplication), and we basically want to show $T$ is bijective. For $sq+r=0$, we must have $q=0$ and $r=0$ because otherwise $\deg sq \geq m > \deg r$. Also, notice that $$\d(\mathcal{P}_{n-m}(\F) \times \mathcal{P}_{m-1}(\F)) = n+1 = \d \mathcal{P}_n(\F).$$
    \end{itemize}
    \item Each zero of a polynomial corresponds to a degree-1 factor, i.e., for $p \in \PF$ and $\lambda \in \F$, $p(\lambda) = 0$ iff $\exists q \in \PF$ such that for all $z \in \F$, $p(z) = (z-\lambda)q(z)$. To prove the $\implies$ direction, division algorithm gives us $p(z) = (z-\lambda)q(z) + r$ (here $r$ is a constant) as $(z-\lambda)$ is of degree 1. Thus, $p(\lambda) = 0$ gives us $r=0$.
    
    \item A polynomial $p \in \PF$ has at most as many zeros as its degree ($\geq 0$). $\deg = 0$ and 1 can be checked, and we proceed with induction on higher orders. If $p$ has no zeros, then done. If $p$ has one zero, let $$p(z) = (z-\lambda)q(z),$$ where $p$ is of degree $m+1$ and $q$ is of degree $m$ (with at most $m$ zeros).
    
    \item Recall the remark on polynomials we made in Chapter 2. Now we prove that polynomial functions over an infinite field $\F$ have a unique representation. Suppose $f,g \in \PF$ and $f(z)=g(z)$ for all $z \in \F$, we want to show that $f=g$.
    
    Define $h = f-g$, and then we have $\forall z\in\F, h(z) = 0$. Suppose $h$ is a nonzero polynomial of degree $n$, then by the $h$ has at most $n$ zeros by the theorem above, yet $\F$ is an infinite field. Contradiction! Thus $h = f-g$ is the zero polynomial, and $f = g$ as a result.
    
    \begin{itemize}
        \item In the case of $\F = \C$ or $\R$, the book gives another proof of the contraposition that if one coefficient out of $$h(z) = a_0+a_1z+\dots+a_mz^m$$ is not zero, then $h(z) \not= 0$ for some $z \in \F$. Assume $m \not= 0$ and pick $$z = \frac{\abs{a_0} + \dots + \abs{a_{m-1}}}{\abs{a_m}}+1 \geq 1.$$ Thus, $z^j \leq z^{m-1}$ for all $j \leq m-1$. By triangle inequality and $\abs{a_m}z > \abs{a_0} + \dots + \abs{a_{m-1}}$, \[\abs{a_0+a_1z+\dots+a_{m-1}z^{m-1}} \leq (\abs{a_0}+\dots+\abs{a_{m-1}})z^{m-1} < \abs{a_m z^m}.\] Therefore $a_0+a_1z+\dots+a_{m-1}z^{m-1} \not= -a_m z^m$.
    \end{itemize}
    
    \item The proof of the fundamental theorem of algebra is left to a complex analysis class. Every nonconstant polynomial on $\C$ has a zero, which directly implies that a polynomial $p$ of degree $m$ has $m$ nondistinct roots, which we sketch below.
    \begin{itemize}
        \item Every nonconstant polynomial over $\C$ has a unique factorization (up to the order of factors) of the form 
        \begin{equation}
            p(z) = c(z - \lambda_1)\cdots(z-\lambda_m), \tag{$\star$}
        \end{equation}
        where $c, \lambda_1,\cdots,\lambda_m \in \C$. We call this the unique factorization theorem.
    \end{itemize}
    \begin{itemize}
        \item To prove this, by the fundamental theorem of algebra we can factorize $p(z)$ into $(z-\lambda)q(z)$, and again factorize $q(z)$, and so on. We have proved the existence of such factorization.
        \item Now we proceed to prove the uniqueness. $c$ is clearly unique as the highest coefficient of $p(z)$. Consider $(z - \lambda_1)\cdots(z-\lambda_m)=(z - \tau_1)\cdots(z-\tau_m)$. We want to show that the $\lambda$'s and $\tau$'s are the same up to their ordering. Let $z = \lambda_1$, one of the $\lambda_1 - \tau_i$'s on the RHS must be 0. WLOG we assume $\lambda_1 = \tau_1$, and $(z - \lambda_2)\cdots(z-\lambda_m)=(z - \tau_2)\cdots(z-\tau_m)$ holds for all $z \in \F$ except for $\lambda_1$. Actually $$(z - \lambda_2)\cdots(z-\lambda_m) - (z - \tau_2)\cdots(z-\tau_m) = 0$$ must hold for all $z \in \F$ because we would have a nonzero polynomial with infinite many zeros. Thus, we can proceed to show $\lambda_2 = \tau_2$, and so on.
    \end{itemize}
    \item For polynomials $p \in \mathcal{P}(\C)$ with real coefficients, the complex conjugate of a root is still a root. This follows from $\lambda^m = \conj{\lambda}^m$.
    
    \item For nonconstant $p \in \mathcal{P}(\R)$, $p$ has a unique factorization into linear and quadratic polynomials with the highest coefficients being 1, i.e.,
    \begin{equation}
        p(x)=c(x-\lambda_1)\cdots(x-\lambda_m)(x^2+b_1x+c_1)\cdots(x^2+b_Mx+c_M),
    \end{equation} where the $c$ at the front, the $\lambda$'s, and the $b$'s and $c$'s at the end are all reals and each quadratic factors have negative discriminant.
    \begin{itemize}
        \item (1) would be obviously true if all zeros are reals by the unique factorization theorem. Suppose $p$ has one pair of nonreal complex zeros $\lambda$ and $\conj{\lambda}$, then \[p(x) = (x-\lambda)(x-\conj{\lambda})q(x) = (x^2-2(\Re{\lambda})x+\abs{\lambda}^2)q(x).\] What we want to show is that such $(x - \lambda)$ appear exactly the same number of times $(x - \conj{\lambda})$ in the factorization.
        
        Our idea is to use induction on the degree of $p$. We assume real nonconstant polynomials of degree less than $\deg p$ can be factorized according to (1). If we prove that $q$ (of degree $\deg p-2$) has real coefficients, then $p$ can be factorized according to (1).
        
        Now for all $x \in \R$, \[q(x) = \frac{p(x)}{x^2-2(\Re{\lambda})x+\abs{\lambda}^2}.\] Therefore, for all $x \in \R$, $q(x) \in \R$. If we write $q(x) = a_0+a_1x+\dots+a_{n-2}x^{n-2}$ ($n = \deg p$ and the $a$'s are complex-valued), then taking the imaginary parts on both sides gives us $$0 = (\Im a_0)+(\Im a_1)x+\dots+(\Im a_{n-2})x^{n-2}$$ for all $x \in \R$. Hence the imaginary-part coefficients must all be $0$, telling us $q$ is of real coefficients.
        
        Now we have a factorization of $p$ given by $(1)$, and we want to show its uniqueness. For the quadratic factors with negative discriminants at the end, each can be uniquely factorized into $(x-\lambda_j)(x-\conj{\lambda_j})$. Therefore, if we have two different factorizations of $p$ of the form in (1), then we would have two different standard factorizations ($\star$) of $p$ over $\C$ (as one of factors over $\R$ being different implies that one of the factors over $\C$ is different). This contradicts the unique factorization theorem.
    \end{itemize}
\end{itemize}


\newpage
\section{Eigenvalues, Eigenvectors, and Invariant Subspaces}
\subsection{Invariant Subspaces}
\begin{itemize}
    \item For $T \in \LV$, the subspace $U$ of $V$ is \df{invariant under $T$} if $T(U) \subseteq U$, or alternatively, $T|_U$ is an operator on $U$.
    \item It can be easily verified that $\{0\}$, $V$, $\n V$, and $\r T$ are invariant subspaces of $V$ under $T$.
    \item Here we focus on invariant subspaces of dimension 1. Any 1-dimensional subspace $U$ has some nonzero vector $u \in V$ as the basis (in fact any nonzero vector in $U$ can be the basis). To check if $T$ is invariant under $U$, we only need to check if the basis vectors are mapped into $U$ (so that the $\r T \subseteq U$). Since $Tu \in U$ iff $Tu = \lambda u$, a subspace with dimension 1 is invariant under $T$ iff a (any) nonzero vector $u$ in the subspace has $Tu = \lambda u$.
    \item For $T \in \LV$, $\lambda \in \F$ is an \df{eigenvalue} of $T$ if there exists a nonzero $v \in V$ such that $Tv = \lambda v$. The $v$ here is called the \df{eigenvector}.
    
    Any nonzero vector in a 1-dimension subspace invariant under $T$ is an eigenvector. If $u$ is an eigenvector of $T$, then $\s(u)$ is invariant under $T$.
    \item We may move $\lambda v$ to the left side: consider the linear operator $\lambda I$ that maps $v$ to $\lambda v$. Therefore, $Tv = \lambda v$ is equivalent to $(T-\lambda I) v = 0$. Since $v \not= 0$, this is equivalent to $T-\lambda I$ is not injective/surjective/invertible (since it is an operator on $V$).
    \begin{itemize}
        \item A special case: $T$ is not invertible iff 0 is an eigenvalue of $T$. This will become useful in Chapter 8.
    \end{itemize}
    \item Eigenvectors corresponding to distinct eigenvalues are linearly independent. The usual proof of this starts with 2 distinct eigenvalues and proceeds with induction, but the book uses the well-ordering principle instead.
    
    Suppose the $\bv$ corresponding to eigenvectors $\lambda_1,\dots,\lambda_n$ are linearly dependent, then at $k \leq n$ we have $v_k \in \s(v_1,\dots,v_{k-1})$ with $v_1,\dots,v_{k-1}$ being linearly dependent. Thus, 
    \begin{equation}
        v_k = a_1 v_1 + \dots + a_{k-1}v_{k-1},
    \end{equation}
    and by applying $T$ to both sides, we have $$\lambda_k v_k = a_1 \lambda_1 v_1 + \dots + a_{k-1} \lambda_{k-1}v_{k-1}.$$ If instead, we multiply (2) by $\lambda_k$ on both sides, then $$\lambda_k v_k = a_1 \lambda_k v_1 + \dots + a_{k-1} \lambda_k v_{k-1}.$$ Subtracting the two equations above give us $$0 = a_1 (\lambda_1 - \lambda_k) v_1 + \dots + a_{k-1} (\lambda_{k-1}-\lambda_k) v_{k-1}.$$ Since $v_1, \dots, v_{k-1}$ are linearly independent, all the coefficients are 0 and thus all the $a_i$'s are the same (because the $\lambda$'s are distinct). However, this implies $v_k = 0$, which contradicts with $v_k$ being an eigenvector. Therefore, the set of eigenvectors are linearly independent.
    \item The direct corollary of this is that the number of eigenvalues of $T \in \LV$ cannot exceed the $\d V$.
    \item Here we restate the $T|_U \in \mathcal{L}(U)$ as the \df{restriction operator} of $T$. The \df{quotient operator} $T/U \in \mathcal{L}(V/U)$ is given by $(T/U)(v+U) = Tv+U$ for any $v \in V$. Of course for the quotient operator we need to verify it is well-defined for different representation of the same quotient space, which is very similar to what we have done before and is thus omitted. These two operators provide nice information about the original $T$.
\end{itemize}

\subsection{Eigenvectors and Upper-Triangular Matrices}
\textit{Remark.} (In the upcoming fourth edition, this section is rewritten in terms of minimal polynomials.) Most theorems in this section rely heavily on induction. This is because we will be considering bases and matrices of arbitrary finite dimensions.
\begin{itemize}
    \item Linear operators allow power operations. Given $T \in \LV$, we define $T^m$ ($m \in \Z^+$) as $T$ composite with itself $m$ times, $T^0$ as the identity operator $I_V$, and $T^{-m}$ as $(T^{-1})^m$, given $T^{-1}$ exists.
    \begin{itemize}
        \item Obviously $T^mT^n = T^{m+n}$ and $(T^m)^n = T^{mn}$. (If $T$ is invertible then we can $m,n$ are allowed to be all integers.)
    \end{itemize}
    \item $p(T)$ is the polynomial $p$ taking $T$ as its indeterminate (although $T$ is not $\F$). For $p(z) = a_0 + a_1 z + \dots + a_n z^n$ over $\F$, we define $p(T) = a_0 I + a_1 T + \dots a_n T^n$.
    \begin{itemize}
        \item If we fix an operator $T$, the evaluation map from $\PF$ to $\LV$ given by $p \mapsto p(T)$ is linear.
    	\item Recall that $p(z)q(z) = q(z)p(z)$ for polynomials. It follows directly that $p(T)q(T) = q(T)p(T)$, because we are only replacing the indeterminate symbol $z$ by the operator symbol $T$. Thus, when computing the polynomial of a linear map, the order of the factors does not matter.
    \end{itemize}
    \item It turns out that every operator on a finite-dimensional and nonzero complex vector space necessarily has an eigenvalue. (This is not necessarily true on real vector spaces. See example 5.8(a) on the book: $T(w,z) = (-z,w)$ for $T \in \mathcal{L}(\R^2)$ over $\F = \R$.) 
    
    The proof employs the fundamental theorem of algebra and is quite tricky. Choose a nonzero $v \in V$ ($\d V=n$) and consider the list $v, Tv, \dots, T^n v$, which must be linearly dependent. Thus, we have $$0 = a_0 v + a_1 Tv + \dots + a_n T^n v,$$ where the $a_i$'s are not all $0$. In particular, $a_1, \dots, a_n$ cannot all be 0 since $v \not= 0$ will then give us $a_0 = 0$. Thus, we have a nonconstant polynomial $a_0+a_1 z+\dots+a_m z^m$ ($m \leq n$), which can be factorized into $$c(z - \lambda_1)\cdots(z - \lambda_m)$$ over $\C$. For $$0=c(T - \lambda_1 I)\cdots(T - \lambda_m I)v,$$ the nonzero vector $w = (T - \lambda_{j+1} I) \cdots (T - \lambda_m I)v$ will be be taken to 0 by $(T - \lambda_j I)$. $(T - \lambda_j I)$ is thus not injective and $\lambda_j$ is an eigenvalue.
    \item We construct the matrix of an operator always with respect to only one ordered basis of $V$. The matrix $\M(T,(v_1,\dots,v_n))$ is a square matrix. In the $j$-th column of $\M(T)$, each entry corresponds to each coefficient from $a_1v_1+\dots+a_nv_n = Tv_j$. A square matrix with all entries being 0 below the diagonal is called an \df{upper-triangular matrix}, which is crucial in the theory of eigenvalues.
    \item How are invariant subspaces and upper-triangular matrices connected? Suppose $T \in \LV$ and $v_1,\dots,v_n$ is a basis of $V$, then the following are equivalent:
    \begin{enumerate}[label=(\alph*)]
        \item $\M(T)$ is upper-triangular;
        \item $Tv_j \in \s(v_1,\dots,v_j)$ for each $j$ from 1 to $n$;
        \item $\s(v_1,\dots,v_j)$ is invariant under $T$ for each $j$ from 1 to $n$.
    \end{enumerate}
    The first two are equivalent follow straight from the definition. (b)$\implies$(c) because for every $i$ between 1 and $j$, $Tv_i \in \s(v_1,\dots,v_j)$. (c)$\implies$(b) is obvious as well.
    \item For FDVS $V$ over $\C$ and $T \in \LV$, then $\M(T)$ is upper-triangular with respect to some basis of $V$. The book gives two proofs relying on the induction on $\d V$. \emph{Remember this result will be improved in Chapter 8, where we will show that $\M(T)$ with respect to some basis is not only upper-triangular.}
    
    The first proof first shows that $U = \r(T - \lambda I)$ is invariant under $T$, which means that $U$ has an upper-triangular matrix with respect to a basis $u_1,\dots,u_m$ of $U$ (by the inductive hypothesis, and is equivalent to $Tu_j \in \s(u_1,\dots,u_j)$ for all $j$).
    
    We extend $u_1,\dots,u_m$ to $u_1,\dots,u_m,v_1,\dots,v_n$, basis of $V$. Now set $Tv_k = (T-\lambda I)v_k+\lambda v_k$. Since $(T-\lambda I)v_k \in U$, $$Tv_k \in \s(u_1,\dots,u_m,v_1,\dots,v_k)$$ for all $1 \leq k \leq n$. This combined with $Tu_j \in \s(u_1,\dots,u_j)$ for all $j$ shows that $u_1,\dots,u_m,v_1,\dots,v_n$ is the basis of $V$ that has an upper-triangular matrix.
    
    The second proof is nonstandard but quite interesting. It sets $U = \s(v_1)$ and looks at $V/U$ of dimension $n-1$ instead. $T/U \in \mathcal{L}(V/U)$ by the inductive hypothesis should have (for all $j$ between 2 and $n$) $$(T/U)(v_j+U) \in \s(v_2+U,\dots,v_j+U).$$ This implies that $Tv_j = \s(v_1,\dots,v_j)$ for $j$ between 1 and $n$. Recall in \lk{3}{5} we showed that $v_2,\dots,v_n$ combined with the basis vector $v_1$ from $U$ is a basis of $V$, and the conclusion follows.
    
    \item Upper-triangular matrices help us determine \textbf{whether $T$ is invertible}. If $T \in \LV$ has an upper-triangular $\M(T)$ with respect to some basis of $V$, then $T$ is invertible iff all entries on the diagonal of $\M(T)$ are nonzero.
    \begin{equation*}
        \begin{bmatrix}
        \lambda_1 & & & \ast \\
         & \lambda_2 & & \\
         & & \ddots & \\
         0 & & & \lambda_n
    \end{bmatrix}.
    \end{equation*}
    
    Suppose the $\lambda$'s on the diagonal in the matrix above are all nonzero, then $Tv_1 = \lambda_1 v_1$ implies that $T(v_1/\lambda_1) = v_1$, so that $v_1 \in \r T$. Looking at the next column of the matrix we can continue to show that $T(v_2/\lambda_2)=av_1+v_2$, which implies that $v_2 \in \r T$. Following this fashion we can conclude that $v_1,\dots,v_n$ are all in $\r T$. Since $\d V = n$, $T$ is surjective and thus invertible.
    
    Suppose in the other direction that $T$ is invertible, then in the first place $\lambda_i \not= 0$ because otherwise $Tv_1 = 0$ (Since $v_1 \not= 0$, $T$ cannot be injective). For $1 < j \leq $ suppose $\lambda_j = 0$, ``looking'' at the $j$-th column above we immediately know that $T|_{\s(v_1,\dots,v_j)}$ is not injective (it maps into $\s(v_1,\dots,v_{j-1})$ of dimension $j-1<j$). Thus $T$ (with respect to the whole $V$) is not injective (thus not invertible) as well. Thus, all the $\lambda$'s on the diagonal must be nonzero.
    \item By the previous theorem, we can determine all the eigenvalues of $T$ if we are fortunate to find a basis that corresponds to an upper-triangular $\M(T)$. $\M(T - \lambda I)$ is of the form
    \begin{equation*}
        \begin{bmatrix}
        \lambda_1 - \lambda & & & \ast \\
         & \lambda_2 - \lambda & & \\
         & & \ddots & \\
         0 & & & \lambda_n - \lambda
    \end{bmatrix}.
    \end{equation*}
    One of the $\lambda_i - \lambda$'s is 0 iff $T - \lambda I$ is invertible iff $\lambda$ is an eigenvalue. Therefore, all the entries $\lambda_i$'s on the diagonal are exactly all the eigenvalues.
    \begin{itemize}
        \item Note that if $T$ has no eigenvalue at all, then by this claim $T$ has no upper-triangular matrix with respect to any basis of $V$.
    \end{itemize}
\end{itemize}

\subsection{Eigenspaces and Diagonal Matrices}
\begin{itemize}
    \item A \df{diagonal matrix} has all its entries off the diagonal 0.
    \item The \df{eigenspace} of $T \in \LV$ corresponding to $\lambda \in \F$ is given by $$E(\lambda,T) = \n (T-\lambda I) = \{v \in V \where Tv = \lambda v\}.$$
    The eigenspace is basically the vector subspace of $V$ with all eigenvectors corresponding to $v$, along with the 0 vector.
    \item The sum of eigenspaces is a direct sum. We consider $u_1+\cdots+u_m = 0$, where each eigenvector $u_i$ is from the eigenspace $E(\lambda_i,T)$. Because eigenvectors corresponding to distinct eigenvalues are linearly independent, all the $u_i$'s must be 0. Furthermore, since $$E(\lambda_1,T)+\dots+E(\lambda_m,T)$$ is a direct sum, we have $$\d E(\lambda_1,T) + \cdots + \d E(\lambda_m,T) = \d (E(\lambda_1,T) \oplus \dots \oplus E(\lambda_m,T)) \leq \d V.$$ Recall the theorem in \lk{3}{5} that says a sum is a direct sum iff the dimensions add up.
    \item An operator $T$ is \df{diagonalizable} if the the operator has a diagonal matrix with respect to some basis of $T$.
    \item For $T$ on FDVS $V$ with dimension $n$ and distinct eigenvalues $\lambda_1, \dots, \lambda_m$, diagonalizability of $T$ is equivalent to the following four conditions:
    \begin{enumerate}[label=(\alph*)]
        \item $V$ has a basis consisting of eigenvectors of $T$;
        \item $V$ can be decomposed into $n$ 1-dimension subspaces $U_1$ to $U_n$, where $V = U_1 \oplus \cdots \oplus U_n$;
        \item $V = E(\lambda_1,T) \oplus \cdots \oplus E(\lambda_m,T)$;
        \item $\d V = \d E(\lambda_1,T) + \cdots + \d E(\lambda_m,T) = \d (E(\lambda_1,T) \oplus \dots \oplus E(\lambda_m,T))$
    \end{enumerate}
    The proof to this theorem is a little involved but not hard at all. One should get an intuitive idea of what each equivalent condition means.
    \begin{itemize}
    \item We consider the whole list of eigenvalues $\lambda_1, \dots, \lambda_n$ with repetition ($n \geq m$). Looking at the matrix
    \begin{equation*}
        \begin{bmatrix}
        \lambda_1 & & 0 \\
         & \ddots & \\
         0 & & \lambda_n
    \end{bmatrix}
    \end{equation*}
    for a moment would tell us diagonalizability and (a) are equivalent.
    \item (a)$\implies$(b): Let each $U_j = \s(v_j)$, each of which are invariant under $T$. Since $v_1,\dots,v_n$ is a basis of $V$, any $v \in V$ can be written uniquely as a linear combination of $v_1,\dots,v_n$, and thus a unique sum of elements from each $U_j$. Therefore, $V$ is a direct sum of the $U_j$'s. Trace this proof backward to show (b)$\implies$(a).
    \item We now show (a)$\iff$(c)$\iff$(d). (c)$\iff$(d) follows from the theorem in \lk{3}{5}.
    \begin{itemize}
        \item (a)$\implies$(c) because $V$ has a basis of $n$ eigenvectors, each of which must belong to any of the $m$ distinct eigenspaces $E(\lambda_j,T)$. Therefore, $V$ is the sum of all these eigenspaces (and thus the direct sum of them).
        \item To show (c) \& (d)$\implies$(a), choose a basis $B_j$ from each $E(\lambda_j,T)$. Therefore, we get a list of vectors $v_1, \dots, v_n$ of length $n = \d V$ by (d). To show the list is linearly independent, consider $0 = u_1+\dots+u_n$, where each $u_j$ is from $E(\lambda_j,T)$. All the $u_j$'s must be 0 because $V$ is a direct sum of the eigenspaces. If $0 = u_j$, since $u_j$ can be expressed as a linear combination of the basis vectors in $B_j$, the coefficients in the linear combination must be 0. Therefore our constructed list of eigenvectors $v_1, \dots, v_n$ is indeed a basis of $V$.
    \end{itemize}
    \end{itemize}
    \item If $T \in \LV$ has $\d V$ number of distinct eigenvalues, then $T$ is necessarily diagonalizable. Having $\d V$ number of distinct eigenvalues tells us we have a basis of eigenvectors of $T$. With respect to this basis $\M(T)$ is a diagonal matrix.
\end{itemize}


\newpage
\section{Inner Product Spaces}
\textit{Remark.} In this section $V$ is by default an inner product space.
\subsection{Inner Products and Norms}
\begin{itemize}
    \item For $x=(x_1,\dots,x_n) \text{ and } y=(y_1,\dots,y_n) \in \R^n$ (the real $n$-dimensional space), the \df{dot product} $x \cdot y \coloneqq x_1y_1+\dots+x_ny_n$ is a scalar in $\R$. The norm $\|x\|$ on $\R^n$ is usually given by $\sqrt{x_1^2+\dots+x_n^2}$, which is the square root of the dot product of $x$ with itself.
    \item The usual norm $\|z\| \coloneqq  \sqrt{\abs{z_1}^2+\dots+\abs{z_n}^2}$ on $\C^n$ (the complex $n$-dimension space) is also a scalar in $\R$.
    \item Generally on both real and complex $n$-dimensional spaces, we define the \df{Euclidean inner product} by $$\inp{(w_1,\dots,w_n)}{(z_1,\dots,z_n)} =  w_1\conj{z_1}+\dots+w_n\conj{z_n}.$$ This generalizes the definition of the dot product on $\R^n$. Moreover, we have $$\inp{(z_1,\dots,z_n)}{(z_1,\dots,z_n)} = z_1\conj{z_1}+\dots+z_n\conj{z_n} = \abs{z_1}^2 + \dots + \abs{z_n}^2 = \|z\|^2,$$ so that $\|z\|$ becomes the square root of the inner product of $z$ with itself.
    
    We regard $\R^n$ and $\C^n$ by default as the $n$-dimensional real and complex Euclidean spaces endowed with the Euclidean inner product. 
    
    \item Observation on our definition of the Euclidean inner product suggests how we could possibly define a general abstract inner product on arbitrary real or complex vector spaces. The \df{inner product} is a function $\inp{\cdot}{\cdot}: V \times V \to \F$ ($\R$ or $\C$) with the following properties:
    \begin{itemize}
        \item Positive definiteness: $\inp{v}{v} \geq 0$ (\textbf{real} and nonnegative), which achieves ``$=$'' iff $v = 0$;
        \item Linearity in the first slot: $\inp{\lambda u+v}{w} = \lambda \inp{u}{w} + \inp{v}{w}$; (This means $\phi_w(v) \coloneqq \inp{v}{w}$ with respect to a fixed $w$ gives us a linear functional on $V$.)
        \item Conjugate symmetry: $\inp{u}{v} = \conj{\inp{v}{u}}$.
    \end{itemize}
    The properties that directly ensue include:
    \begin{itemize}
        \item Conjugate symmetry gives us ``conjugate linearity'' (anti-linearity) in the second slot: $$\inp{u}{\lambda v+w} = \conj{\lambda} \inp{u}{v} + \inp{u}{w};$$
        \item For fixed $w \in V$, $f: v \mapsto \inp{v}{w}$ is a linear map from $V$ to $\F$ because of linearity in the first slot;
        \item $\inp{0}{v} = 0$ because linear maps take 0 to 0; and $\inp{v}{0} = 0$ by conjugate symmetry.
    \end{itemize}
    \item The \df{norm} $\|v\| \coloneqq \sqrt{\inp{v}{v}}.$ It has properties
    \begin{itemize}
        \item $\|v\| = 0$ iff $v = 0$, and 
        \item $\|\lambda v\| = |\lambda| \|v\|$.
    \end{itemize}
    \item Two vectors are \df{orthogonal} if their inner product is 0. Therefore, 0 is orthogonal to any vector and 0 is the only vector that is orthogonal to itself.
    \item Introducing these concepts provide us with a list of useful equations and inequalities about \df{inner product spaces} (vector spaces endowed with an inner product) that we know as facts in the Euclidean spaces.
    \begin{itemize}
        \item The Pythagorean theorem now holds for orthogonal vectors $u,v \in V$: $$\|u+v\|^2 = \|u\|^2 + \|v\|^2.$$ Expanding the LHS, we have $\|u+v\|^2 = \inp{u+v}{u+v} = \inp{u}{u} + \inp{u}{v} + \inp{v}{u} + \inp{v}{v}$, where the two terms in the middle are 0.
        The converse of this theorem holds in real inner product spaces because then we would have $\inp{u}{v}+\inp{v}{u} = 2\inp{u}{v} = 0$, showing that the two vectors are orthogonal.
        \item The method of orthogonal decomposition is extended to inner product spaces. Suppose $u,v \in V$ with $v$ being nonzero, we wish write $u$ as the sum of scalar multiple of $v$ and another vector $w$ that is orthogonal to $v$.
        
        Basically we want to choose a scalar $c \in \F$ such that $$0 = \inp{u-cv}{v} = \inp{u}{v} - c\|v\|^2.$$ Let $c = \frac{\inp{u}{v}}{\|v\|^2}$ and we have $w = u - cv$ and we have our orthogonal decomposition $$\inp{w}{v} = 0 \quad \text{and} \quad u = cv+w.$$
        \item The well-known Cauchy-Schwarz inequality states that $\abs{\inp{u}{v}} \leq \|u\|\|v\|$, which reaches equality iff one of $u,v$ is a scalar multiple of the other. This can be proved using the orthogonal decomposition theorem we have above, but we choose to use the more standard method. When $v = 0$, the result is immediate. By the equality condition given, for $v \not= 0$ we may consider $$0 \leq \|u - cv\|^2 = \inp{u}{u} - \conj{c}\inp{u}{v} - c\inp{v}{u} + c\conj{c}\inp{v}{v}.$$ Set $c = \nm{\inp{u}{v}/\inp{v}{v}}$, then the RHS of the ``$=$'' becomes $ \nm{u}^2 - \frac{\abs{\inp{u}{v}}^2}{\nm{v}^2}$, as desired. The equality is achieved if and only if $u = cv$ or $v = 0$.
        
        Two special cases should be kept in mind.
        \begin{itemize}
            \item On real numbers, we have $\abs{x_1y_1+\dots+x_ny_n}^2 \leq (x_1+\dots+x_n)^2(y_1+\dots+y_n)^2$.
            \item One can define an inner product on function spaces such as the vector space $V$ of continuous functions from $[a,b]$ to $\F$. One can define the inner product on $V$ as such: $$\inp{f}{g} = \int_a^b f(x)\conj{g(x)} \dx.$$ This is an important inner product that will be useful in later courses. If $\F = \R$, then we simply have $$\inp{f}{g} = \int_a^b f(x)g(x) \dx.$$ By the Cauchy-Schwarz inequality, we then have (over $\R$) \[\left\lvert\int_a^b f(x)g(x) \dx\right\rvert ^2 \leq \left(\int_a^b (f(x))^2 \dx \right) \left(\int_a^b (g(x))^2 \dx \right).\]
        \end{itemize}
        \item The triangle inequality states that $\|u+v\| \leq \|u\|+\|v\|$, which reaches equality iff one of $u,v$ is a nonnegative real multiple of the other.
        To prove, expanding the LHS gives us
        \begin{align*}
            \inp{u}{u}+\inp{v}{v}+\inp{u}{v}+\conj{\inp{u}{v}} & = \nm{u}^2+\nm{v}^2+2 \Re{\inp{u}{v}} \\ & \leq \nm{u}^2+\nm{v}^2+2\abs{\inp{u}{v}} \\ & \leq \nm{u}^2+\nm{v}^2+2\nm{u}\nm{v} = (\nm{u}+\nm{v})^2.
        \end{align*}
        Taking the square root gives us the result. To have equality in the third row of our derivation, we need $\abs{\inp{u}{v}} = \nm{u}\nm{v}$, which implies the equality in the second row because the norm is a real number. Therefore, the triangle inequality achieves equality iff $\abs{\inp{u}{v}} = \nm{u}\nm{v}$.
        
        Note that $\abs{\inp{u}{v}} = \nm{u}\nm{v}$ implies $u=cv$ or $v=0$ by the Cauchy-Schwarz inequality. Also, it is easy to show $u=cv$ or $v=0$ implies $\abs{\inp{u}{v}} = \nm{u}\nm{v}$. Thus, the triangle inequality achieves equality iff one of $u,v$ is a scalar multiple of the other.
        \item The parallelogram equality says that $\|u+v\|^2+\|u-v\|^2 = 2(\|u\|^2+ \|v\|^2)$. To prove, expand the LHS and after some cancellations, we get the RHS.
    \end{itemize}
\end{itemize}

\subsection{Orthonormal Bases}
\begin{itemize}
    \item A list (set) of vectors is \df{orthonormal} if each vector is of norm 1 and is orthogonal to all other vectors in the list. That is to say for an orthonormal list of vectors $e_1,\dots,e_n \in V$,
    \begin{equation*}
    \inp{e_i}{e_j} = 
        \left\{
            \begin{array}{rl}
                1 \quad & \text{if } i = j, \\
                0 \quad & \text{if } i \not= j.
            \end{array}
        \right.
    \end{equation*}
    \begin{itemize}
        \item Note that the canonical basis in $\F^n$ is orthonormal.
    \end{itemize}
    \item Given an orthonormal list $e_1,\dots,e_n$, by repeated use of the Pythagorean theorem, we get $$\nm{a_1 e_1 + \cdots + a_n e_n}^2 = \abs{a_1}^2 + \cdots +\abs{a_n}^2$$ for all scalars $a_1, \dots, a_m$.
    \item It follows from the theorem above that if we set $a_1 e_1 + \cdots + a_n e_n = 0$, then all the $a_i$'s must be 0, showing that an orthonormal list of vectors $e_1,\dots,e_n$ are linearly independent. Therefore, an orthonormal list of length $\d V$ is an \df{orthonormal basis}, i.e., an orthonormal list (set) that is also a basis.
    \item Let $u_1,\dots,u_n$ be a basis of a vector space $U$, we may endow a natural inner product on $U$ (which you can verify) given by $$\inp{a_1u_1+\dots+a_nu_n}{b_1u_1+\dots+b_nu_n}=a_1b_1+\dots+a_nb_n.$$ What is special about this inner product is that $\inp{u_i}{u_i} = 1$ for all $i$, so that the list of $u_i$'s becomes an orthonormal basis of the inner product space $U$.
    \item Now any $v \in V$ can be written as a unique linear combination of the orthonormal basis vectors $e_1,\dots,e_n$. In fact, we have $$v = \inp{v}{e_1}e_1 + \dots + \inp{v}{e_n}e_n.$$
    Consider $v = a_1 e_1 + \dots + a_n e_n$ and take an inner product with $e_j$ on both sides. Then, $\inp{v}{a_j} = a_j$, as desired. Note that $\nm{v}^2 = \abs{\inp{v}{e_1}}^2 + \dots + \abs{\inp{v}{e_n}}^2$.
    \begin{itemize}
        \item There is an important corollary to the theorem above. For an orthonormal basis of $V$ $e_1,\dots,e_n$, every $$Te_k = \inp{Te_k}{e_1}e_1 + \dots + \inp{Te_k}{e_n}e_n.$$ Therefore, we have $\M(T)_{j,k} = \inp{Te_k}{e_j}$ for the matrix representation of $T$ with respect to $e_1,\dots,e_n$.
    \end{itemize}
    \item The Gram-Schmidt process can now be proved. For a linearly independent list $v_1,\dots,v_n$ in $V$, define $e_1 = \frac{v_1}{\nm{v_1}}$, and then for $2 \leq j \leq n$, define $e_j$ inductively by $$e_j = \frac{v_j - \inp{v_j}{e_1}e_1 - \dots - \inp{v_j}{e_{j-1}}e_{j-1}}{\nm{v_j - \inp{v_j}{e_1}e_1 - \dots - \inp{v_j}{e_{j-1}}e_{j-1}}}.$$ Then for any $j \leq m$, $e_1, \dots, e_j$ is still orthonormal and has the same span as $v_1,\dots,v_j$. 
    
    We use induction for this proof. $j = 1$ obviously holds. Suppose for $1 < j \leq m$ we already have \[\s(v_1,\dots,v_{j-1}) = \s(e_1,\dots,e_{j-1}).\] By the linear independence of the list of $v$'s, $v_j \notin \s(v_1,\dots,v_{j-1})$ and thus \[v_j \notin \s(e_1,\dots,e_{j-1}).\] Therefore, $v_j - \inp{v_j}{e_1}e_1 - \dots - \inp{v_j}{e_{j-1}}e_{j-1}$ is nonzero, and we can indeed divide it by its norm to get a new vector $e_j$ with norm 1.
    
    For $1 \leq k < j$, we can show that the new vector $e_j$ is orthogonal to the existing $e_k$'s, as
    \begin{align*}
        \inp{e_j}{e_k} & = \left\langle \frac{v_j - \inp{v_j}{e_1}e_1 - \dots - \inp{v_j}{e_{j-1}}e_{j-1}}{\nm{v_j - \inp{v_j}{e_1}e_1 - \dots - \inp{v_j}{e_{j-1}}e_{j-1}}}, e_k \right\rangle \\ & = \frac{\inp{v_j}{e_k}-\inp{v_j}{e_k}}{\nm{v_j - \inp{v_j}{e_1}e_1 - \dots - \inp{v_j}{e_{j-1}}e_{j-1}}} = 0.
    \end{align*}
    $v_j \in \s(e_1,\dots,e_j)$ holds by construction and hence $\s(v_1,\dots,v_j)$ is a subspace of $\s(e_1,\dots,e_j)$. The two spaces have the same dimension and are thus equal. This finishes our inductive step.
    \begin{itemize}
        \item This algorithm for constructing an orthonormal basis from a given basis leads to two useful corollaries.
    \begin{itemize}
        \item Every FD inner product space has an orthonormal basis. Proof is obvious.
        \item If $V$ is FD, then every orthonormal list of vectors can be extended to an orthonormal basis. We start with $e_1,\dots,e_m$ and extend the list to a basis $e_1,\dots,e_m,v_{m+1},\dots,v_n$ of $V$. It is easily observed that if we apply the Gram-Schmidt process from $e_1$ to $v_n$, then $e_1$ to $e_m$ will remain themselves, yet $v_{m+1},\dots,v_n$ will be orthonormalized to $f_{m+1},\dots,f_n$. Thus the $e$'s and the $f$'s become the basis extended from the original list of $e$'s.
    \end{itemize}
    \end{itemize}
    \item If $T \in \LV$ has an upper-triangular matrix with respect to some basis of $V$, then it has an upper-triangular matrix with respect to some orthonormal basis of $V$.
    
    (In the case of $\F = \C$, since the condition always holds, $T$ must have an upper-triangular matrix with respect to some orthonormal basis of $V$. This is known as Schur's theorem.)
    
    Recall having upper-triangular $\M(T)$ is equivalent to $\s(v_1,\dots,v_j)$ being invariant under for each $1 \leq j \leq n$. Our claim is ensured by the Gram-Schmidt process.
    
    \item We briefly mentioned a while ago that an inner product with the second slot fixed is a linear functional. In fact there is a \textbf{one-to-one correspondence} between every $u \in V$ and every $\phi \in V'$. The Riesz representation theorem states that for finite-dimensional inner product space $V$ and \textbf{any} $\phi \in V'$, there exists a \textbf{unique} $u \in V$ such that $\phi(v) = \inp{v}{u}$ for every $v \in V$. This is a very important theorem that leads to the definition of adjoint in Chapter 7.
    
    To prove, choose an arbitrary orthonormal basis $e_1,\dots,e_n$. Then for any $v$,
    \begin{align*}
        \phi(v) & = \phi(\inp{v}{e_1}e_1+\cdots+\inp{v}{e_n}e_n) \\ & = \inp{v}{e_1}\phi(e_1)+\cdots+\inp{v}{e_n}\phi(e_n) \\ & = \inp{v}{\conj{\phi(e_1)}e_1+\cdots+\conj{\phi(e_n)}e_n}.
    \end{align*}
    Therefore $\conj{\phi(e_1)}e_1+\cdots+\conj{\phi(e_n)}e_n$ is our desired $u$.
    
    To show uniqueness, suppose $\phi(v) = \inp{v}{u_1} = \inp{v}{u_2}$, then $\inp{v}{u_1 - u_2} = 0$ for all $v$. And by setting $v = u_1 - u_2$ we have $u_1 = u_2$.
    
    \emph{(If for all $v$, $\inp{v}{u_1} = \inp{v}{u_2}$ or $\inp{u_1}{v} = \inp{u_2}{v}$, then $u_1 = u_2$; we will use this frequently in Chapter 7.)}
    \begin{itemize}
        \item By uniqueness, whatever what orthonormal basis $e_1,\dots,e_n$ we choose, the expression $$\conj{\phi(e_1)}e_1+\cdots+\conj{\phi(e_n)}e_n$$ remains the same.
        \item Thus, if we choose any $u = a_1 e_1 + \dots + a_n e_n$ in $U$ and define a linear functional $\phi$ such that $$\phi(e_1)=\conj{a_1},\dots,\phi(e_n)=\conj{a_n},$$ then $\phi$ here is the \textbf{unique} linear functional such that $\phi(v) = \inp{v}{u}$.
    \end{itemize}
\end{itemize}

\subsection{Orthogonal Complements and Minimization Problems}
\begin{itemize}
    \item For $U \subseteq V$ (we assume $U$ is nonempty by default), the \df{orthogonal complement} of $U$ (denoted by $U^\perp$) is the set of all vectors in $V$ that are orthogonal to every vector in $U$, i.e., $$U^\perp = \{v \in V \where \forall u \in U, \inp{v}{u}=0\}.$$
    \begin{itemize}
        \item It is good to see an example in $\R^3$: if $U$ is a line in $\R^3$ through the origin, then all the vectors perpendicular to the line $U$ form the orthogonal complement of $U$, the plane through the origin perpendicular to the line $U$. Similarly, if on the other hand, $U$ is a plane in $\R^3$ through the origin, then its orthogonal complement is the line through the origin perpendicular to the plane $U$.
        \item Given $U \subseteq V$, $U^\perp$ is a subspace of $V$. Check $0 \in U^\perp$ and $U^\perp$ is closed under linear operation.
    \end{itemize}
    \item It is easy to verify and understand the following properties of orthogonal complements:
    \begin{enumerate}[label=(\alph*)]
        \item $0^\perp = V$.
        \item $V^\perp = \{0\}$.
        \item For $U \subseteq V$, $U \cap U^\perp = \left\{\begin{array}{rl} \{0\} & \text{if } 0 \in U, \\ \emptyset \;\, & \text{if } 0 \notin U. \end{array} \right.$
        \item If $U \subseteq W \subseteq V$, then $W^\perp \subseteq U^\perp$.
    \end{enumerate}
    And we skip their proofs.
    \item If in particular $U$ is a finite-dimensional subspace of $V$ ($V$ does not have to be finite-dimensional), then $V = U \oplus U^\perp$.
    
    First we prove $V = U + U^\perp$. Let $e_1,\dots,e_m$ be the orthonormal basis of $U$. Any $v \in V$ can be written into $$\{\inp{v}{e_1}e_1+\dots+\inp{v}{e_m}e_m\} + \{v - \inp{v}{e_1}e_1-\dots-\inp{v}{e_m}e_m\}.$$ Here $\inp{v}{e_1}e_1+\dots+\inp{v}{e_m}e_m \in U$, and one can easily show that for $w = v - \inp{v}{e_1}e_1-\dots-\inp{v}{e_m}e_m$, $\inp{w}{e_j} = \inp{v}{e_j} - \inp{v}{e_j} = 0$ (for all $j$ between 1 and $m$).
    
    Therefore, $w$ is orthogonal to $\s(e_1,\dots,e_m) = U$, meaning that $w \in U^\perp$. Since $U$ is a subspace and is thus nonempty, $U \cap U^\perp = \{0\}$, showing that $U + U^\perp$ is a direct sum.
    \begin{itemize}
        \item It follows directly that $\d U^\perp = \d V - \d U$.
    \end{itemize}
    \item Let $U$ again be a finite-dimensional subspace of $V$, then $U=(U^\perp)^\perp$. For the ``$\subseteq$'' direction, any $u \in U$ is orthogonal to any vectors $v \in U^\perp$, while \emph{all} vectors in $(U^\perp)^\perp$ are orthogonal to any $v \in U^\perp$. (Note that we can prove ``$\supseteq$'' this way because we do not know if $U$ contains all the vectors orthogonal to $U^\perp$.)
    
    For the ``$\supseteq$'' direction, suppose $v \in (U^\perp)^\perp \subseteq V$. Then $v$ can be uniquely written as $u+w$, where $u \in U$ and $w \in U^\perp$. We have just proved that $U \subseteq (U^\perp)^\perp$, meaning that $u \in (U^\perp)^\perp$, subspace of $V$. Therefore, $w = v-u \in (U^\perp)^\perp$ and $w \in U^\perp$ simultaneously. Since $U$ is nonempty, $w = 0$ and thus $v = u \in U$.
    \item The \df{orthogonal projection} $P_U$ of $V$ to its finite-dimensional subspace $U$ is the function $P_U: V \to U$ defined by $P_U(v) = u$, where $u \in U$ and $w \in U^\perp$ are the pair of \emph{unique} elements such that $v = u+w$. It is easy to verify $P_U \in \LV$ (note that $U \subseteq V$).
    \item Also, for orthonormal basis $e_1,\dots,e_m$, $P_U(v) = \inp{v}{e_1}e_1+\dots+\inp{v}{e_m}e_m$, which we have showed earlier. In general, if we want to find $P_U(v)$, we should first use Gram-Schmidt process to find an orthonormal basis of $U$ and then use the expression above the find $P_U(v)$.
    \item We give a list of properties of $P_U$ below.
    \begin{enumerate}[label=(\alph*)]
        \item $\forall u \in U, P_U(u) = u$; [for $u = u+0$]
        \item $\forall w \in U^\perp, P_U(w) = 0$; [for $w = 0+w$]
        \item $\r P_U = U$; [for $\r P_U \subseteq U$ by definition, and (a) above; \\ thus $P_U$ is surjective onto $U$]
        \item $v - P_U(v) \in U^\perp$; [true because $w = v - P_U(v)$ is in $U^\perp$]
        \item $\n P_U = U^\perp$; [$U^\perp \subseteq \n P_U$ by (b); also, \\ for $v$ such that $P_U(v) = 0$, meaning that $v - 0 = v \in U^\perp$ by (c)]
        \item $P_U$ is an idempotent operator: ${P_U}^2 = P_U$; [by (b)]
        \item $\nm{P_U(v)} \leq \nm{v}$; [since $\nm{u}^2 \leq \nm{u}^2 + \nm{w}^2 = \nm{v}^2$]
    \end{enumerate}
    \item For finite-dimensional subspace $U$ of an inner product space $V$ and a fixed $v \in V$, the minimum distance between $v$ and all vectors on $U$ is achieved at $u = P_U(v)$ (so that $v - u$ is orthogonal to the subspace $U$), as expected. Formally, we want to show that for all $u \in U$, $$\nm{v - P_U(v)} \leq \nm{v-u},$$ which achieves equality iff $u = P_U(v)$.
    
    The proof obviously employs the Pythagorean theorem. We have
    \begin{equation*}
        \nm{v - P_U(v)}^2 \leq \nm{v - P_U(v)}^2+\nm{P_U(v) - u}^2 = \nm{v - u}^2,
    \end{equation*}
    because $v - P_U(v) \in U^\perp$ and $P_U(v) - u \in U$, so that the two are orthogonal. The equality condition is obvious.
\end{itemize}


\newpage
\section{Operators on Inner Product Spaces}
\textit{Remark.} In this chapter we regard $V$ by default as FDVS over $\F = \R$ or $\C$.
\subsection{Self-Adjoint and Normal Operators}
\begin{itemize}
    \item For $T \in \LVW$, the \df{adjoint} of $T$ is the function $T^*: W \to V$ such that $$\inp{Tv}{w}_W = \inp{v}{T^*w}_V.$$
    
    Pay attention to the fact that the inner product on the left is on $W$, while the inner product on the right is on $V$. What we have above is well-defined by the Riesz representation theorem in \lk{6}{2}. Fix $w$, then $\inp{Tv}{w}$ becomes a linear functional $\psi_{T,w}(v)$ on $V$ (because $T$ is linear), which uniquely determines a $u \in V$ such that $\inp{v}{u} = \psi_{T,w}(v)$. This unique $u$ is our $T^*w$. Thus, this $T^*$ always exists and is unique given $T$.
    
    To calculate $T^*$ based on $T$, fix the second slot $w$ in $\inp{Tv}{w}$ and convert $\inp{Tv}{w}_W$ into the form $\inp{v}{f(w)}_V$, where the function $f$ of $w$ is our desired $T^*$.
    \item As expected, $T^* \in \mathcal{L}(W,V)$, since
    \begin{align*}
        \inp{v}{T^*(\lambda w_1 + w_2)} = \inp{Tv}{\lambda w_1 + w_2} & = \conj{\lambda}\inp{Tv}{w_1} + \inp{Tv}{w_2} \\ & = \conj{\lambda}\inp{v}{T^*w_1} + \inp{v}{T^*w_2} \\ & = \inp{v}{\lambda T^* w_1 + T^*w_2}.
    \end{align*}
    The above is true for all $v$, and thus the second slot must be the same, showing that $T^*$ is linear.
    \item Here are some properties of the adjoint. Move the adjoint from the second slot to the first slot and then back to the second slot will give these results.
    \begin{enumerate}[label=(\alph*)]
        \item $(S+T)^* = S^* + T^*$;
        \item $(\lambda T)^* = \conj{\lambda}T^*$;
        \item $(T^*)^* = T$;
        \item $I^* = I$;
        \item $(ST)^* = T^*S^*$ for all $T \in \LVW$ and $S \in \mathcal{L}(W,U)$.
    \end{enumerate}
    We prove (c) here and omit the others. For any $v \in V$ and $w \in W$, $$\inp{w}{(T^*)^*v} = \inp{T^*w}{v} = \conj{\inp{v}{T^*w}} = \conj{\inp{Tv}{w}} = \inp{w}{Tv}.$$ Because the first slot $w$ is fixed, the second slot must be the same for all $v$ and hence $(T^*)^*=T$.
    
    As you may have noticed from (a), (b), \& (c), the map $^*: \LVW \to \mathcal{L}(W,V)$ turns out to be a \textbf{conjugate-linear bijective map}. The map is injective because $$S^*=T^* \implies S=(S^*)^*=(T^*)^*=T;$$ and the map is surjective because for any $S \in \mathcal{L}(W,V)$, we always have $S^* \in \LVW$ such that $(S^*)^*=S$.
    \item The nullspace and range of the adjoint of $T$ is connected to the orthogonal complement of $\n T$ and $\r T$. For $T \in \LVW$, we have
    \begin{enumerate}[label=(\alph*)]
        \item $\n T^* = (\r T)^\perp$;
        \item $\r T = (\n T^*)^\perp$; [take the orthogonal complement of both sides in (a)]
        \item $\n T = (\r T^*)^\perp$; [replace $T^*$ in (a) by $(T^*)^* = T$]
        \item $\r T^* = (\n T)^\perp$. [take the orthogonal complement of both sides in (d)]
    \end{enumerate}
    For (a), $w \in \n T^* \iff T^*(w) = 0 \iff \inp{v}{T^*(w)} = 0$ for all $v \in V \iff \inp{Tv}{w} = 0$ for all $v \in V \iff w \in (\r T)^\perp$.
    \item With respect to an orthonormal basis $e_1,\dots,e_n$ of $V$ and an orthonormal basis $f_1,\dots,f_m$ of $W$, $\M(T^*,(f_1,\dots,f_m),(e_1,\dots,e_n))$ is the conjugate transpose of $\M(T,(e_1,\dots,e_n),(f_1,\dots,f_m))$. (If we are considering operators $T \in \LV$ instead, by default we use the same basis on the domain $V$ and on the range of $V$.)
    
    The entries in $k$-th column of $\M(T)$ are exactly the coefficients in
    $$\inp{Te_k}{f_1}f_1+\dots+\inp{Te_k}{f_m}f_m = Te_k.$$ Therefore, $\M(T)_{j,k} = \inp{Te_k}{f_j}$. Similarly we can conclude that $\M(T^*)_{k,j} = \inp{T^*f_j}{e_k} = \inp{f_j}{Te_k}$, where $1 \leq j \leq m$ and $1 \leq k \leq n$. It is clear that $\M(T)$ and $\M(T^*)$ (with switched bases) are conjugate transposes of one another.

    Note that in the usual case when we are considering an operator $T$ with respect to a single basis, then $\M(T)$ and $\M(T^*)$ are simply conjugate transposes of one another (because we do not need to switch bases).
    \item An operator $T \in \LV$ is \df{self-adjoint} (or \df{Hermitian}) if $T=T^*$. That is to say, $T$ is self-adjoint iff $\inp{Tv}{w}=\inp{v}{Tw}$ for all $v,w \in V$.
    \begin{itemize}
        \item It follows from definition that the sum of two self-adjoint operators is self-adjoint and the product of a real scalar and a self-adjoint operator is self-adjoint.
        \item As the book points out, over $\C$ we can draw a parallel between the adjoint of an operator and the complex conjugation of a number. In some abstract sense saying that an operator is self-adjoint is similar to saying that a number is equal to its conjugation and is thus real. Self-adjoint operators are closely tied to real numbers, as we will see.
    \end{itemize}
    \item Eigenvalues of self-adjoint operators are real. Consider the self-adjoint operator $T$ on $V$ and $Tv = \lambda v$ for nonzero $v \in V$, then $$\lambda \nm{v}^2 = \inp{\lambda v}{v} = \inp{Tv}{v} = \inp{v}{Tv} = \inp{v}{\lambda v} = \conj{\lambda}\nm{v}^2,$$ telling us $\lambda = \conj{\lambda}$.
    \item ($\dagger$) For a \textbf{complex} inner product space $V$ and $T \in \LV$, if $\inp{Tv}{v}=0$ for all $v \in V$, then $T = 0$. We prove this via some calculation tricks: for all $u,w \in V$,
    \begin{align*}
        \inp{Tu}{w} = & \frac{\inp{T(u+w)}{u+w} - \inp{T(u-w)}{u-w}}{4} \\ & + i \frac{\inp{T(u+iw)}{u+iw} - \inp{T(u-iw)}{u-iw}}{4}.
    \end{align*}
    Therefore, $\inp{Tu}{w} = 0$ for all $u,w \in V$. Taking $w = Tu$ gives us $T = 0$.
    \item For operator $T$ on a \textbf{complex} inner product spaces $V$, $T$ is self-adjoint iff for every $v \in V$, $\inp{Tv}{v} \in \R$.
    
    Let us look at the equation (this is the general trick for proving something is real)
    $$\inp{Tv}{v}-\conj{\inp{Tv}{v}}=\inp{Tv}{v}-\inp{v}{Tv}=\inp{(T-T^*)v}{v}.$$ If $\inp{Tv}{v} \in \R$ for every $v$, then $T-T^*=0$ by ($\dagger$). If in the other direction $T=T^*$, then $\inp{Tv}{v}=\conj{\inp{Tv}{v}}$ and is thus real for every $v$.
    \item If $T = T^*$ on a (real or complex) inner product space $V$ and $\inp{Tv}{v}=0$ for all $v$, then $T = 0$. By ($\dagger$), now we only have to check this fact on the real inner product space. As one may check, $$\inp{Tu}{w} = \frac{\inp{T(u+w)}{u+w}-\inp{T(u-w)}{u-w}}{4}.$$ We let $w = Tu$ and get $T = 0$.
    \item An operator $T$ on $V$ is \df{normal} if $TT^* = T^*T$. Obviously self-adjoint operators $T = T^*$ are normal.
    \item $T$ is normal iff $\nm{Tv}=\nm{T^*v}$ for all $v \in V$.
    
    To prove this, first note that $TT^* - T^*T$ is self-adjoint because $(TT^*-T^*T)^*=(TT^*)^*-(T^*T)^*=TT^* - T^*T$. Therefore,
    \begin{align*}
        TT^*-T^*T=0 & \iff \inp{(TT^*-T^*T)v}{v}=0 \text{ for all } v \in V \\ & \iff \inp{TT^*}{v} = \inp{T^*T}{v} \text{ for all } v \in V \iff \nm{T^*v}=\nm{Tv}.
    \end{align*}
    \begin{itemize}
        \item As a corollary, $Tv = 0 \iff T^*v = 0$, showing that $\n T = \n T^*$.
    \end{itemize}
    \item All the eigenvalues of $T \in \LV$ are exactly the complex conjugates of all the eigenvalues in $T^*$. We basically need to prove that $\lambda$ is an eigenvalue of $T$ iff $\conj{\lambda}$ is an eigenvalue of $T^*$:
    \begin{align*}
        \lambda \text{ is not an eigenvalue of } T & \iff T - \lambda I \text{ is injective} \\ & \iff \n (T-\lambda I) = \{0\} \\ & \iff \r (T-\lambda I)^* = \{0\}^\perp = V \\ & \iff (T-\lambda I)^* = T^* - \conj{\lambda} I \text{ is surjective} \\ & \iff \conj{\lambda} \text{ is not an eigenvalue of $T^*$}.
    \end{align*}
    \item If additionally $T$ is normal, $T$ and $T^*$ share the same set of eigenvectors. Suppose $T$ has eigenvector $v$ with $T^*v = \lambda v$, then $T^*$ has the same eigenvector $v$ with $Tv = \conj{\lambda}v$.
    
    We first check $T-\lambda I$ is normal given $T$ is normal:
    \begin{equation*}
        (T-\lambda I)(T-\lambda I)^* = (T-\lambda I)(T^* - \conj{\lambda} I) = (T^* - \conj{\lambda} I)(T-\lambda I) = (T-\lambda I)^* (T-\lambda I).
    \end{equation*}
    Then, $0 = \nm{(T-\lambda I)v} = \nm{(T-\lambda I)^*v} = \nm{(T^* - \conj{\lambda} I)v}$, showing that $v$ is an eigenvector of $T^*$ corresponding to the eigenvalue $\conj{\lambda}$.
    \item Eigenvectors corresponding to distinct eigenvalues of the normal operator $T$ are orthogonal. Consider $Tu = \alpha u$ and $Tv = \beta v$ with $\alpha \not= \beta$. Also, by the last theorem, $T^*v=\conj{\beta}v$. If we now look at $(\alpha - \beta)\inp{u}{v}$, we have
    \begin{equation*}
        \alpha\inp{u}{v}-\beta\inp{u}{v}=\inp{Tu}{v}-\inp{u}{T^*v} = 0,
    \end{equation*}
    telling us that $u$ and $v$ are orthogonal.
\end{itemize}

\subsection{The Spectral Theorem}
Now we extend the diagonalizability theorem proved in Chapter 5 from general vector spaces to inner product spaces. 
\begin{itemize}
    \item The complex spectral theorem says that for operator $T$ on an complex inner product space $V$, the following are equivalent:
    \begin{enumerate}[label=(\alph*)]
        \item $T$ is normal.
        \item $V$ has an orthonormal basis consisting of eigenvectors of $T$.
        \item $T$ has a diagonal matrix with respect to some orthonormal basis of $V$.
    \end{enumerate}
    
    The equivalency between (b) and (c) is by looking at the diagonal matrix with respect to an orthonormal basis, as we did back in \lk{5}{3}.
    
    (c)$\implies$(a): If $\M(T,(e_1,\dots,e_n))$ is diagonal, then $\M(T^*,(e_1,\dots,e_n))$ would be the conjugate transpose $\M(T)$ and is thus still diagonal. Because diagonal matrices commute, $$\M(TT^*)=\M(T)\M(T^*)=\M(T^*)\M(T)=\M(T^*T),$$ showing that $T$ is a normal operator.
    
    (a)$\implies$(c): Given $T$ is normal, by Schur's theorem (since $\F = \C$) we can find an orthonormal basis $e_1,\dots,e_n$ such that
    \begin{equation*}
        \M(T)=
        \begin{bmatrix}
            a_{11} & \cdots & a_{1n} \\
            & \ddots & \vdots \\
            0 & & a_{nn}
        \end{bmatrix}.
    \end{equation*}
    The idea is to show all the entries above the diagonal are 0. From the first column of $\M(T)$ we have $\nm{Te_1}^2=\abs{a_{11}}^2$, while from the first column of $\M(T^*)$ (the conjugate of the first row of $\M(T)$) we have $$\nm{T^*e_1}^2=\abs{a_{11}}^2+\dots+\abs{a_{1n}}^2.$$ Since $\nm{Te_1}^2 = \nm{T^*e_1}^2$, $a_{12},\dots,a_{1n}$ are all 0.
    
    We can proceed in this way to show that $a_{23},\dots,a_{2n}$ are also all 0 and so on. Therefore, $\M(T)$ is indeed a diagonal matrix, as desired.
\end{itemize}
\textit{Remark.} Schur's theorem gives us a shortcut to prove (a)$\implies$(c) by comparing $\M(T)$ and $\M(T^*)$. By contrast, in the spectral theorem for $T$ on real inner product spaces, (a)$\implies$(c) is not easy to prove because in the first place we do not know over $\R$ (unlike over $\C$) whether $T$ has an upper-triangular matrix with respect to some basis of $V$. Neither do we know whether any eigenvalue exists for $T \in \LV$ over $\R$.

Therefore, a stronger condition on $T$ is required for the similar spectral theorem on real inner product spaces, and as you may guess, $T$ is now required to be self-adjoint in (a). We leave (b) and (c) unchanged in the real spectral theorem. We can show that if $T$ is self-adjoint, then $T$ must have an eigenvalue, from which we can prove that (a)$\implies$(b).
    \begin{itemize}
    \item Self-adjoint operators has at least an eigenvalue. Recall how we proved operators on complex vector spaces always have an eigenvalue in \lk{5}{2}. The approach is exactly the same, but we use the polynomial factorization theorem over $\R$ instead of $\C$.
    \begin{itemize}
        \item We can show that for self-adjoint $T \in \LV$, $T^2+bT+CI$ is injective under $b,c\in\R$, $b^2-4c<0$. For an arbitrary nonzero $v \in V$,
        \begin{align*}
            \inp{(T^2+bT+CI)v}{v} & = \inp{T^2v}{v}+b\inp{Tv}{v}+c\inp{v}{v} \\
            & = \inp{Tv}{Tv}+b\inp{Tv}{v}+c\inp{v}{v} \\
            & \geq \nm{Tv}^2+\nm{v}^2 -b\nm{Tv}\nm{v} \quad \text{by Cauchy-Schwarz}\\
            & = \left(\nm{Tv}-\frac{\abs{b}\nm{v}}{2}\right)^2 + \left(c - \frac{b^2}{4}\right)\nm{v}^2 > 0,
        \end{align*}
        showing that $(T^2+bT+CI)v \not= 0$ for all nonzero vector $v$ and is thus injective.
    \end{itemize}
    
    Once we have shown $T^2+bT+CI$ is injective, for any $v \in V$,
    \begin{align*}
        0 & = a_0+a_1Tv+\dots+a_nT^nv \\
        & = (a_0+a_1T+a_nT^n) v \\
        & = c(T^2+b_1T+c_1I)\cdots(T^2+b_MT+c_MI)(T-\lambda_1I)\cdots(T-\lambda_mI)v.
    \end{align*}
    Since the first $M$ terms in the factorization are all injective, one of the $\lambda$'s is our desired eigenvalue of $T$. For the full proof, see this section in the book.
    \item For self-adjoint $T \in \LV$ and the subspace $U$ of $V$ invariant under $T$, we have
    \begin{enumerate}[label=(\alph*)]
        \item $U^\perp$ is invariant under $T$;
        
        Let $v \in U^\perp$, then for all $u \in U$, we have \[\inp{Tv}{u}=\inp{v}{Tu}=0.\] Since this holds for all $u$, $Tv \in U^\perp$ and thus $U^\perp$ is invariant under $T$.
        \item $T|_U \in \mathcal{L}(U)$ is self-adjoint;
        
        For any $u,v \in U$,
        \[\inp{T|_U u}{v}=\inp{Tu}{v}=\inp{u}{Tv}=\inp{u}{T|_U v}.\]
        \item $T|_{U^\perp} \in \mathcal{L}(U)$ is self-adjoint.
        
        Since $U^\perp$ is invariant from (a), we can repeat what we did in proving (b).
    \end{enumerate}
    
    \item The real spectral theorem says that for operator $T$ on an real inner product space $V$, the following are equivalent:
    \begin{enumerate}[label=(\alph*)]
        \item $T$ is self-adjoint.
        \item $V$ has an orthonormal basis consisting of eigenvectors of $T$.
        \item $T$ has a diagonal matrix with respect to some orthonormal basis of $V$.
    \end{enumerate}
    
    For (b)$\implies$(c), check out \lk{5}{3}.
    
    For (c)$\implies$(a), the diagonal matrix $\M(T)$ is equal to its transpose $\M(T^*)$, meaning that $T = T^*$.
    
    (a)$\implies$(b) by induction on $\d V$:
    
    If $\d V = 1$, then since $T$ has an eigenvalue, we have an eigenvector of norm 1. This eigenvector itself is a basis of $V$.
    
    Suppose (a)$\implies$(b) holds for all real inner product spaces with dimension less than $n = \d V$ ($n > 1$). Given $T$ is self-adjoint, we must have an eigenvector $u$ with norm 1. $U \coloneqq \s(u)$ is thus invariant under $T$, and therefore $T|_{U^\perp}$ is self-adjoint, where $U^\perp$ is of dimension $n-1$.
    
    The inductive hypothesis suggests there is an orthonormal basis of $U^\perp$ consisting of eigenvectors of $T|_{U^\perp}$. Since $u$ has norm 1 and are perpendicular to these eigenvectors of $T|_{U^\perp}$, the combined list of vectors gives an orthonormal basis of $U$ (as $V=U\oplus U^\perp$), which completes the inductive step.
    
\end{itemize}

\subsection{Positive Operators and Isometries}
\begin{itemize}
    \item $T \in \LV$ is \df{positive} if $T$ is self-adjoint and $\inp{Tv}{v} \geq 0$ for all $v \in V$.
    \begin{itemize}
        \item For example, we can verify that $P_U$ is a positive operator on $V$.
    \end{itemize}
    \item An operator $R$ is a \df{square root} of $T$ if $R^2 = T$.
    \item For $T \in \LV$, the following are equivalent:
    \begin{enumerate}[label=(\alph*)]
        \item $T$ is positive;
        \item $T$ is self-adjoint and all eigenvalues of $T$ are nonnegative;
        \item $T$ has a positive square root;
        \item $T$ has a self-adjoint square root;
        \item there is an operator $R$ such that $T = R^* R$.
    \end{enumerate}
    We provide some key steps to the proof below.
    
    (a)$\implies$(b): $T$ is positive implies $T$ is self-adjoint, which implies it must have an eigenvalue $\lambda$. Then for an eigenvector $v$ such that $Tv = \lambda v$, $$0 \leq \inp{Tv}{v} \leq \inp{\lambda v}{v} \leq \lambda\inp{v}{v}.$$
    
    (b)$\implies$(c): By the spectral theorems we have an orthonormal basis $e_1,\dots,e_n$ corresponding to nonnegative eigenvalues $\lambda_1,\dots,\lambda_n$. Construct $R \in \LV$ such that $Re_j = \sqrt{\lambda_j} e_j$ for every $e_j$. Then $R^2e_j = \lambda e_j = Te_j$, so that $R^2 = T$. Show that $R$ is a positive operator by the definition completes the proof.
    
    (c)$\implies$(d)$\implies$(e) is trivial.
    
    (e)$\implies$(a): $T^* = (R^*R)^* = R^*R = T$, and $\inp{Tv}{v}=\inp{Rv}{Rv} \geq 0$.
    
    \item Each positive operator $T$ on $V$ has a \textbf{unique} positive square root, which we will denote by $\sqrt{T}$ in place of $R$. From the proof of (b)$\implies$(c) above, we then have for any eigenvector $v$ and its corresponding eigenvalue $\lambda$ of $T$, $\sqrt{\lambda}$ is the eigenvalue of $\sqrt{T}$ corresponding to the same eigenvector $v$.
    
    The idea is simple, for positive operator $T$, each eigenvalue $v$ of $T$ has $Tv = \lambda v$ for some nonnegative $\lambda$. Once we show $Rv = \sqrt{\lambda}v$ for $R^2=T$, since the spectral theorems say that $V$ has a basis consisting entirely of these eigenvalues of $T$, we can show that $R$ is the unique square root.
    
    By the spectral theorem, there is an orthonormal basis $e_1,\dots,e_n$ consisting of eigenvectors of the positive operator $R$. Let $\sqrt{\lambda_1},\dots,\sqrt{\lambda_n}$ be the corresponding eigenvalues. If we now consider 
    $$v = a_1 e_1+\dots+a_ne_n,$$ applying $R^2$ to both sides gives $$R^2v = a_1 \lambda_1 e_1 + \dots + a_n \lambda_n e_n,$$ while applying $T$ to both sides gives 
    $$Tv = a_1 \lambda e_1 +\dots+ a_n \lambda e_n.$$ The two expressions above are equal, and since $e_1,\dots,e_n$ is a basis, we have $a_j(\lambda - \lambda_j) = 0$ for all $j$. Thus, 
    $$v = \sum_{\{j: \lambda_j=\lambda\}}a_j e_j, \text{ and then } Rv = \sum_{\{j: \lambda_j=\lambda\}} a_j \sqrt{\lambda} e_j = \sqrt{\lambda}v.$$
    
    \item An operator $S \in \LV$ is an \df{isometry} if $\nm{Sv}=\nm{v}$ for all $v \in V$. (an operator that preserves norm)
    \item There are many equivalent characterizations of isometries. The following are equivalent for $S \in \LV$:
    \begin{enumerate}[label=(\alph*)]
    \item $S$ is an isometry;
    \item $\inp{Su}{Sv} = \inp{u}{v}$ for all $u,v \in V$; (\emph{This says that norm-preserving is equivalent to inner-product-preserving.})
    \item $Se_1,\dots,Se_n$ is orthonormal for every orthonormal list of vectors $e_1,\dots,e_n$ in $V$;
    \item there exists an orthonormal basis $e_1,\dots,e_n$ of $V$ such that $Se_1,\dots,Se_n$ is orthonormal;
    \item $S^*S = I$;
    \item $SS^* = I$;
    \item $S^*$ is an isometry;
    \item $S$ is invertible and $S^{-1} = S^*$.
    \end{enumerate}
    
    (a)$\implies$(b) because inner products can be calculate from norms. On real and complex inner product spaces, respectively,
    \begin{equation*}
        \inp{u}{v} = \frac{\nm{u+v}^2-\nm{u-v}^2}{4} \quad \text{and} \quad \inp{u}{v} = \frac{\nm{u+v}^2-\nm{u-v}^2+\nm{u+iv}^2i+\nm{u-iv}^2i}{4}.
    \end{equation*}
    (b)$\implies$(c): replace $u$ and $v$ by $e_j$ and $e_k$
    
    (c)$\implies$(d): trivial
    
    (d)$\implies$(e): For orthonormal basis of $e_1,\dots,e_n$ that makes $Se_1,\dots,Se_n$ orthonormal, $$\inp{S^*Se_j}{e_k}=\inp{e_j}{e_k}.$$ By the fact that all $u,v \in V$ can be expressed as a linear combination of these $e$'s, one can show $\inp{S^*Su}{v}-\inp{u}{v} = \inp{(S^*S-I)u}{v} = 0$. If we set $v = (S^*S-I)u$, we will have $\nm{(S^*S-I)u}=0$ for all $u$, and thus $S^*S=I$.
    
    (e)$\implies$(f): It is not hard to prove the general case that $ST = I$ iff $TS =I$ for operators $S,T \in \LV$.
    
    (f)$\implies$(g) is standard: $\nm{S^*v}^2=\inp{SS^*v}{v} = \inp{Iv}{v} = \nm{v}^2$.
    
    (g)$\implies$(h): We can replace $S$ by $S^*$ in (a) and similarly get $SS^*=I$ [from (a)$\implies$(e)] and $S^*S=I$ [from (a)$\implies$(f)], showing that $S$ is invertible with $S^{-1}=S^*$.
    
    (h)$\implies$(a) is again standard: $\nm{Sv} = \inp{S^*Sv}{v} = \inp{Iv}{v} = \nm{v}^2$.
    \begin{itemize}
        \item An isometry on a real (resp.\ complex) inner product space is often called an \df{orthogonal} (resp.\ \df{unitary}) \df{operator}.
        \item Every isometry is normal by $(e)$ and $(f)$, and actually we can extend properties of normal operators to describe properties of isometry. The complex case is given below and the real case will be given later.
    \end{itemize}
    \item If there is an orthonormal basis $e_1,\dots,e_n$ of $V$ all being eigenvectors of $S \in \LV$, with corresponding eigenvalues $\lambda_1,\dots,\lambda_n$ all having absolute value 1, then $S$ must be an isometry.
    
    First, $Sv = \inp{v}{e_1}Se_1 + \dots + \inp{v}{e_n}Se_n = \lambda_1\inp{v}{e_1}e_1 + \dots + \lambda_n\inp{v}{e_n}$. Since the $\lambda$'s all have absolute value 1, $\nm{Sv} = \abs{\inp{v}{e_1}}^2 + \dots + \abs{\inp{v}{e_n}}^2 = \nm{v}^2$.
    \item If we let $V$ be a \textbf{complex} inner product space, then the above example $S$ is the only type of isometry allowed.
    
    For an isometry $S \in \LV$ over $\C$, since $S$ is normal, by the complex spectral theorem we know that there is an orthonormal basis $e_1,\dots,e_n$ consisting of eigenvectors of $S$. For every $j$ between 1 and $n$, the eigenvalue $\lambda_j$ corresponding to $e_j$ has \[\abs{\lambda_j}=\nm{\lambda_je_j}=\nm{Se_j}=\nm{e_j}=1.\]
    Therefore, under $\C$, $S$ is an isometry is equivalent to saying that there is an orthonormal basis of $V$ consisting of eigenvectors of $S$ with corresponding eigenvalues all of absolute value 1.
\end{itemize}


\subsection{Polar Decomposition and Singular Value Decomposition}
\textit{Remark.} In the fourth edition, this section is written as well. SVD will instead be used to prove the polar decomposition theorem.
\begin{itemize}
\item The polar decomposition theorem states that for $T \in \LV$, we can find an isometry $S$ such that $T = S \sqrt{T^*T}$. (breaking $T$ into a composition of $S$ with $\sqrt{T^*T}$)

Note that $T^*T$ by its form is already a positive operator, so we know it has a positive square root.

There is a good analogy of this theorem with the polar decomposition of a complex number. For nonzero $z \in \C$, we have $z = \left(\frac{z}{\abs{z}}\right)\sqrt{\conj{z}z}$, where the first factor is a point on the unit circle and the second factor resembles $T^*T$. To prove the theorem, we need several steps:
\begin{itemize}
    \item \textit{Step I}. Define the function $S_1: \r \sqrt{T^*T} \to \r T$ by $S_1(\sqrt{T^*T}v)=Tv$.
    
    What is special about $\sqrt{T^*T}$ is that $\nm{Tv}=\nm{\sqrt{T^*T}v}$ for all $v$, and one uses this to prove that the function $S_1$ is well-defined and injective. One should also verify that $S_1$ is a linear map and thus a linear isometry. The linear map is surjective by definition.
    
    \item \textit{Step II}. Our idea is to extend $S_1$ on $\r T$ to $S$ on the entire $V$. 
    
    Because $S_1: \r \sqrt{T^*T} \to \r T$ is now bijective, $\d \sqrt{T^*T} = \d \r T$, from which we know $\d(\r \sqrt{T^*T})^\perp = \d(\r T)^\perp$ because both $T^*T$ and $T$ are linear operators on $V$. Thus we have an orthonormal basis $e_1,\dots,e_m$ of $(\r \sqrt{T^*T})^\perp$ and another orthonormal basis $f_1,\dots,f_m$ of $(\r T)^\perp$. If we let $S_2(e_1) = f_1,\dots,S_2(e_m)=f_m$, the fact that the $e$'s and the $f$'s are two orthonormal bases gives us an isometry $S_2$.
    
    \item \textit{Step III}. Merge $S_1$ and $S_2$ into an isometry $S$ on $V$.
    
    Because $v$ can be unique written into as the sum of $u \in \r \sqrt{T^*T}$ and $w \in (\r \sqrt{T^*T})^\perp$, defining $$Sv = S_1 u+ S_2 w$$ gives $S(\sqrt{T^*T}v) = S_1(\sqrt{T^*T}v) = Tv$ for all $v \in V$, so that $T=S\sqrt{T^*T}$. Furthermore, by the Pythagorean theorem and $S_1$ and $S_2$ are  isometries themselves, one can show $\nm{Sv}^2=\nm{v}^2$ for all $v$.
\end{itemize}
\item The \df{singular values} of $T \in \LV$ are the eigenvalues of $\sqrt{T^*T}$, which each eigenvalue repeating $\d E(\lambda, \sqrt{T^*T})$ times.

\begin{itemize}
\item Naturally the singular values should all be nonnegative because $\sqrt{T^*T}$ is a positive operator (the positive square root of the positive operator $T^*T$).
\item Also, recall that the sum of the dimensions of all eigenspaces is the dimension of the whole space, the number of singular values of $T \in \LV$ is equal to $\d V$.
\end{itemize}

\item The singular values of $T \in \LV$ are the nonnegative square roots of the eigenvalues of $T^*T$, with each of these eigenvalues repeated $\d E(\lambda,T^*T)$ times.

$T^*T$ is positive, so by the spectral theorems there is an orthonormal basis $e_1,\dots,e_n$ and nonnegative $\lambda_1,\dots,\lambda_n$ such that $T^*Te_j = \lambda_j e_j$ for every $j$. Thus $\sqrt{T^*T}e_j=\sqrt{\lambda_j}e_j$, which we emphasized in the last section. Note that this establishes a one-to-one correspondence between every $\lambda_j$ and every $\sqrt{\lambda_j}$, and hence $\d E(\lambda, T^*T) = \d E(\lambda, \sqrt{T^*T})$.

\item The singular value decomposition theorem says the following: suppose $T \in \LV$ has singular values $s_1,\dots,s_n$, then there exists two orthonormal bases $e_1,\dots,e_n$ and $f_1,\dots,f_n$ of $V$ such that $$Tv = s_1\inp{v}{e_1}f_1 + \dots + s_n\inp{v}{e_n}f_n$$ for every $v \in V$.

The spectral theorems applied to the self-adjoint operator $\sqrt{T^*T}$ gives us an orthonormal basis $e_1,\dots,e_n$ such that $Te_j = s_j v_j$ for all $j$. Since for all $v$, $v = \inp{v}{e_1}e_1 + \dots + \inp{v}{e_n}e_n$, applying $T$ to both side leads to $$\sqrt{T^*T}v = s_1\inp{v}{e_1}e_1 + \dots + s_n\inp{v}{e_n}e_n.$$ Since we have isometry $S \in \LV$ such that $T = S \sqrt{T^*T}$ by the polar decomposition theorem, applying $S$ to both sides of the above equation leads further to $$S\sqrt{T^*T}v = s_1\inp{v}{e_1}Se_1 + \dots + s_n\inp{v}{e_n}Se_n.$$ Because $S$ is an isometry, we know that $f_1 \coloneqq Se_1,\dots,f_n \coloneqq Se_n$ is an orthonormal basis of $V$ as well. Therefore, we could simplify the equation above to $$Tv = s_1 \inp{v}{e_1}f_1 + \dots + s_n \inp{v}{e_n}f_n,$$ as desired.
\begin{itemize}
	\item This theorem introduces a new way to decompose an operator/matrix on an inner product space known as the \df{singular value decomposition} (SVD). Replace $v$ in the equation above by each individual $e_j$, and we have $Te_j = s_jf_j$ for every $j$ between 1 and $n$. Thus we have
	\begin{equation*}
		\M(T,(e_1,\dots,e_n),(f_1,\dots,f_n)) = \begin{bmatrix}
		s_1 & & 0 \\
		 & \ddots & \\
		 0 & & s_n
		\end{bmatrix},
	\end{equation*}
a diagonal matrix representation of $T$ with respect to \emph{two} different bases rather than one basis.
\end{itemize}
\end{itemize}


\newpage
\section{Operators on Complex Vector Spaces}
\textit{Remark.} We assume $V$ is a nonzero FDVS in this chapter, and $T \in \LV$. (Although some results are possible to be extended to the infinite-dimensional setting, it is not that useful here.) We take $n$ as $\d V$ by default in addition.
\subsection{Generalized Eigenvectors and Nilpotent Operators}
\begin{itemize}
    \item $\{0\} = \n T^0 \subseteq \n T^1 \subseteq \n T^2 \cdots \subseteq \n T^k \subseteq \n T^{k+1} \subseteq \cdots$ holds with a quite obvious proof.
        \item Furthermore, once $\n T^m = \n T^{m+1}$ for some $m \geq 0$, $\n T^m = \n T^{m+1} = \n T^{m+2} = \cdots$ and so on. To prove this we need to further show that $\n T^{m+k} \supseteq \n T^{m+k+1}$, which is not hard either.
        \begin{itemize}
            \item As a corollary, if $\n T = {0}$, then all powers of $T$ are injective.
            \item When do we hit the first $m$ such that $\n T^m = \n T^{m+1}$? This $m$ always exists, and $m \leq \d V$.
        \end{itemize}
    \item For $T$ on $V$ with dimension $n$, we have $\n T^n = \n T^{n+1} = \cdots$.
    
    For the proof, assume that $\n T^n \ne \n T^{n+1}$, then by the previous two claims we have
    $$\{0\} = \n T^0 \subsetneq \n T^1 \subsetneq \cdots \subsetneq \n T^n \subsetneq \n T^{n+1}.$$
    Every proper subset means the dimension of the next one on the right must be one more than the dimension of the previous one on the left. Therefore, at the point $\n T^n$ is at least of dimension $n$, from which we can increase no further.
    
    From now on we will call \[\{0\} = \n T^0 \subseteq \n T^1 \subseteq \cdots \subseteq \n T^n = \n T^{n+1} = \cdots\] the \emph{nullspace power chain}, in which \[\n T^0 \subsetneq \cdots \subsetneq \n T^k = \cdots = \n T^n\] for some $k$ between $0$ and $n$.
    
    \item \emph{Note that all the results above hold for the ranges of integer powers of $T$, with ``$\supseteq$'' replacing ``$\subseteq$'' accordingly. See the exercises of this section in the book.}
    \item Say $T$ is on $V$ with dimension $n$, $V = \n T^n \oplus \r T^n$. (The claim is actually true for all $m$ such that $T^m = T^{m+1}$.)
    The proof divides into two steps. First, we need to show that $\n T^n \cap \r T^n = \{0\}$. Second, we need to show that the direct sum is actually $V$.
    
    Suppose $v \in \n T^n \cap \r T^n$, then $T^n v= 0$ and there exists $v$ such that $T^n u=v$. Therefore, $$0 = T^{2n} v = T^n (T^n u) = T^{2n} u,$$ showing that $u \in \n T^{2n} = \n T^n$. Therefore $v = T^n u = 0$.
    
    Now we have $\d (\n T^n \oplus \r T^n) = \d\n T^n + \d\r T^n = \d V$. Since $\n T^n \oplus \r T^n$ is a subspace of $V$, the direct sum is $V$ itself.
    \item We now generalizes the concept of eigenvectors so that we will show soon that a complex vector space can be written into a direct sum of generalized eigenspaces. The \df{generalized eigenvector} of $T$ corresponding to eigenvalue $\lambda$ is a nonzero vector $v \in V$ such that $(T-\lambda I)^j v = 0$ for some $j \in \Z^+$. (Note that when generalizing eigenvectors, no new eigenvalues are introduced because $T - \lambda I$ being injective implies that $(T -\lambda I)^n$ is injective.)
    
    The \df{generalized eigenspace} of $T$ corresponding to $\lambda$ is the set $$G(\lambda, T) = \{0\} \cup \text{the set of generalized eigenvectors of $T$ corresponding to $\lambda$}.$$
    Obviously $E(\lambda, T) \subseteq G(\lambda, T)$. (The $E$ is actually a subspace of $G$ because $E$ is closed under the induced operations from $G$ again induced from $V$.) When $\lambda$ is not an eigenvalue, $G(\lambda, T) = \{0\}$.
    \item We now state that $G(\lambda, T) = \n (T - \lambda I)^n$, where $n = \d V$. (In particular, $G(0,T) = \n T^n$) You should already have an idea that the proof uses the nullspace power chain.
    \item A list of generalized eigenvectors corresponding to distinct eigenvalues still needs to be linearly independent. Let $\lambda_1,\dots,\lambda_m$ be the list of eigenvalues and $v_1,\dots,v_m$ be the list of corresponding eigenvectors we are considering. Suppose $0 = a_1v_1 + \dots + a_mv_m$.
    
    Let $k$ be the largest nonnegative integer such that $w \coloneqq (T-\lambda_1 I)^k v_1 \not= 0$ (clearly $k < n$). Then $$(T - \lambda_1 I)w = (T - \lambda_1 I)^{k+1} v_1 = 0,$$ which says that $Tw = \lambda_1 w$. Therefore for any $\lambda$, $(T-\lambda I) w = (\lambda_1 - \lambda) w$, which implies that $$(T-\lambda I)^n w = (\lambda_1 - \lambda)^n w.$$ Consider the operator $(T-\lambda_1 I)^k (T-\lambda_2 I)^n \cdots (T-\lambda_m I)^n$ and apply it the two sides of $0 = a_1v_1 + \dots + a_mv_m$, then all the terms on the RHS vanish except for the first one:
    \begin{align*}
    0 & = a_1 (T-\lambda_2 I)^n \cdots (T-\lambda_m I)^n (T-\lambda_1 I)^k v \\ & = a_1 (\lambda_1 -\lambda_2) \cdots (\lambda_1 - \lambda_m)^n w.
    \end{align*}
    Since $w$ is nonzero and the $\lambda_1$ is distinct from the other $\lambda$'s, $a_1 = 0$. In the same way, all the coefficients $a_i$ must be zero and thus the eigenvectors are linearly independent.
	\item A operator $N$ on a nonzero FDVS $V$ is \df{nilpotent} if $N^k = 0$ for some $k \in \Z^+$.
	\begin{itemize}
        \item A nilpotent operator cannot be injective/surjective.
        
        Similar to functions on finite sets, the composition of injective (surjective/bijective) linear maps must be injective (surjective/bijective) on a finite dimensional vector space. If $N$ is injective, i.e., $\n N = \{0\}$, then $\n N^k = \{0\} \neq V$.
	\item The only eigenvalue of a nilpotent operator is 0. Proof is obvious.
	\end{itemize}
	\item Given nilpotent operator $N$ on $V$, $N^{\d V} = 0$, again by the nullspace power chain. (The reverse direction is obviously true as well.)
	\item $\M(N)$ with respect to some basis of $V$ has the form $	\begin{bmatrix}
		0 & & \ast \\
		 & \ddots & \\
		 0 & & 0
	\end{bmatrix}$,
    a strictly upper-triangular matrix.
	
    By the nullspace power chain we can extend a basis of $\n N$ to a basis of $\n N^2$, which can be extended to $\n N^3$ and so on.
    The first few columns concerning $N(\text{basis vector of } \n N)$ must be all 0. The next few columns concerning $N(\text{basis vectors of } \n N^2)$ have nonzero entries strictly above the diagonal because $$N(\text{basis vector of } \n N^2) \in \n N, \text{ since }N^2v = N(Nv) = 0.$$ Thus, $N(\text{basis vector of } \n N^2)$ can be written into a linear combination of the previous vectors in $\n N$. Proceeding in this fashion gives us a strictly upper-triangular $\M(N)$.
\end{itemize}

\subsection{Decomposition of an Operator}
\begin{itemize}
    \item We noted before that $\n T$ and $\r T$ are invariant under $T$. It is true in general that for $p \in \PF$, $\n p(T)$ and $\r p(T)$ are invariant under $T$. 
	
    For any $v \in \n p(T)$, $(p(T))(Tv) = T(p(T)v) = T(0) = 0 \implies Tv \in \n p(T)$; for any $v = p(T)u$, $Tv = T(p(T)u) = p(T)(Tu) \in \r p(T)$.
    \item For operator $T$ on a \textbf{complex} vector space $V$ and its distinct eigenvalues $\lambda_1,\dots,\lambda_m$, we have
    \begin{enumerate}[label=(\alph*)]
        \item $V = G(\lambda_1,T) \oplus \cdots \oplus G(\lambda_m,T)$;
	\item $G(\lambda_j,T)$ is invariant under $T$ for all $j$;
	\item $(T-\lambda_j I)|_{G(\lambda_j,T)}$ is nilpotent for all $j$.
    \end{enumerate}
	
    For (b), given $p(z) = (z-\lambda_j)^n$, we know $\n (T - \lambda_j I)^n = G(\lambda_j, T)$ is invariant under $T$. (c) holds straight from definition.
	
    For (a), we prove by induction on $n = \d V$. If $n = 1$, then all nonzero vectors of $V$ are eigenvectors of $V$. Thus $V = G(\lambda_1,T)$. For $n > 1$, suppose (a) holds for all complex vector spaces of dimension less than $n$. Then 
    \begin{equation}
        V = \n (T-\lambda I)^n \oplus \r (T-\lambda_1)^n = G(\lambda_1,T)\oplus U,
    \end{equation}
    if we let $U = \r (T-\lambda_1)^n$. This $U$ is invariant under $T$ by applying the last theorem to $p(z) = (z-\lambda_1)^n$. Since $G(\lambda_1,T) \ne \{0\}$, we have $\d U < n$, so that we can (a) is true in $T|_U \in \mathcal{L}(U)$: \[U = G(\lambda_1,T|_U) \oplus \cdots \oplus G(\lambda_m,T|_U).\] Since $G(\lambda_1,T)$ contains all generalized eigenvectors corresponding to $\lambda_1$ by definition, the eigenvectors of $T|_U$ can only correspond to $\lambda_2,\dots,\lambda_m$.

    We now intend to prove that $G(\lambda_k,T|_U) = G(\lambda_k,T)$ for all $k$ between 2 and $m$, i.e., $U$ contains all the generalized eigenvectors of $T$ corresponding to $\lambda_k$. ``$\subseteq$'' is clear because, and we prove ``$\supseteq$'': suppose $v \in G(\lambda_k,T) \subseteq U$, then by (3) above, $v = v_1 + u$ for some $v_1 \in G(\lambda_1,T)$ and $u \in U$. Furthermore, $u$ can broken up into $v_2+\dots+v_m$ for each $v_j \in G(\lambda_j,T|_U) \subseteq G(\lambda_j,T)$ [which we proved just now], so that $v = v_1 + v_2 + \dots + v_m$. Because generalized eigenvectors corresponding to distinct eigenvalues of linearly independent, all the $v_1$ to $v_m$ on the RHS must be 0 except for $v_k$. In particular $v_1 = 0$, so that $v = u \in U$. Therefore, $v \in G(\lambda_k,T)$, as desired.
    \begin{itemize}
        \item Note that if $T$ is diagonalizable, since $E(\lambda_j,T) \subseteq G(\lambda_j,T)$, comparing the (a) here and $V = E(\lambda_1,T) \oplus \dots \oplus E(\lambda_m,T)$ gives us $E(\lambda_j,T) = G(\lambda_j,T)$ for all $j$, or otherwise the $G$'s would not give a direct sum.
    \end{itemize}
    \item Because all the generalized eigenspaces $G(\lambda_j,T)$ are vector spaces, we can choose a basis from each of $G(\lambda_1,T)$ to $G(\lambda_m,T)$. Put all these bases together and by (a), we have a basis of $V$ consisting of generalized eigenvectors of $T$.
    \item For an eigenvalue $\lambda$ of $T \in \LV$, its \df{algebraic multiplicity} is defined as $\d G(\lambda,T) = \d \n (T-\lambda I)^{\dim V}$, and its \df{geometric multiplicity} is defined as $\d E(\lambda,T) = \d \n (T-\lambda I)$. By default, when the book says ``multiplicity'' it means specifically the algebraic multiplicity.
    \begin{itemize}
        \item Since $V = G(\lambda_1,T) \oplus \cdots \oplus G(\lambda_m,T)$ for operator $T$ on a \textbf{complex} vector space $V$, $\d V$ is the sum of multiplicities of all eigenvalues of $T$.
    \end{itemize}
    \item A \df{block diagonal matrix}, as its name tells, is a matrix of the form $\begin{bmatrix}
	A_1 & & 0 \\ & \ddots & \\ 0 & & A_m
    \end{bmatrix}$, where the $A_i$'s are square matrices on the diagonal and all other entries outside these $A_j$'s are 0. The block diagonal matrix is often related to the direct sum of invariant subspaces, as its form indicates.
    \item We give the block diagonal matrix (with upper triangular blocks) decomposition theorem. Suppose $T$ is an operator over a complex vector space $V$. For the list of all distinct eigenvalues $\lambda_1, \dots, \lambda_m$ of $T$, each with corresponding multiplicity $d_1,\dots,d_m$. Then there exists a basis of $V$ with respect to which $T$ has a block diagonal matrix of the form $\begin{bmatrix}
	A_1 & & 0 \\ & \ddots & \\ 0 & & A_m
    \end{bmatrix}$, with each $A_j$ being upper-triangular and having $\lambda_j$ on all diagonal entries, i.e., $A_j = \begin{bmatrix}
	\lambda_j & & \ast \\ & \ddots & \\ 0 & & \lambda_j
    \end{bmatrix}$.

    Since each $(T-\lambda_j I)|_{G(\lambda_j,T)}$ is nilpotent, its matrix with respect to some basis of $G(\lambda_j,T)$ is strictly upper-triangular. Therefore, $(T-\lambda_j I)|_{G(\lambda_j,T)} + \lambda_j I|_{G(\lambda_j,T)}$ is in the form of each $A_j$. Combining all these bases together gives a block diagonal matrix $\M(T)$ described above.
    \item As a preliminary lemma to the upcoming theorem, for nilpotent operator $N \in \LV$, $I+N$ has a square root.
	
    The Taylor expansion of $\sqrt{1+x}$ looks like \[1+a_1x+a_2 x^2+\cdots,\] and we guess that the square root of $I+N$ should look like $I+a_1N+a_2N^2+\dots+a_{m-1}N^{m-1}$, in which the higher order terms vanish because $N^m = 0$ for some $m$ ($N$ is nilpotent). We square this expression: 
    \begin{align*}
    & (I+a_1N+a_2N^2+\dots+a_{m-1}N^{m-1})^2 \\
    = & I + 2 a_1 N + (2a_2 + {a_1}^2) N^2 + (2a_3 + 2a_1a_2)N^3 + \dots + \\ & (2a_{m-1}+\text{terms involving $a_1,\dots,a_{m-2}$}) N^{m-1}
    \end{align*}
    We can let $2a_1 = 1$, $2a_2 + {a_1}^2 = 0$, $2a_3 + 2a_1a_2 = 0$, and so on so that all the coefficients of the expression above are 0 (because the value of $a_1$ gives the value of $a_2$, and the value of $a_1, a_2$ gives the value of $a_3$,\dots). In this manner we have found a square root of $I+N$: $I+a_1N+a_2N^2+\dots+a_{m-1}N^{m-1}$.
    \item For complex vector space $V$ and invertible $T \in \LV$, $T$ must have a square root.

    First, we look at nilpotent operators that arises from each generalized eigenspaces: \[N_j \coloneqq T|_{G(\lambda_j,T)} - \lambda_j I|_{G(\lambda_j,T)}.\] Then, since $T$ is invertible, none of the $\lambda_j$'s can be 0, so $T|_{G(\lambda_j,T)} = \lambda_j (
    \frac{N_j}{\lambda_j} + I|_{G(\lambda_j,T)})$. Clearly by the lemma above, the square root of $\frac{N_j}{\lambda_j} + I|_{G(\lambda_j,T)}$ times the square root of the complex number $\lambda_j$ gives a square root $R_j$ of $T|_{G(\lambda_j,T)}$.

    We know $v \in V$ can always be written as a unique sum of $u_1 + \dots + u_m$ with each $u_j$ in $G(\lambda_j,T)$. Define $R$ such that \[Rv = R_1u_1 + \dots + R_m u_m\] for all $v \in V$, which we can verify is a square root operator of $T$ on $V$, as follows.

    Note that $R_1u_1$ is still in $G(\lambda_1,T)$, $R(R_1u_1) = R_1(R_1u_1) = (R_1)^2u_1$. Taking this as an example, we have \[Rv = (R_1)^2 u_1 + \dots + (R_m)^2 u_m = T(u_1 + \dots +u_m) = Tv,\] as desired.
\end{itemize}

\subsection{Characteristic and Minimal Polynomials}
\textit{Remark.} By default in this section, $q$ is the characteristic polynomial, and $p$ is the minimal polynomial.
\begin{itemize}
    \item For an operator $T$ over a complex vector space $V$, let $\lambda_1, \dots, \lambda_m$ be all distinct eigenvalues of $T$, each with corresponding multiplicity $d_1,\dots, d_m$. Then the \df{characteristic polynomial} of $T$ is defined to be \[(z-\lambda_1)^{d_1}\cdots (z-\lambda_m)^{d_m}.\]
    \begin{itemize}
        \item The characteristic polynomial must be of degree $\d V$ because $d_1+\dots+d_m = \d V$.
        \item The zeros of the characteristic polynomial are the eigenvalues of $T$.
    \end{itemize}
    \item Now we prove the important \textbf{Cayley-Hamilton theorem} on complex vector spaces. With the tools of generalized eigenspaces we introduced in the last two sections, the proof becomes straightforward. Let $T$ be an operator on a complex vector space $V$. Let $q$ be the characteristic polynomial of $T$, then $q(T) = 0$.

    Let $\lambda_1,\dots,\lambda_m$ are distinct eigenvalues of $T$ and $d_1,\dots,d_m$ be the corresponding dimensions of the generalized eigenspaces $G(\lambda_j,T)$, i.e., the multiplicities of these eigenvalues. We know the restricted operator $(T - \lambda_j I)|_{G(\lambda_j,T)}$ is nilpotent, and thus it raised to the power of $\d G(\lambda_j,T)$ is the zero operator: $(T - \lambda_j I)^{d_j}|_{G(\lambda_j,T)} = 0$.

    Since every vector in $V$ is a sum of vectors in the individual generalized eigenspaces, it suffices to prove that each $q(T)|_{G(\lambda_j,T)} = 0$. Consider $q(T) = (T - \lambda_1 I)^{d_1} \cdots (T - \lambda_m I)^{d_m}$, in which the factor on the RHS are commutative. For any chosen $j$ between 1 and $m$, we may move the factor $(T - \lambda_j I)^{d_j}$ to the very right. Since for all $v \in G(\lambda_j,T)$, $(T - \lambda_j I)^{d_j}v = 0$, $q(T)|_G(\lambda_j,T)$ is the zero operator, as desired.

    \item A \df{monic polynomial} is polynomial with highest-term coefficient being 1.
    \item For $T \in \LV$, then there exists a unique monic polynomial $p$ of smallest degree such that $p(T)=0$. We call it the \df{minimal polynomial} of the operator $T$. This definition and the forthcoming theorems apply to operators on both real and complex vector spaces.
    
    Note $\d \LV = \d V \times \d V = n^2$, if we define $n = \d V$. Thus, the list of powers of $T$ of length $n+1$ \[I, T, T^2,\dots,T^{n^2}\] is linearly dependent. By the well-ordering principle applied to the linear dependence lemma, there exists a least $m$ such that \[I,T,T^2,\dots,T^{m-1}\] is linearly independent, while $T^m$ is in its span. Therefore, we have \[a_0 I + a_1 T + \dots + a_{m-1} T^{m-1} + T^m = 0\] for some $a_0,\dots,a_{m-1} \in \F$. Accordingly we define $p(z) = a_0 + a_1 z +\dots+ a_{m-1}z^{m-1} + z^m \in \PF$, which is a monic polynomial of the smallest possible degree such that $p(T) = 0$.

    For the uniqueness part, by the choice of $m$ if we have another minimal polynomial $q$, then $\deg q = m$ as well. Consider $(p-q)(z)$, which is of degree less than $m$ and has $(p-q)(T) = p(T) - q(T) = 0$. It follows that $p = q$ because otherwise $p$ cannot be the minimal polynomial.
    \begin{itemize}
        \item We remark that the Cayley-Hamilton theorem tells us that the minimal polynomial has to be of degree at most $\d V$, an upper bound stronger than $(\d V)^2 = \d \LV$, over $\C$. This holds over $\R$ as well, as we will see in the next chapter.
        \item Keep in mind that the minimal polynomial has \emph{real coefficients when $V$ is a real vector space, and has complex coefficients when $V$ is a complex vector space}.
    \end{itemize}
    \item $q(T) = 0$ iff $q$ is a polynomial multiple of the minimal polynomial $p$ of $T$ (i.e., there exists $s \in \PF$ such that $q=ps$).

    ``$\impliedby$'' is obvious. For ``$\implies$'', suppose we have $q(z)$ such that $q(T) = 0$, then by the division algorithm with respect to the minimal polynomial $p(z)$, we have $q = ps+r$ for a (unique) pair of $s$ and $r$ with $\deg r < \deg p$. Then \[0 = q(T) = p(T)s(T)+r(T) = r(T).\] If $r$ is not the zero polynomial, then we may divide $r$ by its highest coefficient to get a monic polynomial $\tilde{r}$ such that $\tilde{r}(T) = 0$ still. Yet $\deg \tilde{r} < \deg p$, which leads to contradiction. Therefore, $r = 0$, and we have $q = ps$.
    \begin{itemize}
        \item It follows directly that for an operator $T$ on a \textbf{complex} vector space, the characteristic polynomial must be a polynomial multiple of the minimal polynomial. (We have not defined the characteristic polynomials in the case $\F = \R$, but the result is similarly true. See the next chapter.) This is an important criterion for us to find the minimal polynomial of an operator.
    \end{itemize}
    \item The following result is another important criterion for finding the minimal polynomial. For an operator $T$ on a real/complex vector space $V$, the zeros of the minimal polynomial $p$ (over $\R$/$\C$) are \textbf{precisely} the eigenvalues of $T$.

    Suppose $\lambda \in \F$ is a zero of $p$, so that $p(z) = (z-\lambda)q(z)$ for a monic polynomial $q$ (because $p$ is monic). Replace $z$ by $T$, and we have \[0 = p(T) = (T - \lambda I)q(T).\] Since $\deg q < \deg p$, for some $v \in V$ $q(T)v \ne 0$ so that $q$ is not a minimal polynomial. Therefore, $T - \lambda I$ is not injective, which means that $\lambda$ is an eigenvalue of $T$.

    For the other direction, suppose $\lambda \in \F$ is an eigenvalue of $T$ instead, then $\exists v \in V\backslash \{0\}$ such that $Tv = \lambda v$. We have used a number of times already that $T^j v= \lambda^j v$ for arbitrary integer exponent $j$. Thus
    \begin{align*}
        0 = p(T)v & = (a_0 I + a_1 T + \dots + a_{m-1} T^{m-1} + T^m)v \\ & = (a_0 + a_1 \lambda + \dots + a_{m-1} \lambda^{m-1} + \lambda^m)v \\ & = p(\lambda) v.
    \end{align*}
    Since $v \ne 0$, $p(\lambda) = 0$, as desired.
\end{itemize}

\subsection{Jordan Form}
\begin{itemize}
    \item Let $N \in \LV$ be nilpotent. Then there exist vectors $v_1,\dots,v_n \in V$ and nonnegative integers $m_1,\dots,m_n$ such that
    \begin{enumerate}[label=(\alph*)]
        \item $N^{m_1}v_1,\dots,Nv_1,v_1, \\ N^{m_2}v_2,\dots,Nv_2,v_2, \\ \cdots \\ N^{m_n}v_n,\dots,Nv_n,v_n$ is a basis of $V$;
        \item $N^{m_1+1}v_1 = \cdots = N^{m_n+1}v_n = 0$.
    \end{enumerate}

    Induction on $\d V$: if $\d V = 1$, the only nilpotent operator is the 0 operator. We may take $v_1$ to be any nonzero vector in $V$ and $m_1 = 0$.

    Assume $\d V > 1$ and the claim holds on all vector spaces of smaller dimensions. Since $N$ is nilpotent, $N$ is not surjective and thus $\r N$ is proper subspace of $V$ with dimension smaller than $V$. We may then use the induction hypothesis on $N|_{\r N} \in \mathcal{L}(\r N)$. [In the trivial case $\r N = \{0\}$, $N$ is just 0 operator, and we may choose $v_1,\dots,v_n$ to be any basis of $V$ and let $m_1,\dots,m_n=0$.]
    
    By the induction hypothesis we have $v_1,\dots,v_n \in \r N$ and nonnegative integers $m_1,\dots,m_n$ such that \begin{align}
    \begin{split}
        N^{m_1}v_1,&\dots,Nv_1,v_1, \\ N^{m_2}v_2,&\dots,Nv_2,v_2, \\ &\cdots \\ N^{m_n}v_n,&\dots,Nv_n,v_n
    \end{split}
    \end{align} is a basis of $\r N$, and \begin{equation}
        N^{m_1+1}v_1 = \cdots = N^{m_n+1}v_n = 0.
    \end{equation} By $v_j \in \r N$ we know there exists $u_j \in V$ such that $v_j = Nu_j$. It follows that $N^{k+1}u_j = N^k v_j$ for each $j$ and each nonnegative $k$.

    Now we prove that \begin{align}
    \begin{split}
        N^{m_1+1}u_1,&\dots,Nu_1,u_1, \\ &\cdots \\ N^{m_n+1}u_n,&\dots,Nu_n,u_n
    \end{split} \end{align} is linearly independent in $V$. Consider a linear combination of the vectors in the list above and apply $N$ to it. By $N^{k+1}u_j = N^k v_j$ it is easy to see that the result is a linear combination of (4) and $N^{m_1+1}v_1,\dots,N^{m_n+1}v_n$, with coefficients left unchanged. Therefore by (5) the linear combination of terms in (4) and of $N^{m_1}v_1,\dots,N^{m_n}v_n$ must both be 0. Since (4) is a basis of $\r N$, all these coefficients must be 0. Thus the list is linearly independent, and we can extend it to a a basis \begin{align}
    \begin{split}
        N^{m_1+1}u_1,&\dots,Nu_1,u_1, \\ &\cdots \\ N^{m_n+1}u_n,&\dots,Nu_n,u_n \\ w_1,&\dots,w_p
    \end{split}
    \end{align} of $V$.

    Note that $Nw_j \in \r N =$ the span of (4), and each vector in (4) is $N$ applied to a vector in (6). Therefore, there is a vector $x_j$ in the span of (6) such that $Nx_j = Nw_j$ for every $j$.

    Now let $u_{n+j} = w_j - x_j$, which follows that $Nu_{n+j} = 0$. Meanwhile \begin{align*}
        N^{m_1+1}u_1,&\dots,Nu_1,u_1, \\ &\cdots \\ N^{m_n+1}u_n,&\dots,Nu_n,u_n, \\ u_{n+1},&\dots,u_{n+p}
    \end{align*}
    spans $V$ because each $x_j$ and $u_{n+j}$ is contained in its span, meaning that each $w_j$ is in its span. Since the length of the list is same as (7), it is the desired basis for $V$, which finishes the induction.
    \item For $T \in \LV$, a basis of $V$ is called a \df{Jordan basis for $T$} if with respect to this basis $\M(T)$ is a block diagonal matrix \[\begin{bmatrix}
        A_1 & & 0 \\ & \ddots & \\ 0 & & A_p
    \end{bmatrix},\] where each block $A_j$ is an upper-triangular matrix of the form \[A_j = \begin{bmatrix}
        \lambda_j & 1 & & 0 \\
        & \ddots & \ddots & \\
        & & \ddots & 1 \\
        0 & & & \lambda_j
    \end{bmatrix}.\]

    From the end of \lk{5}{2} we know that all the $\lambda_j$'s are precisely the eigenvalues of $T$ because $\M(T)$ is upper-triangular. The $\lambda_j$'s do not have to be distinct.
    \item Existence of Jordan Basis: for an operator $T$ on a complex vector space $V$, there is a basis of $V$ that is a Jordan basis for $T$.

    Consider a nilpotent operator $N$ and $v_1,\dots,v_n \in V$ as given by the previous theorem. What is the special about the result we just proved is that for each $j$, $N$ sends the first vector in the list $N^{m_j}v_j,\dots,Nv_j,v_j$ to 0 and the rest to the previous vector. In terms of matrix, the previous theorem gives a basis of $V$ with respect to which $\M(N)$ is block diagonal, with each diagonal blocks of the form \[\begin{bmatrix}
        0 & 1 & & 0 \\
        & \ddots & \ddots & \\
        & & \ddots & 1 \\
        0 & & & 0
    \end{bmatrix}.\] Thus Jordan bases exist for nilpotent operators.
    
    In general for $T \in \LV$, we know \[V = G(\lambda_1,T) \oplus \cdots \oplus G(\lambda_m,T),\] where the $\lambda_j$'s are distinct eigenvalues of $T$. In \lk{8}{2} we proved that each $(T-\lambda_j I)|_{G(\lambda_j,T)}$ is nilpotent. With respect to the basis described in the last paragraph we have \[\M(T|_{G(\lambda_j,T)}) = \begin{bmatrix}
        \lambda_j & 1 & & 0 \\
        & \ddots & \ddots & \\
        & & \ddots & 1 \\
        0 & & & \lambda_j
    \end{bmatrix}.\] Therefore, adjoining the described bases of each nilpotent $(T-\lambda_j I)|_{G(\lambda_j,T)}$ gives us the Jordan basis for $T$, with respect to which $\M(T)$ has the \df{Jordan canonical form}.
\end{itemize}


\newpage
\section{Operators on Real Vector Spaces}
\textit{Remark.} We continue to assume $V$ is a nonzero FDVS in this chapter, and $T \in \LV$.
\subsection{Complexification}
\textit{Remark.} In this section, $u,v$ by default denotes vectors in $V$, and $a,b$ are real numbers to represent an arbitrary complex number $a+bi$. $V$ is a nonzero finite-dimensional real vector space.
\begin{itemize}
    \item For a real vector space $V$, its \df{complexification}, denote by $V_\C$, is $V \times V$. Customarily we write $(u,v) \in V_\C$ as $u + iv$, although it sometimes makes more sense to \emph{}
    \begin{enumerate}
        \item [(1)] Addition is defined entrywise, i.e., \[(u_1+iv_1)+(u_2+iv_2)=(u_1+u_2)+i(v_1+v_2),\] or equivalently, $(u_1,v_1)+(u_2,v_2) = (u_1+u_2,v_1+v_2)$.
        \item [(2)] \textbf{Complex} scalar multiplication is defined by seeing $u+iv \in V_\C$ as a complex number, i.e., \[(a+bi)(u+iv) = (au-bv)+i(av+bu)\] for $a,b \in \R$, or equivalently, $(a+bi)(u,v) = (au-bv,av+bu)$.
    \end{enumerate}

    We can show that $V_\C$ is a complex vector space. Clearly it is abelian group under addition. The multiplicative identity is $1+0i$, and because complex number multiplication follows the distributive and associative properties, scalar multiplication defined on $V_\C$ here also follows these properties. Since now $V_\C$ is a vector space, the original real vector space $V$ is naturally embedded into $V_\C$: each $u \in V$ is naturally identified with each $u+0i \in V_\C$.
    \item For a real vector space $V$, if $v_1,\dots,v_n$ is its basis, then the list is also a basis of the complex vector space $V_\C$. It follows directly that $\d V_\C = \d V$.

    Note that by the definition of scalar multiplication, $(0+1i)(u+i0) = 0+iu$ for any $u \in V$. Therefore, $\s(v_1,\dots,v_n)$ contains all the vectors $v_1,\dots,v_n$ and $iv_1,\dots,iv_n$, so that all $u+iv \in V_\C$ can be expressed.
    
    \emph{It might make more sense to think about this in terms of the $(\cdot,\cdot)$ notation. The list of vectors $(v_1,0),\dots,(v_n,0)$ spans the first slot, while the list $(0,v_1),\dots,(0,v_n)$ spans the second slot in $V \times V$.}

    Regarding the linear independence of $v_1,\dots,v_n$, suppose $a_1v_1+\dots+a_nv_n = 0$ with coefficients from $\C$, then 
    \[(\Re a_1)v_1+ \dots + (\Re a_n)v_n = 0 \quad \text{and} \quad (\Im a_1)v_1 + \dots + (\Im a_n)v_n = 0.\]
    Since the list off $v_i$'s are linearly independent on the real vector space $V$, the $\Re$'s and $\Im$'s above are all zero, and thus the $a_i$'s are all zero, finishing the proof.
    \item $T_\C$, the \df{complexification of an operator} $T$ on a real vector space $V$, is an operator on the complex vector space $V_\C$ given by $T_\C (u+iv) = Tu + i Tv$. In particular, $T_\C u = Tu$ for $u \in V$.

    Now we show that given $T \in \LV$, $T_\C \in \mathcal{L}(V_\C)$: first, \begin{align*}
        T_\C((u_1+iv_1)+(u_2+iv_2)) & = T_\C((u_1+u_2)+i(v_1+v_2)) \\ & = T(u_1+u_2) + iT(v_1+v_2) \\ & = (Tu_1 + iTv_1) + (Tu_2 + iTv_2) \\ & = T_\C(u_1 + iv_1) + T_\C(u_2 + iv_2);
    \end{align*}
    and second, given $a+bi \in \C$ (since $V_\C$ is a \textbf{complex} vector space),
    \begin{align*}
        T_\C((a+bi)(u+iv)) & = T_\C(au-bv + i(bu+av)) \\ & = T(au-bv) + iT(bu+av) \\ & = (a+bi)Tu+(ia-b)Tv \\ & = (a+bi)Tu + (a+bi)iTv \\ & = (a+bi) T_\C(u+iv).
    \end{align*}
    \item For $T$ on a real vector space $V$ with basis $v_1,\dots,v_n$, $\M(T) = \M(T_\C)$ with respect to the same basis $v_1,\dots,v_n$, because $T_\C v_i = Tv_i$ for all $i$ between 1 and $n$. \emph{This gives us the insight that $T_\C$ can be thought as the complete extension of $T$ from $V$ over $\R$ to the complexified space $V \times V$ over $\C$, so that the matrix of $T$ is still preserved.}
    \item We mentioned that a real vector space can have no eigenvalues at all and therefore no invariant subspace of dimension 1. However, we can conclude that an invariant subspace of dimension 1 or 2 always exists in a finite-dimensional real vector space. The proof follows quite mystically from the trick of complexification.

    For an operator $T$ on a real vector space $V$, its complexification $T_\C$ on the complex vector space $V_\C$ must have an eigenvalue $a+bi$. We can simplify $T_\C(u+iv) = (a+bi)(u+iv)$ into \[Tu+iTv = (au-bv)+(av+bu)i,\] from which we get $Tu = au-bv$ and $Tv = av+bu$. Let $U$ be the span of $\{u,v\}$ in $V$, which is clearly invariant under $T$ with dimension $\leq 2$.
    
    \item Clearly by repeated application of $T_\C$ to $u+iv$ we have $(T_\C)^n (u+iv) = T^n u + i T^n v$. It directly follows that $p(T_\C) = (p(T))_\C$.
    \item The minimal polynomial of $T_\C$ is equal to the minimal polynomial of $T$ on a real vector space. We use $p\in \mathcal{P}(\R)$ to denote the minimal polynomial of $T$.
    
    First from $p(T) = 0$ we immediately know that $p(T_\C) = 0$.
    
    We now show that $p$ is indeed the \emph{minimal} polynomial of $T_\C$. Suppose $q \in \mathcal{P}(\C)$ is a monic polynomial such that $q(T_\C) = 0$. Then for any $u \in V$ (not in $V_\C$), $q(T_\C)u = 0$, i.e., given \[q(z) = (a_0+b_0) + (a_1+b_1)z + \dots + (a_{m-1} + b_{m-1})z^{m-1} + z^m,\] we have 
    \begin{align*}
        0 = q(T_\C)u & = (a_0+b_0 i)u + (a_1+b_1 i)Tu + \dots + (a_{m-1} + b_{m-1} i)T^{m-1}u + T^mu \tag{equal to $q(T)u$}\\ & = (a_0 + a_1T + \dots + a_{m-1}T^{m-1} + T^m)u + i(b_0 + b_1T + \dots + b_{m-1}T^{m-1})u \tag{$\dagger$}
    \end{align*}
    Take $r(z) = a_0 + a_1z + \dots + a_{m-1}z^{m-1} + z^m$. Then $r$ is a monic polynomial in $\mathcal{P}(\R)$ with $r(T)u = 0$ for all $u \in V$. Thus $\deg q = m = \deg r \geq \deg p$, showing that $p$ is of the minimum degree and thus the \emph{unique} minimal polynomial of $V_\C$.
    \begin{itemize}
        \item We give another more involved way of thinking about the proof, but it helps you understand the theorem. Consider $q$ instead as the the minimal polynomial of $T_\C$. Again restrict $q(T_\C)$ to $V$ and look at ($\dagger$). If $b_0 + b_1z + \dots + b_{m-1}z^{m-1}$ is not a zero polynomial, then we can divide it by its highest-degree coefficient to get a monic polynomial $s(z)$ such that $s(T)u = 0$ for all $u \in V$, with degree $< m$. Therefore, \emph{$q$ must be of real coefficients} [$q(z) \in \mathcal{P}(\R)$]. Since we know $p$ is the minimal polynomial of $T$ on $V$, $q$ must be just $p$ because we have showed that $p$ is a monic polynomial such that $p(T_\C) = 0$.
    \end{itemize}
    \item $\lambda \in \R$ is an eigenvalue of $T$ on a real vector space $V$ iff it is an eigenvalue of $T_\C$.
    
    The (real) eigenvalues of $T$ are the (real) zeros of the minimal polynomial of $T$, which are the real zeros of the minimal polynomial of $T_\C$ and thus the real eigenvalues of $T_\C$. Proof straight from the definition is also viable.
    \item For $\lambda \in \C$ and any nonnegative $j$, for $T$ on a real vector space $V$, \[(T_\C - \lambda I)^j (u+iv) = 0 \iff (T_\C - \conj{\lambda} I)^j (u-iv) = 0.\] WLOG we only need to prove ``$\implies$''. Proof is based on induction and some calculation tricks, which you can refer to the book.
    \begin{itemize}
        \item Take $j = 1$, and it follows directly that the eigenvalues of $T_\C$ come in pairs: if $\lambda \in \C$ is one, then $\conj{\lambda}$ is also one.
        \item The multiplicity of $\lambda \in \C$ is equal to the multiplicity of $\conj{\lambda}$ (both considered as an eigenvalue of $T_\C$). Suppose $u_1+iv_1,\dots,u_m+iv_m$ is a basis of $G(\lambda, T_\C)$, then $u_1-iv_1,\dots,u_m-iv_m$ is a basis of  $G(\conj{\lambda}, T_\C)$. Both generalized eigenspaces are of the same dimension, and thus $\lambda$ and $\conj{\lambda}$ have the same multiplicities.
    \end{itemize}
    \item An operator $T$ on an odd-dimension real vector space $V$ has an eigenvalue. Because nonreal eigenvalues of $T_\C$ come in pairs of of 2, and each eigenvalue in such a pair share the same multiplicity, the sum of multiplicities of the nonreal eigenvalues is even.
    
    However, the sum of multiplicities of all eigenvalues is $\d V_\C = \d V$, which is odd, there must be a real eigenvalue of $T_\C$, which is an eigenvalue of $T$.
\end{itemize}
\textit{Remark.} We defined in \lk{8}{3} the minimal polynomial altogether for operators on both real and complex vector spaces, because the whether $\F$ is the real or the complex field does not make any difference for the properties and theorems about the minimal polynomial. For example, the coefficients of the minimal polynomial for an operator on real (resp.\ complex) vector space must be real (resp.\ complex). But this was not the case for the characteristic polynomial. For example, we know $\d V$ is equal to the sum of multiplicities of all eigenvalues of an operator $T$ on a complex vector space, but not on a real vector space. However, with the tool of complexification we can now define the characteristic polynomial for operators on real vector spaces, and make sense of the theorems we have proved about characteristic polynomials in the context of real vector spaces.
\begin{itemize}
    \item For $T$ on a real vector space $V$, the coefficients of the characteristic polynomial of $T_\C$ are all real.

    We know the multiplicities of nonreal eigenvalues of $T_\C$ in a conjugate pair are the same. Thus we can the characteristic polynomial of $T_\C$ should include the following: \[(z-\lambda)^m(z-\conj{\lambda})^m = (z^2 - 2(\Re \lambda)z + \abs{\lambda}^2),\] which is of real coefficients. The characteristic polynomial of $T_\C$ should therefore be of the product of expressions above with expressions $(z-t)^d$, where $t$ is a real eigenvalue of $T_\C$ and $d$ is its multiplicity. The coefficients of the characteristic polynomial of $T_\C$ should therefore be all real,
    \item and we define the characteristic polynomial of $T_\C$ to be the \df{characteristic polynomial} of $T$ on a \textbf{real} vector space $V$. Properties to be noted include:
    \begin{enumerate}[label=(\alph*)]
        \item the coefficients of it are all real by definition;
        \item it has degree $\d V = \d V_\C$;
        \item its \textbf{real zeros} are precisely the eigenvalues of $T$, because the (real) zeros of $T_\C$ are precisely the (real) eigenvalues of $T_\C$, which are precisely the eigenvalues of $T$.
    \end{enumerate}
    \item The \textbf{Cayley-Hamilton theorem} now holds on real vector spaces: for $T$ on either a real/complex vector space $V$ and the characteristic polynomial $q$ of $T$, $q(T) = 0$.
    
    Since we have $q(T_\C) = 0$, by $q(T_\C) = (q(T))_\C$, we get $q(T) = 0$.
    \item Also, we can conclude in general for $T$ either on a real/complex vector space $V$, let $p$ be the minimal polynomial and $q$ be the characteristic polynomial of $T$, then
    \begin{enumerate}[label=(\alph*)]
        \item $\deg p \leq \d V$; (since $\deg q = \d V$)
        \item $q$ is a polynomial multiple of $p$. (since $q(T) = 0$ by Cayley-Hamilton)
    \end{enumerate}
\end{itemize}

\subsection{Operators on Real Inner Product Spaces}
\textit{Remark.} The real spectral theorem that we proved in \lk{7}{2} offers a complete description of self-adjoint (but not normal) operators on real vector spaces. And in \lk{7}{3}, we gave a completed description of isometries only on complex (but not real) vector spaces. The following section will extend these results with the tools we now have.
\begin{itemize}
    \item For $T$ on a real inner product space $V$ with dimension 2, the following are equivalent:
    \begin{enumerate}[label=(\alph*)]
        \item $T$ is normal but not self-adjoint;
        \item The matrix of $T$ with respect to any orthonormal basis of $V$ is of the form $\begin{bmatrix}
            a & -b \\ b & a
        \end{bmatrix}$ with $b \neq 0$.
        \item The matrix of $T$ with respect to some orthonormal basis of $V$ is of the form $\begin{bmatrix}
            a & -b \\ b & a
        \end{bmatrix}$ with $b > 0$.
    \end{enumerate}
    
    (a)$\implies$(b): Suppose $T$ is normal but not self-adjoint, then let $e_1,e_2$ be an orthonormal basis of $V$, with respect to which $\M(T,(e_1,e_2)) = \begin{bmatrix}
        a & c \\ b & d
    \end{bmatrix}$, and its transpose $\begin{bmatrix}
        a & b \\ c & d
    \end{bmatrix} = \M(T^*,(e_1,e_2))$. Thus $Te_1 = a e_1 + b e_2$ and $T^*e_1 = ae_1 + ce_2$, from which we have \[\nm{Te_1} = a^2 + b^2 \quad \text{and} \quad \nm{T^*e_1} = a^2 + c^2.\] Since $T$ is normal, the two norms above should be equal, and $b = c$ or $-c$. If $b = c$ then $\M(T) = \M(T^*)$, so that $T$ is self-adjoint. Thus $\M(T,(e_1,e_2)) = \begin{bmatrix}
        a & -b \\ b & d
    \end{bmatrix}$.

    Since $T$ is normal, $\M(T)\M(T^*) = \M(T^*)\M(T)$:
    \begin{equation}\begin{bmatrix}
        a & -b \\ b & d
    \end{bmatrix} \begin{bmatrix}
        a & b \\ -b & d
    \end{bmatrix} = \begin{bmatrix}
         a & b \\ -b & d
    \end{bmatrix}\begin{bmatrix}
        a & -b \\ b & d
    \end{bmatrix}. \tag{$\ast$}
    \end{equation}
    Then we have $ab-bd=-ab+bd$. Note that if $b=0$ then $T$ is self-adjoint, so $b \neq 0$. Thus $a-d=-a+d$, which gives $a = d$.

    (b) $\implies$ (c) is trivial. Suppose $\M(T) = \begin{bmatrix}
        a & -b \\ b & a
    \end{bmatrix}$ with respect to an orthonormal basis $e_1,e_2$. Since $b \neq 0$, one of $b$ and $-b$ is greater than 0. If $b > 0$, the proof is finished; if not, then then consider $\M(T,(e_1,-e_2)) =
        \begin{bmatrix}
        a & b \\ -b & a
    \end{bmatrix}$. Since $-b > 0$, this completes the proof.

    (c) $\implies$ (a): Since $b \neq 0$, the matrix is not equal to its transpose and hence $T$ is not self-adjoint. Because ($\ast$) is satisfied by $d=a$, the matrices of $T$ and $T^*$ commute. Thus $T$ is normal.
    \item We now prove a lemma that will be useful in describing all normal operators on real inner product spaces. (Recall in \lk{7}{2} we had to show a similar result in order to prove the real spectral theorem.) Suppose $T$ is a normal operator on an inner product space $V$ (real or complex), and $U$ is a subspace of $V$ that is invariant under $T$, then
    \begin{enumerate}[label=(\alph*)]
        \item $U^\perp$ is invariant under $T$.
        
        Let $e_1,\dots,e_m$ be an orthonormal basis of $U$, from which we can extend to a basis of $V$: $e_1,\dots,e_m,f_1,\dots,f_n$. Since $V = U \oplus U^\perp$, by a claim we mentioned in Chapter 2, $U^\perp$ has $f_1,\dots,f_n$ as a basis.

        Because $U$ is invariant under $T$, we should have \[\M(T,(e_1,\dots,e_m,f_1,\dots,f_n)) = \begin{bmatrix}
            A & B \\ 0 & C
        \end{bmatrix},\] where $A$ is a $m \times m$ matrix. Because the $e_i$'s are orthonormal, \[\sum_{i = 1}^m \nm{Te_j}^2 = \text{ sum of the squares of the absolute values of all entries of }A.\] Similar to what we have done in our last theorem, if we look at the matrix of $T^*$, \[\sum_{i = 1}^m \nm{T^*e_j}^2 = \text{ sum of the squares of the absolute values of all entries of }A \text{ and } B.\] Equating the two formulas above (by $T$ is normal) tells us that $B$ is a zero matrix. This shows that $U^\perp$ is invariant under $T$ (because all $Tf_j$'s are in the span of the basis vectors of $U^\perp$).
        
        \item $U$ is invariant under $T^*$.

        This is easy. Since \[\M(T,(e_1,\dots,e_m,f_1,\dots,f_n)) = \begin{bmatrix}
            A & 0 \\ 0 & C
        \end{bmatrix},\] we have \[\M(T^*,(e_1,\dots,e_m,f_1,\dots,f_n)) = \begin{bmatrix}
            \conj{A^t} & 0 \\ 0 & \conj{C^t}
        \end{bmatrix},\] (where $\conj{\cdot^t}$ is our notation for the conjugate transpose of a matrix). Thus $U$ is invariant under $T^*$.
        
        \item $(T|_U)^* = T^*|_U$.

        Set $S = (T|_U) \in \mathcal{L}(U)$, and we want to show $S^* v = T^* v$ for all $v \in U$. Fix $u \in U$, and for all $v \in U$ we have $\inp{u}{S^* v} = \inp{Su}{v} = \inp{Tu}{v} = \inp{u}{T^* v}$ (here we consider $\inp{\cdot}{\cdot}$ as an inner product on $V$). By (b), $T^* v \in U$, and so we can restrict this inner product to the subspace $U$. It follows that $S^* = T^*$. [More formally, $\inp{u}{(S^*-T^*)v} = 0$, where $u$ is an arbitrary vector in $U$, and $S^* v, T^* v$ are both in $U$ as well. Let $u = (S^*-T^*)v$, and we have $(S^*-T^*)v = 0$ for all $v \in U$.]
        
        \item $T|_U \in \mathcal{L}(U)$ and $T|_{U^\perp} \in \mathcal{L}(U^\perp)$ are both normal.

        $(T|_U)^*(T|_U) = (T^*|_U)(T|_U) = (T|_U)(T^*|_U) = (T|_U)(T|_U)^*$ [essentially this holds because restriction does not change the properties of maps]. Because $U^\perp$ is also invariant under $T$, by the same token we can prove that $T|_{U^\perp}$ is normal as well.
    \end{enumerate}
\end{itemize}
Since a normal operator restricted to an invariant subspace is still normal, we can use our description of normal but not self-adjoint operators of dimension 2 to describe normal operators on real inner product spaces of arbitrary dimensions by induction, from which we can completely describe isometries on real inner product spaces.
\begin{itemize}
    \item For $T$ on a real inner product space $V$, $T$ is normal iff there is an orthonormal basis of $V$ with respect to which $T$ has a block diagonal matrix such that each block is $1 \times 1$ or $2 \times 2$ of the form $\begin{bmatrix}
        a & -b \\ b & a
    \end{bmatrix}$, with $b > 0$.
    
    We first mention an easy lemma: for block diagonal matrices $A$ and $B$ with the same block sizes on their diagonals, $AB$ is still block diagonal with each block of $AB$ being the product of the corresponding block in $A$ times the corresponding block in $B$.
    
    ($\impliedby$) Each $1 \times 1$ block commutes with its transpose (itself), and the $2 \times 2$ block of the given form must also commute with its transpose [showed earlier in this chapter; see ($\ast$)]. By the preceding lemma, the block diagonal matrix of $T$ should commute with its transpose, showing that $T$ commutes with $T^*$. This completes the proof.
    
    ($\implies$) We use induction. The case $\d V = 1$ is trivial. When $\d V = 2$, if $T$ is self-adjoint, then by the real spectral theorem we should find two $1 \times 1$ blocks on the diagonal and two 0s off the diagonal; if $T$ is not self-adjoint, then the first theorem in the section applies.
    
    Assume $\d V > 2$, and that the result holds on real inner product spaces of smaller dimensions. In \lk{9}{1} we showed that every operator must have invariant subspaces of dimension 1 or 2. Let $U$ be a dimension 1 subspace of $V$ invariant under $T$, if a dimension 1 subspace exists (equivalently we can say if $T$ has an eigenvector in $V$, then let $U$ be the span of this eigenvector); if not, let $U$ be a dimension 2 subspace of $V$ invariant under $T$, and in this case $T|_U$ cannot be a self-adjoint operator (because a self-adjoint operator must have an eigenvalue, which was proved in Chapter 7). \emph{This construction maximizes the number of invariant subspaces of dimension 1.}
    
    If $\d U = 1$, then consider a vector in $U$ of norm 1. This vector forms an orthonormal basis of $U$, with respect to which $T|_U$ has a $1\times1$ matrix. If $\d U = 2$, then the matrix of $T|_U$ with respect to some orthonormal basis of $V$ is of the form $\begin{bmatrix}
    a & -b \\ b & a
    \end{bmatrix}$ with $b>0$ (because $T|_U$ is a normal but not self-adjoint operator).
    
    Now since $\d U^\perp < \d V$, and $T|_{U^\perp}$ is normal on $U^\perp$ by the preceding lemma, the inductive hypothesis tells us that the desired orthonormal basis of $U^\perp$ exists. Adjoining the orthonormal basis of $U$ we created to this orthonormal basis of $U^\perp$ gives us an orthonormal basis of the whole $V$, with respect to which $\M(T)$ is still of the form desired.
    \item Now we can describe every isometry on a real inner product space. Let $S$ be an operator on a real inner product space $V$, then $S$ is an isometry iff there is an orthonormal basis of $V$ with respect to which $S$ has a block diagonal matrix such that each block on the diagonal is a $1\times1$ matrix containing 1 or $-1$, or is a $2\times2$ rotation matrix of the form $\begin{bmatrix}
    \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta)
    \end{bmatrix}$, where $\theta \in (0,\pi)$.

    If we assume that $S$ is an isometry, then $S$ is normal tells us that there is an orthonormal basis of $S$ with respect to which $\M(S)$ has the special block diagonal form. If $\lambda \in \R$ is an entry in the $1\times1$ blocks on the diagonal of $\M(S)$, then $Se_j = \lambda e_j$ for some $e_j$ of norm 1. Because $S$ is an isometry, $\abs{\lambda} = 1$.

    Now we consider $2\times2$ blocks of the form $\begin{bmatrix}
    a & -b \\ b & a
    \end{bmatrix}$ with $b>0$ on the diagonal of $\M(S)$. This means that two orthonormal vectors $e_j$ and $e_{j+1}$ make $Se_j = ae_j + be_{j+1}$. It follows that \[1 = \nm{e_j}^2 = \nm{Se_j}^2 = a^2 + b^2.\] By $b > 0$ we can now reparameterize $\begin{bmatrix}
    a & -b \\ b & a
    \end{bmatrix}$ into $\begin{bmatrix}
    \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta)
    \end{bmatrix}$ by a single $\theta \in (0,\pi)$.

    Conversely, we suppose that there is an orthonormal basis of $V$ with respect to which $\M(S)$ is of the special block diagonal form as stated in the theorem. For clarity we refer to the basis as $e_1,\dots,e_n$. From the block diagonal form, we know $V$ can be expressed as $U_1 \oplus \cdots \oplus U_m$, where each $U_j$ is of dimension 1 or 2, and $S$ is invariant under each $U_j$. It follows that for every $v \in V$, $v = u_1 + \dots + u_m$, where each $u_j \in U_j$. Note that all the $u_j$'s must be pairwise orthogonal, because the basis vector(s) of one $U$ is orthogonal to the basis vector(s) of another $U$. Note also that for every vector $v_j \in U_j$, if $\d U_j = 1$, then we have $Sv_j = v_j$ or $Sv_j = -v_j$. If $\d U_j = 2$, then given \[\M(S|_{U_j}) = \begin{bmatrix}
        \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta)
    \end{bmatrix}\] for some $\theta$ with respect to the orthonormal basis vectors $e_k$ and $e_{k+1}$ of $U$, by matrix computation we see that \[S(ae_k + be_{k+1}) = [a \cos(\theta) - b\sin(\theta)]e_k + [a \sin(\theta) + b\cos(\theta)]e_{k+1}.\] This implies that \[\nm{S(ae_k + be_{k+1})}^2 = [a \cos(\theta) - b\sin(\theta)] + [a \sin(\theta) + b\cos(\theta)]^2 = a^2 + b^2 = \nm{ae_k + be_{k+1}}^2\] for every vector $ae_k + be_{k+1} \in U_j$. Our work above in general shows that $S|_{U_j} \in \mathcal{L}(U)$ is an isometry. Therefore, for any $v \in V$
    \begin{align*}
        \nm{Sv}^2 & = \nm{Su_1 + \dots + Su_m}^2 \\ & = \nm{Su_1}^2 + \dots + \nm{Su_m}^2 \quad \text{(by orthogonality, because each $Su_j \in U_j$)}\\ & = \nm{u_1}^2 + \dots + \nm{u_m}^2 \\ & = \nm{u_1 + \dots + u_m}^2 \quad \text{(again by orthogonality)} \\ & = \nm{v}^2,
    \end{align*}
    as desired.
\end{itemize}

\newpage
\section{Trace and Determinant}
\textit{Remark.} $V$ still remains a nonzero FDVS in this last chapter.

\setcounter{secnumdepth}{1}

\subsection{Change of Basis}
\begin{itemize}
    \item Our focus finally comes to matrices themselves. We use $I_n$ to denote the $n \times n$ identity matrix, and $A^{-1}$ to denote the inverse matrix of a square matrix $A$.

    You may appeal to the fact that there is one-to-one correspondence between every square matrix and an operator with respect to a fixed basis to show that, or simply prove directly, that $AI = IA = A$ for a square matrix $A$, and that there is a unique inverse $B$ (such that $AB = BA = I$) for an invertible square matrix $A$.
\end{itemize}
\textit{Remark.} The change of basis formula is one of the most important result in linear algebra, and we now consider the matrix of an operator with respect to two different bases (instead of only one basis). The reason is clear, because we are now changing the basis of our operator/matrix. Recall the notation $\M(T,(u_1,\dots,u_n),(v_1,\dots,v_n))$ is the matrix of writing each $Tu_j$ as a linear combination of $v_1,\dots,v_n$ in the $j$-th column.
\begin{itemize}
    \item Suppose $u_1,\dots,u_n$, $v_1,\dots,v_n$, and $w_1,\dots,w_n$ are all bases of $V$. For $S,T \in \LV$, we have 
    \begin{align*}
        & \M(ST,(u_1,\dots,u_n),(w_1,\dots,w_n)) \\ = \: & \M(S,(v_1,\dots,v_n),(w_1,\dots,w_n))\M(T,(u_1,\dots,u_n),(v_1,\dots,v_n)).
    \end{align*}
    We proved this result for linear maps in Chapter 3, and we restate it here.
    \item For two bases $u_1,\dots,u_n$ and $v_1,\dots,v_n$ of $V$, the two matrices $\M(I,(u_1,\dots,u_n),(v_1,\dots,v_n))$ and $\M(I,(v_1,\dots,v_n),(u_1,\dots,u_n))$ are inverses of one another. (Note that the $I$ here is the identity operator on $V$.)

    This follows from the last claim and \[I_n = \M(II,(u_1,\dots,u_1),(u_1,\dots,u_1)) = \M (II,(v_1,\dots,v_1),(v_1,\dots,v_1)).\]
    \item Now we can prove the \textbf{change of basis formula}. For $T \in \LV$, where $V$ has two bases $u_1,\dots,u_n$ and $v_1,\dots,v_n$. Let $A = \M(I,(u_1,\dots,u_n),(v_1,\dots,v_n))$. Then \[\M(T,(u_1,\dots,u_n)) = A^{-1}\M(T,(v_1,\dots,v_n))A.\]

    By the previous two claims, $A^{-1}\M(T,(v_1,\dots,v_n)) = \M(T,(v_1,\dots,v_n),(u_1,\dots,u_n))$, which when multiplied by $A$ on the right gives $\M(T,(u_1,\dots,u_n))$, as desired.
    \end{itemize}
    \rmk A number of times in this book we have discussed finding a basis of an operator with respect to which there is a ``nice-looking'' matrix (e.g., a diagonal, a block diagonal, or a Jordan matrix). Given a matrix $P = \M(T,(u_1,\dots,u_n))$, if we have shown that we can always find a kind of ``nice-looking'' matrix $Q$ of the operator $T$ with respect to a basis $(v_1,\dots,v_n)$, then we can always decompose $P$ into $A^{-1}QA$, where $A = \M(I,(u_1,\dots,u_n),(v_1,\dots,v_n))$.

    $P$ and $Q$ are oftentimes called \df{similar matrices} when $P = A^{-1} Q A$. They are similar in the sense that $Q = (A^{-1})^{-1} P (A^{-1})$ as well.

    Now we look back at the diagonalization computation from our first course in linear algebra. Consider the linear operator $T \in \mathcal{L}(\F^n)$ of multiplying a $n\times n$ matrix $P$ on the left. Then $P = \M(T,(e_1,\dots,e_n))$, and $A^{-1} = \M(I,(v_1,\dots,v_n),(e_1,\dots,e_n))$. We know $(v_1,\dots,v_n)$ is a basis of eigenvectors of $\F^n$, and thus the $j$-th column of $A^{-1}$ is exactly the $j$-th eigenvector $v_j \in \F^n$.
    
    Recall when diagonalizing a matrix $P$, we used to find all the eigenvalues $\lambda_1,\dots,\lambda_n$ and their corresponding eigenvectors $v_1,\dots,v_n$, and then adjoin all the eigenvectors to get the $A^{-1} = \begin{bmatrix} v_1 & \cdots & v_n \end{bmatrix}$ and its inverse $A$, that $Q = \begin{bmatrix}
    \lambda_1 & & 0 \\
    & \ddots & \\
    0 & & \lambda_n
    \end{bmatrix}$ needs to multiply on the left and right respectively. These computation steps are finally justified.
\subsection{Trace and Determinant of an Operator}
\begin{itemize}
    \item The \df{trace of an operator} $T$ on a complex (resp.\ real) vector space is the sum of eigenvalues of $T$ (resp.\ $T_\C$), with each eigenvalue repeated according to its multiplicity. The \df{determinant of an operator} $T$ on a complex (resp.\ real) vector space is the product of the eigenvalues of $T$ (resp.\ $T_\C$), with each eigenvalue repeated according to its multiplicity.
    \item Let $\d V = n$, and $\lambda_1,\dots,\lambda_n$ be all the eigenvalues of $T$/$T_\C$ (counting repeats according to multiplicities), then the characteristic polynomial of $T$ should be \[(z-\lambda_1)\cdots(z-\lambda_n).\] By polynomial expansion (or Vieta's formulas), $\tr T = \lambda_1 +\dots+\lambda_n$ is the negative coefficient of $z^{n-1}$ in the characteristic polynomial of $T$, and $\det T = \lambda_1 \cdots \lambda_n$ is $(-1)^n$ times the constant term of the characteristic polynomial of $T$. The characteristic polynomial of $T$ now explicitly should look like \[z^n - (\tr T) z^{n-1} + \dots + (-1)^n (\det T).\]
    \item An operator on $V$ is invertible iff its determinant is nonzero.
    
    Note that $T$ on a complex vector space is invertible iff $T$ does not have 0 as an eigenvalue, which happens iff the constant term of the characteristic polynomial if nonzero, i.e., $\det T \neq 0$. If $T$ is instead on a real vector space, then we need one additional step in the middle: $T$ does not have 0 as an eigenvalue iff $T_\C$ does not have 0 as an eigenvalue.
    \item We now relate the idea of the determinant to the characteristic polynomial. The following statement can be taken as an alternatively definition of the characteristic polynomial: 
    \begin{center}
        For $T$ on $V$, the characteristic polynomial of $T$ equals $\det(zI - T)$.
    \end{center}
    
    Suppose $V$ is a complex vector space. Let $\lambda \in \C$, and for now we see $z$ as a fixed complex indeterminate. Then $\lambda$ is an eigenvalue of $T$ iff $z-\lambda$ is an eigenvalue of $zI - T$, as can be seen from \[-(T - \lambda I) = (zI-T) - (z-\lambda)I.\] Raise both sides to the power $n = \d V$ and take nullspaces. Then the multiplicities of $\lambda$ in $T$ should match the multiplicities of $z-\lambda$ in $zI - T$. Let $\lambda_1,\dots,\lambda_n$ be all eigenvalues of $T$ (including repeats according to multiplicities), then $z - \lambda_1, \dots,z - \lambda_n$ are all the eigenvalues of $zI - T$ (including repeats according to multiplicities). Therefore, for the complex indeterminate $z$, \[\det(zI - T) = (z - \lambda_1)\cdots(z-\lambda_n),\] where the RHS is exactly the characteristic polynomial of $T$.
    
    If $V$ is a real vector space instead, then by definition  the characteristic polynomial of $T$ is the characteristic polynomial of $T_\C$, and $\det(zI - T)$ is the product of eigenvalues of $(zI - T)_\C = zI_\C - T_\C$ (because when $z$ is a complex number, $zI - T$ is a polynomial in $T$). Then we can use the claim above in the complex case.
\end{itemize}

\subsection{Trace of a Matrix}
\begin{itemize}
    \item The \df{trace of a square matrix} $A$, which we denote by $\tr A$, is defined as the sum of the diagonal entries of $A$. We will show that the trace of an operator $T \in \LV$ is exactly the trace of $\M(T)$ with respect to any basis of $V$.
    \item The trace of the transpose of a matrix is the same as the trace of the matrix itself.
    \item $\tr(AB) = \tr(BA)$ for matrix $A$ of dimension $m \times n$ and $B$ of dimension $n \times m$. Proof is easy.
    \item Similar matrices have the same trace by the claim above. Let $P = A^{-1}QA$, then \[\tr P = \tr(A^{-1}QA) = \tr(QA^{-1}A) = \tr Q.\]
    \begin{itemize}
        \item In particular, for two bases $u_1,\dots,u_n$ and $v_1,\dots,v_n$ of $V$, \[\tr \M(T,(u_1,\dots,u_n)) = \tr \M(T,(v_1,\dots,v_n)).\]
    \end{itemize}
    \item By the theorem above, the trace of the matrix of an operator is irrespective of the basis chosen. \emph{We will soon see that the determinant of the matrix of an operator is irrespective of the basis chosen as well.} From the block diagonal decomposition of $T$ on a complex vector space (from \lk{8}{2}), we may conclude that $\tr \M(T)$ should be the sum of eigenvalues of $T$ (repeated according to their multiplicities), which is exactly $\tr T$.
    
    If $T$ is on a real vector space, then consider $T_\C$. Remember earlier we said that $\M(T) = \M(T_\C)$, so that $\tr \M(T) = \tr \M(T_\C)$. By definition $\tr T$ now is the sum of eigenvalues of $T_\C$ (repeated according to their multiplicities), which is exactly $\tr \M(T_\C)$ by the last paragraph. The two cases combined finish the proof.
    \begin{itemize}
        \item When computing the trace (and also the determinant, as we will see later) of an operator, we may just look at any of its matrix without computing the eigenvalues and their multiplicities.
    \end{itemize}
    \item Trace is additive: $\tr(S+T) = \tr S + \tr T$ for $S,T \in \LV$. This holds obviously if we look at $\M(S)$ and $\M(T)$ with respect to the same basis of $V$.
    \begin{itemize}
        \item Note that meanwhile for any $\lambda \in \F$, $\tr(\lambda T) = \lambda \tr(T)$ because $\M(\lambda T) = \lambda \M(T)$ with respect to the same basis of $V$.
        
        These facts tell us that the trace of an operator is a linear functional on $\LV$.
    \end{itemize}
    \item There do not exist operators $S,T \in \LV$ such that $ST - TS = I$. This statement has nothing to do with traces on the surface, but interestingly the use of trace makes this claim of nonexistence obvious.
    
    Consider $\M(S)$ and $\M(T)$ with respect to the same basis of $V$, then
    \begin{align*}
        \tr(ST - TS) & = \tr(ST) - \tr(TS) \\ & = \tr(\M(ST)) - \tr(\M(TS)) \\ & = \tr(\M(S)\M(T)) - \tr(\M(T)\M(S)) = 0,
    \end{align*}
    yet $\tr I = 1 \cdot \d V = \d V \neq 0$.
\end{itemize}

\subsection{Determinant of a Matrix}
\begin{itemize}
    \item We use $(m_1,m_2,\dots,m_n)$ as a shorthand for the permutation $\begin{pmatrix} 1 & 2 & \cdots & n \\ m_1 & m_2 & \cdots & m_n \end{pmatrix}$. Note this is not the cycle notation. The \df{sign of a permutation} $(m_1,m_2,\dots,m_n)$ is defined to be
    \begin{enumerate}
        \item [i)] 1 if the number of pairs of integers $(j,k)$ with $1 \leq j \leq k \leq n$ such that $j$ appears after $k$ in the list $(m_1,m_2,\dots,m_n)$ is even;
        \item [ii)] $-1$ if the number of such pairs is odd.
    \end{enumerate}
    In other words, the sign of $(m_1,\dots,m_n)$, which we denote by $\sgn(m_1,\dots,m_n)$, is 1 (resp.\ $-1$) if the natural order of the list has been changed an even (resp.\ odd) number of times.
    
    Examples 10.26 and 10.28 in the book provide good intuition to why we should define the sign of a permutation in this way, and how this helps the definition of the determinant of a matrix.
    
    \item Interchanging two entries in a permutation (i.e., a single transposition) multiplies the sign of the permutation by $-1$.
    
    Consider two permutations, where the second permutation can be obtained from the first in one transposition. This two interchanged entries from $b,a$ to $a,b$ first leads $+1$ or $-1$ pairs not in the natural order.

    Consider any entry $c$ between the two interchanged entries $b$ and $a$.
    \begin{enumerate}
        \item [i)] If $b < c < a$ originally, then $a > c > b$ now, so $+2$ pairs not in the natural order.
        \item [ii)] If $b > c$ and $c < a$ (or $b < c$ and $c > a$) originally, then switching $b$ and $a$ does not change the number of pairs not in the natural order.
        \item [iii)] If $b > c > a$ originally, then similar to i) we have $-2$ pairs not in the natural order after switching $b$ and $a$.
    \end{enumerate}
    Therefore, the parity of the number of pairs not in the natural order changes in a transposition. This means that the sign of a permutation is multiplied by $-1$ in a transposition.
    \begin{itemize}
        \item A corollary crucial to the study of permutations in abstract algebra: a permutation must be either an even permutation or an odd permutation, i.e., it can be broken into only an odd number of transpositions or an even number of transpositions. (Recall that every permutation can be broken into transpositions.)

        We know $(1,\dots,n)$ has sign 1. If $\sgn(m_1,\dots,m_n) = 1$ (resp.\ $-1$), then the number of transpositions must be even (resp.\ odd).

        \emph{The sign of a permutation proves that the parity of a permutation is well-defined.} The two equivalent definitions have their use in their own areas. Here because the determinant of a matrix is a number, we use the sign property. But in abstract algebra when we are dealing with permutations as functions, the parity property is more important.
    \end{itemize}
    \item The Leibniz formula for the definition of determinants: for the $n \times n$ square matrix \[A = \begin{bmatrix}
        A_{1,1} & \cdots & A_{1,n} \\ \vdots & & \vdots \\ A_{n,1} & \cdots & A_{n,n}
    \end{bmatrix},\] $\det A$, the \df{determinant of the square matrix} $A$, is given by \[\sum_{(m_1,\dots,m_n) \in S_n} \sgn (m_1,\dots,m_n) A_{m_1,1} \cdots A_{m_n,n},\] where $S_n$ is the symmetric group of permutations on $n$ letters.
    \begin{itemize}
        \item It is equivalent to define $\det A$ as \begin{equation}
            \sum_{(k_1,\dots,k_n) \in S_n} \sgn (k_1,\dots,k_n) A_{1,k_1} \cdots A_{n,k_n}.
        \end{equation}

        For fixed $m_1,\dots,m_n$, let $\sigma = (m_1,\dots,m_n)$. Then \[A_{m_1,1}\cdots A_{m_n,n} = A_{1,\sigma^{-1}(1)}\cdots A_{n,\sigma^{-1}(n)}.\] Remember that $\sgn(\sigma^{-1}) = \sgn(\sigma)$: since the identity permutation $\iota = \sigma^{-1}\sigma$ is even, $\sigma^{-1}$ and $\sigma$ must have the same parity, and thus the two have the same sign. Therefore, we have shown that \[\sgn(\sigma) A_{m_1,1}\cdots A_{m_n,n} = \sgn(\sigma^{-1}) A_{1,\sigma^{-1}(1)}\cdots A_{n,\sigma^{-1}(n)}.\] Clearly summing $\sigma$ over $S_n$ and summing $\sigma^{-1}$ over $S_n$ are the same, and thus we have our alternative definition (8).
    \end{itemize}
    \item The determinant of an upper-triangular matrix equals the product of all the diagonal entries.

    This is pretty obvious. Except for $A_{1,1}\cdots A_{n,n}$ with $\sgn(\iota) = 1$, all the other products in the sum vanish because one of the $A_{j,m_j}$ in each product is always below the diagonal.
    \item The determinant of a matrix is equal to that of its transpose.

    For fixed $m_1,\dots,m_n$, again let $\sigma = (m_1,\dots,m_n)$. Note that \[A_{1,m_1} \cdots A_{n,m_n} = A_{\sigma^{-1}(1),1} \cdots A_{\sigma^{-1}(n),n},\] and so essentially we need to show that $\sgn(\sigma^{-1}) = \sgn(\sigma)$, which we already know.
    \item Interchanging two columns/rows of a matrix changes the sign of the determinant.

    This is easy. Consider the product $A_{m_1,1} \cdots A_{m_n,n}$. For matrix $B$ obtained from $A$ by switching two columns, the sign in $B$ associated with the same product $A_{m_1,1} \cdots A_{m_n,n}$ is $-1$ multiplied by the original $\sgn(m_1,\dots,m_n)$ in $A$.

    Interchanging two rows of $A$ can be seen as interchanging two columns of $A^t$. By the fact that the determinant of a matrix is equal to that of its transpose, we see that interchanging two rows of a matrix as well changes the sign of the determinant as well.
    \begin{itemize}
        \item Matrices with two equal columns/rows have 0 determinant. (Consider switching the two equal columns.)
    \end{itemize}
    \item For square matrix $A = \begin{bmatrix}
        A_{\cdot,m_1} & \cdots & A_{\cdot,m_n}
    \end{bmatrix}$ and permutation $(m_1,\dots,m_n)$, \[\det \begin{bmatrix}
        A_{\cdot,1} & \cdots & A_{\cdot,n}
    \end{bmatrix} = \sgn(m_1,\dots,m_n) \det A.\]

    When $(m_1,\dots,m_n)$ is an even (resp.\ odd) permutation, $\begin{bmatrix}
        A_{\cdot,1} & \cdots & A_{\cdot,n}
    \end{bmatrix}$ is obtained from $A$ by switching columns an even (resp.\ odd) number of times, exactly when $\sgn(m_1,\dots,m_n) = 1$ (resp.\ $-1$).
    \item Let $k,n \in \Z^+$ with $1 \leq k \leq n$. Fix the $n \times 1$ columns $A_{\cdot,1},\dots,A_{\cdot,n}$ except for $A_{\cdot,k}$. The function $f: A_{\cdot,k} \mapsto \det \begin{bmatrix}
        A_{\cdot,1} & \cdots & A_{\cdot,n}
    \end{bmatrix}$ is a linear map from $\F^n$ to $\F$.
    
    Recall from \lk{3}{4} that linear maps from $\F^n$ to $\F^m$ can be uniquely represented by a matrix-left-multiplication. Since $m=1$ here, basically we want to show that $\det \begin{bmatrix}
        A_{\cdot,1} & \cdots & A_{\cdot,n}
    \end{bmatrix}$ is a linear combination of the entries of $A_{\cdot,k}$. This is clearly true by the Leibniz formula.
    \item For square matrices $A,B$ of the same size, $\det(AB) = \det(A)\det(B) = \det (BA)$.

    \begin{align*}
        \det(AB) & = \det \begin{bmatrix}
            AB_{\cdot,1} & \cdots & AB_{\cdot,n}
            \end{bmatrix} \\
            & = \det \begin{bmatrix}
                A(\sum_{m_1 = 1}^n B_{m_1,1}e_{m_1}) & \cdots & A(\sum_{m_n = 1}^n B_{m_n,n}e_{m_n})
            \end{bmatrix} \\
            & = \det \begin{bmatrix}
                \sum_{m_1 = 1}^n B_{m_1,1} Ae_{m_1} & \cdots & \sum_{m_n = 1}^n B_{m_n,n} Ae_{m_n}
            \end{bmatrix} \\
            & \quad \, \text{(because the $B_{m_i,i}$'s are constants)} \\
            & = \sum_{m_1 = 1}^n \cdots \sum_{m_n = 1}^n B_{m_1,1} \cdots B_{m_n,n} \det \begin{bmatrix}
                Ae_{m_1} & \cdots & Ae_{m_n}
            \end{bmatrix} \\
            & \quad \, \text{(by repeated applications of the linearity of det as a function of one column)}
    \end{align*}
    Since the determinant of a a matrix with two equal columns is 0, the sum above is equivalent to  \[\sum_{(m_1,\dots,m_n) \in S_n} B_{m_1,1} \cdots B_{m_n,n} \det \begin{bmatrix}
        Ae_{m_1} & \cdots & Ae_{m_n}
    \end{bmatrix}.\]
    Therefore, \begin{align*}
        \det(AB) & = \sum_{(m_1,\dots,m_n) \in S_n} B_{m_1,1} \cdots B_{m_n,n}(\sgn(m_1,\dots,m_n)) \det(A) \\ & = \det(A) \sum_{(m_1,\dots,m_n) \in S_n} \sgn(m_1,\dots,m_n) B_{m_1,1} \cdots B_{m_n,n},
    \end{align*}
    which shows that $\det(AB) = \det(A)\det(B) = \det(B)\det(A) = \det(BA)$.
    \item By $\det(AB) = \det(BA)$, similar matrices have the same determinant, just like the trace.
    \begin{itemize}
        \item In particular, the determinant of the matrix of an operator $T$ is irrespective of the basis chosen, which we can simply write as $\det \M(T)$.
         \item It is also the case that the determinant of an operator is exactly the determinant of its matrix.
    \end{itemize}
    The proofs of these results are the same as the proofs of the same results about the trace.
    \item While trace is additive, determinant is multiplicative: $\det(ST) = \det(S)\det(T) = \det(TS)$ for $S,T \in \LV$. Just like the similar result about trace, the result here holds obviously if we fix a basis of $V$ and work with $\M(S)$ and $\M(T)$.
\end{itemize}

\subsection{Geometric Interpretation of the Determinant}
\begin{itemize}
    \item For an isometry $S$ on an inner product space $V$, $\abs{\det S} = 1$.
    
    In the case where $\F = \C$, recall from \lk{7}{3} that all eigenvalues of $S$ has absolute value 1. Their product (counting multiplicities), which is $\det S$, must have absolute value 1 as well.
    
    In the case where $\F = \R$, the book offers two proofs:
    \begin{itemize}
        \item The first proof uses the result that for real inner product space $V$, it can be routinely verified from definition that \[\inp{u+iv}{x+iy} = \inp{u}{x} + \inp{v}{y} + (\inp{v}{x} - \inp{u}{y})i\] for $u,v,x,y \in V$ defines a complex inner product on $V_\C$.
        
        Consider $S_\C(u+iv) = Su +iSv$ and $S_\C(x+iy) = Sx + iSy$. To show that $S_\C$ is inner-product-preserving, we look at  $\inp{S_\C(u+iv)}{S_\C(x+iy)}$:
        \begin{align*}
            \inp{Su +iSv}{Sx + iSy} & = \inp{Su}{Sx} + \inp{Sv}{Sy} + (\inp{Sv}{Sx} - \inp{Su}{Sy})i \\ & = \inp{u}{x} + \inp{v}{y} + (\inp{v}{x} - \inp{u}{y})i \\ & \quad \, \text{[exactly because $S$ is inner-product-preserving]} \\ & = \inp{u+iv}{x+iy}.
        \end{align*}
        This shows that $S_\C$ is an isometry on $V_\C$, and so by the case $\F = \R$, $\abs{\det S_\C} = 1$. Since $\det S$ is $\det S_\C$ for $S$ on a real inner product, the proof is completed.
        
        In summary, we have chosen an inner product for the complexification $V_\C$ such that $S_\C$ is an isometry on it. This \emph{enables} us to use the complex case we have already established.
        \item The second proof uses the result that the determinant of a block diagonal matrix is equal to the product of the determinants of each block. The proof of this is some complicated calculations involving the Leibniz formula. In \lk{9}{2} we described every isometries on a real inner product space by a block diagonal matrix of a specific form (when the right basis is chosen). The diagonal blocks must be $\begin{bmatrix}
        1
        \end{bmatrix}$, $\begin{bmatrix}
        -1
        \end{bmatrix}$, or the $2\times 2$ rotational matrix, which has determinant 1. Therefore $\abs{\det S} = \abs{\det \M(S)} = 1$.
        \item The sign of the $\det T$ for an invertible operator $T$ on a real inner product space $V$ can be geometrically comprehended. (An invertible operator $T$ must not have 0 as an eigenvalue, and thus $\det T$ must be either positive or negative.)
        
        By the polar decomposition theorem, $T = S \sqrt{T^*T}$ for an isometry $S \in \LV$. It follows that $\det(T) = \det(S) \det(\sqrt{T^*T})$.
        
        First recall that $\sqrt{T^*T}$ is the unique positive square root of $T^*T$, and then must have all its eigenvalues nonnegative. By the real spectral theorem, because $\sqrt{T^*T}$ is self-adjoint, with respect to an orthonormal basis of $V$ the matrix $\M(\sqrt{T^*T})$ becomes diagonal and has all eigenvalues repeated according to their multiplicities. Therefore $\det \sqrt{T^*T}$ is the product of these eigenvalues of $\sqrt{T^*T}$ (repeated accordingly), which is nonnegative. Since $\det T \neq 0$, $\det \sqrt{T^*T} > 0$, which means that the sign of $\det T$ is exactly the sign of $\det S$.
        
        In the second proof of $\abs{\det S} = 1$ above, we see that whether $\det S = 1$ or $-1$ depends on whether the number of $\begin{bmatrix}
        -1
        \end{bmatrix}$ blocks on the diagonal is even or odd. Recall the construction of the block diagonal matrix in \lk{9}{2} maximizes the number of invariant subspaces of dimension 1, and thus the dimension of $E(-1,S) = \{v \in V \where Sv = -v\}$ is exactly the number of $\begin{bmatrix}
        -1
        \end{bmatrix}$ blocks on the diagonal. We may conclude that whether $\det T$ is positive or negative depends on whether $\d E(-1,S)$, i.e., the subspace of $V$ on which vectors reverse directions through $S$, has even or odd dimension.
        
        % We know $T = S \sqrt{T^*T}$, where $\sqrt{T^*T}$ has nonnegative eigenvalues and thus cannot reverse the direction of vectors. Also, any $v \in V$ can be written as a linear combination of the orthonormal basis vectors of $V$ (with respect to which $\M(S)$ has the nice block diagonal form). We may then say \emph{whether the number of times vector $v$ reverses its direction is even or odd decides whether $\det T$ is positive or negative}. By reversing direction we mean reversing the sign of a component vector of $v$. It may be more illuminating to think about the statement in $\R^n$ in terms of matrices.
    \end{itemize}
\end{itemize}

The book continues with a pseudo-proof of the change of variables theorem. The pseudo-proof clearly illustrates where the determinant in the formula comes from. Some measure theory is required to make the proof actually rigorous. Please check out the book for the exposition.
\setcounter{secnumdepth}{3}
\end{document}