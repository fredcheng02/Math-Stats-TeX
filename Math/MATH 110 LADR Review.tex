\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsthm,amssymb,tikz-cd}
\usepackage{enumitem}
\usepackage{fullpage}
\usepackage{setspace}
\usepackage{mathtools}
\usepackage[bookmarks]{hyperref}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\s}{\operatorname{span}}
\newcommand{\n}{\operatorname{null}}
\renewcommand{\r}{\operatorname{range}}
\renewcommand{\d}{\dim}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\inp}[2]{\langle #1, #2 \rangle}
\newcommand{\nm}[1]{\| #1 \|}
\newcommand{\LV}{\mathcal{L}(V)}
\newcommand{\LVW}{\mathcal{L}(V,W)}
\newcommand{\M}{\mathcal{M}}
\newcommand{\PF}{\mathcal{P}(\F)}
\newcommand{\bv}{v_1,\dots,v_n}
\newcommand{\bw}{w_1,\dots,w_n}
\renewcommand{\phi}{\varphi}

\onehalfspacing
\setlength{\parskip}{0em}

\begin{document}

\begin{center}
    {\Large MATH 110 Notes}
    \\based on \textit{Linear Algebra Done Right}, third edition
    \vspace{0.5em}
    \\ Cheng, Feng
    \vspace{-0.5em}
\end{center}

\section{Vector Spaces}
\begin{itemize}
    \item A vector space $V$ is an abelian group under addition that has the distributive and associative properties in scalar multiplication and the multiplicative identity. (Here addition and scalar multiplication are closed operations in $V$.)
    \item $\F^S$ denotes the set of functions from $S$ to $\F$. It is a vector space over $\F$ under the sum and scalar multiplication of functions defined in the usual pointwise way.
    \item A vector space holds the cancellation law and has a unique additive identity and a unique additive inverse because it is a group. $0v = 0$, $a0 = 0$, and $(-1)v = -v$ hold by the distributive property.
    \item A subset $U$ of the whole vector space $V$ that is still a vector space is called a vector subspace. Usually we use the three criteria i) $0_V \in U$ (or $U$ is nonempty), ii) closed under addition in $U$, and iii) closed under scalar multiplication in $U$ to verify. To show the criteria are equivalent to the definition, in one direction we show that $0_V$ is the (unique) identity element of $U$ and in the other direction we only need to check the additive inverse property (satisfied by $(-1)u = -u$).
    \item The sum $U_1 + U_2 +\cdots+ U_m$, where $U_i$'s are subsets of $V$, is the set of all possible sums of the respective elements from each $U_i$.
    \item When the $U_i$'s are subspaces of $V$, then the sum is the smallest vector space containing all $U_i$'s.
    \item When the $U_i$'s are subspaces of $V$, and every element of the sum $U_1 + U_2 +\cdots+ U_m$ can be written in only one way as $u_1+u_2+\cdots+u_m$, we call the sum a \textit{direct sum}, denoted by $\oplus$ replacing $+$.
    \item The equivalent criterion for direct sum is that the only way to express $0$ is to write it as the sum of $0$'s from each $U_i$. One direction is trivial, while the other direction employs the common trick of subtracting two different expressions of the same vector $v \in V$.
    \item If we are only discussing two subspaces $U$ and $W$ of $V$, then $U+W$ is a direct sum iff their intersection is the $\{0\}$. The proof of this, in both directions, considers $0 = v + (-v)$. The fact $W$ always contains an element and its inverse will give us the answer.
    \item Arbitrary intersection of subspaces of $V$ is a subspace of $V$, and the union of two subspaces is still a subspace iff one is contained in the other.
\end{itemize}

\section{Finite-Dimensional Vector Spaces}

\textit{Remark}. Note that this section in the book builds on the idea of a ``list" of vectors, which is no different than using a ``set'' of vectors because vector addition is commutative. (We will use either a list or a set of vectors later depending on whether we have to specify the ordering of these vectors.) Finite-dimensional is defined in terms of a spanning list, and the concept of a basis and the dimension of a vector space is pushed backward. In our exposition, we rearrange the progression of these ideas and reconstruct some proofs.

\begin{itemize}
    \item The \textit{span} of a set $S$ of vectors in $V$ is defined to be all possible linear combinations of these vectors. In particular, $\s(\emptyset)$ $\coloneqq \{0\}$.
    \item Similar to the fact that the sum $U_1 + U_2 +\cdots+ U_m$ is the smallest vector space containing all $U_i$'s, span($S$) is the smallest subspace of $V$ containing all the vectors in $S$. The proof of the two are almost exactly the same.
    \item A polynomial $p$ in an indeterminate $z$ with coefficients in $\F$ is the \textbf{expression} $a_0 + a_1 z + \cdots + a_n z^n$ with all $a_i$'s in $\F$. The \textit{degree} of a polynomial is the largest exponent of $z$ with nonzero coefficient, and if all the coefficients are 0, we customarily define its degree to be $-\infty$ in the book, or $-1$ elsewhere.
    \begin{itemize}
        \item Since we are dealing with $z$ and coefficients in $\F = \R$ or $\C$ in this book, a polynomial here can be alternatively defined as the \textbf{function} $p: \F \rightarrow \F$ with $p(z) = a_0 + a_1 z + \cdots + a_n z^n$. The reason behind is that a polynomial function with coefficients from an infinite field can be written into only one polynomial expression with its unique coefficients. See Chapter 4 of this book or Appendix E, Theorem 10 of \textit{Linear Algebra} by Friedberg, Insel, and Spence. It is the consequence of the fact that a polynomial of degree $n \geq 1$ has at most $n$ distinct zeros. We will proceed with this definition of polynomials.
        \item We denote the set of all polynomials with coefficients in $\F$ by $\mathcal{P}(\F)$. Under the usual coefficient-wise addition and scalar multiplication, it is obvious that $\mathcal{P}(\F)$ is a vector space over $\F$. In addition, the set of all polynomials with degree at most $m$, denoted by $\mathcal{P}_m(\F)$ is a vector subspace of $\mathcal{P}(\F)$.
    \end{itemize}
    \item Linear independence and dependence can be characterized in multiple ways. A set $S$ of vectors is linearly independent iff the only way to write 0 as $a_1v_1 + a_2v_2 + \cdots +a_mv_m$ is the trivial way. (We also sometimes say that some vectors themselves are linearly independent.) It is equivalent to saying that every linear combination has its only representation, or none of the vectors is in the span of the other vectors (can be written as a linear combination of them). Linear dependence is the negation of independence, but in particular we should remember that it means there exists at least one vector $v_i$ that is in the span of the rest of the vectors.
    \begin{itemize}
        \item Obviously removing such vector $v_i$ from $S$ will not change the span.
        \item In particular, for a linearly independent set $S = \{s_1, s_2, \dots , s_m\} \subsetneq$ a linearly dependent set $T = \{s_1, s_2, \dots , s_m, t_1, t_2, \dots , t_n\}$, one such vector is always in $T \backslash S$. This is because for the nontrivial way of writing $0 = a_1s_1 + \cdots +a_ms_m + b_1 t_1 + \cdots + b_n t_n$, if all $b$'s are 0, then all $a$'s will also be 0. Thus, some $b_i$ must be nonzero, and the corresponding $t_i$ can be removed without changing the span.
        \begin{itemize}
            \item The special case $n=1$ of the contraposition will be useful to us. If one new vector is added to the linearly independent $S$ but does not belong to span($S$), then the resulting set is still linearly independent.
            \item From this special case we know that for a linear dependent set $\{v_1,\dots,v_n\}$, at some index $k \leq n$ we have $\{v_1,\dots,v_{k-1}\}$ being linearly independent, while $v_k \in \s(\{v_1,\dots,v_{k-1}\})$. (This is the well-ordering case of the linear dependence lemma 2.21 in book that we will use again in Chapter 5.)
        \end{itemize}
        \item Note that a linearly independent set of vectors cannot have the $0$ vector.
    \end{itemize}
    \item We now have the essential tools to prove the following important theorem, that if a vector space $V$ can be spanned by $S = \{w_1, w_2, \dots,w_n\}$, then all linearly independent sets of vector $\{u_1,u_2,\dots,u_m\} \subseteq V$ must have $m \leq n$. The proof suggests adding a new $u_i$ into $S$ and then remove another $w_j$ from $S$ in each step. In the end all the $u$'s will be added and the corresponding number of $w$'s are removed from $S$. Therefore, $m \leq n$.
    \begin{itemize}
        \item The direct consequence of this theorem is the definition of dimension. But before that we need to introduce the notion of a basis and its existence.
    \end{itemize}
    \item A \textit{basis} of $V$ is a linearly independent spanning set of $V$. A set is a basis iff all vectors in $V$ can be written as a unique linear combination of the basis vectors. This follows directly from the definition.
    \item Most of time we are concerned with vector spaces with finite bases. The vector spaces that have a finite spanning set are called \textit{finite-dimensional vector spaces} (FDVS), and we will soon know why they bear this name. First of all, we can show that every finite spanning set of a FDVS $V$ can be reduced to a finite basis. Furthermore, every linearly independent set of a FDVS $V$ can be extended to a basis.
    \begin{itemize}
        \item To prove the first claim, we construct our basis $B$ from scratch and try to add all the elements of the spanning set $S = \{v_1, v_2, \dots, v_n\}$ one by one into the basis, as long as the element added is not in the span of the existing vectors in $B$. In this way, every time a new vector is added to $B$, $B$ will still be linearly independent, and this $B$ will be linearly independent in the end. Furthermore, span$(B) = $ span$(S) = V$ because all the $v$'s left out are in span($B$).
        \begin{itemize}
            \item It is also possible to start $B$ from $S$ and check from $v_1$ to $v_n$ whether each vector is in the span of the rest of the vectors in $B$. If it is, then we remove it from $B$; if it is not, then we keep it. In the end, span($B$) $=$ span($S$) and none of the vectors in $B$ is in the span of the others. However, this is top-down approach is harder in practice to actually find the basis.
            \item This claim shows that every FDVS has a finite basis.
        \end{itemize}
        \item To prove the second claim, similarly construct our basis from the given linearly independent set $L$. We pick a finite basis $B = \{b_1, b_2,\dots,b_n\}$ of $V$ and check whether the next $b$ may be added to $L$ (same to what we did above). In the end, all the $b$'s are in the span of $L$ and $L$ remains linearly independent.
    \end{itemize}
    
    \item Choose two finite bases of a vector space. The fact that they are both linearly independent and both span $V$ tells us that the two bases should have the same size, which we define as the \textit{dimension} of a vector space. Thus, the vector spaces with finite bases are called \textit{finite-dimensional}.
    \begin{itemize}
        \item The fact that the bases of $V$ has a fixed size implies every spanning set or linearly independent set of this size $\d V$ is a basis itself (because the reduction/extension here is the trivial one).
    \end{itemize}
    \item The subspace dimension $\d U$ is always $\leq$ the whole space dimension $\d V$. To accomplish this we have to resort to the classic procedure: we add vectors from $U$ that are not in the span of the existing ones in $L$. Thus $L$ always remains linearly independent, and it should always be smaller in size than $B$, a spanning set of $U$. Our procedure will eventually terminate and no more vectors can be added to $L$, i.e., all the vectors in $V$ is in $\s(L)$. $L$ is our desired basis, and $|L| = \d U \leq \d V = |B|$.
    \begin{itemize}
        \item Note it is WRONG to assume that a finite basis of the whole space can always be reduced to a finite basis of the subspace!
        \item Corollary: for FDVS $V$ and its subspace $U$, if the two spaces have the same dimension, then they are the same space. This is an important result useful in many proofs.
    \end{itemize}
    \item There are two more things that deserve mentioning. For FDVS $V$ and its subspace $U$, we can always find subspace $W$ such that $V = U \oplus W$. The idea is to find a basis $B_U$ of $U$ and extend it to the basis $B_V$ of $V$. $\s (B_V \backslash B_U)$ is our desired W. We then only have to show $V = U+W$ and $U \cap W = \{0\}$, which is not difficult.
    \item The dimension of the sum of two vector spaces $U_1$ and $U_2$ can be expressed as follows: $$\d (U_1 + U_2) = \d U_1 + \d U_2 - \d (U_1 \cap U_2).$$
    \begin{itemize}
        \item Despite the fact that it looks like the inclusion-exclusion formula, $+$ and $\cup$ are quite different, thus the formula cannot be similarly generalized to higher orders. To prove the formula, start from a basis $\{u_1,u_2,\dots,u_m\}$ of $U_1 \cap U_2$ (intersection is a subspace). $U_1$ and $U_2$ then have their respective bases $B_1$ and $B_2$. We can show $B_1 \cup B_2$ is a basis of $U_1 + U_2$. See page 47 of the book for the full proof.
        \item Note that when the sum is a direct sum, then the formula is just $\d(U_1 \oplus U_2) = \d U_1 + \d U_2$.
    \end{itemize}
    
\end{itemize}

\section{Linear Maps}
\subsection{The Vector Space of Linear Maps}
\begin{itemize}
    \item A linear map(ping)/transformation is a homomorphism of vector spaces under addition and scalar multiplication. The linear structure is preserved after the transformation $T$. To use symbols, a linear map is a function $T: V \rightarrow W$ such that for all $u,v \in V$ and $\lambda \in \F$ (the field of $V$ and $W$),
    \begin{equation*}
        T(u+v) = Tu + Tv \quad \text{and} \quad
        T (\lambda u) = \lambda T(u).
    \end{equation*}
    \item The zero transformation maps every vector of $V$ to $0_W$. The identity transformation $I_V: V \rightarrow V$ maps each element of $V$ to itself in $V$.
    \item We define the addition and scalar multiplication of linear maps in the vector-wise way: $$(S+T)(v) = Sv + Tv \quad \text{and} \quad (\lambda T)(v) = \lambda T(v),$$ following which we can easily show that the set of linear maps $\mathcal{L}(V,W)$ from $V$ to $W$ is a vector space over $\F$. In addition to the linear properties, we define the \textit{product} $ST$ of linear maps same as $S \circ T$, given the appropriate domain and codomain. Why the product is essential will become clear later. The theory of functions tells us this product is associative [$T_1(T_2 T_3) = (T_1 T_2) T_3$] and the identity works as usual ($I_W T = T I_V = T$). The distributive properties ($(S_1 + S_2) T = S_1 T + S_2 T \text{ and } S(T_1 + T_2) = S T_1 + S T_2$) obviously hold as well.
    \item If $T(x_1,\dots,x_n) = (a_{11}x_1+\cdots+a_{1n}x_n,\dots,a_{m1}x_1+\cdots+a_{mn}x_n)$, then $T$ is obviously a linear map from $\F^n$ to $\F^m$. For a linear map $T \in \mathcal{L}(\F^n,\F^m)$, it must be of this form, and all the $a_{ij}$'s are determined by the mapping of the $n$ canonical basis vectors of $\F^n$.
    \item The uniqueness of the linear map that takes basis vectors $v_1, v_2, \dots, v_n$ to corresponding vectors $w_1, w_2, \dots, w_n$ is the most fundamental theorem in this section. It is our first step toward showing that linear maps and matrices are isomorphic. To prove the theorem, for arbitrary $a_i$'s, let $$T: \sum_{i=1}^n a_i v_i \mapsto \sum_{i=1}^n a_i w_i.$$
    \item Here are a few properties that are useful in linear maps. The proofs are quite routine.
    \begin{itemize}
        \item $T(0_V) = 0_W;$
        \item $T$ is linear iff $T(cx+y) = cT(x) + T(y)$ for all $c,x,y.$
    \end{itemize}
\end{itemize}

\subsection{Null Spaces and Ranges}
\begin{itemize}
    \item The \textit{nullspace}/\textit{kernel} $\n T$ is the preimage of $0_W$. The \textit{range}/\textit{image} $\r T$ is defined the same as the range of a function. The nullspace and the range are respectively subspaces of $V$ and $W$ (by checking the three criterion). If the nullspace (resp. range) is finite-dimensional, then its dimension is the \textit{nullity} (resp. \textit{rank}) of $T$.
    \begin{itemize}
        \item Since $\d U = \d W$ for subspace $U$ of $W$ implies $U = W$, $\text{rank }T = \d W$ implies surjection.
        \item An easy theorem to always keep in mind is that the map of the basis vectors of $V$ spans $\r T$. We will use it in the proof of the next theorem.
    \end{itemize}
    \item We are now endowed with all the tools to prove the \textbf{rank-nullity theorem}: for a FDVS $V$ and $T$ from $V$ to $W$, $\r T$ is finite-dimensional with $$\d V = \d \n T + \d \r T.$$
    \begin{itemize}
        \item As always, we need to construct a basis to work with first. Let ${u_1, u_2, \dots, u_n}$ be a basis of $\n T$, and thus can be extended to ${u_1,\dots,u_m,v_1,\dots,v_n}$ a basis of $V$. We prove $T(\{v_1, v_2, \dots, v_n\})$ is a basis for $\r T$. The fact that the $u$'s are in the nullspace shows that $Tv_1, \dots,Tv_n$ spans $\r T$ (by the ``easy" theorem above). To show the elements are linearly independent, consider that $$a_1 T(v_1)+\cdots+a_n T(v_n) = 0$$ implies $a_1v_1+\dots+a_nv_n \in \n T$. Yet we also have $b_1u_1+\dots+b_nu_n \in \n T$, so we can set the two linear combinations equal. Because the $u$'s and $v$'s are altogether linearly independent, the coefficients $a$'s (and $b$'s) are all 0.
    \end{itemize}
    \item The rank-nullity theorem gives rise to a number of equivalent characterizations regarding injection and surjection. First of all, there is a criterion that says $T$ is injective iff $\n T = {0}$ (proof follows straight from the definition of nullspace). The results below follow.
    \begin{itemize}
        \item A map to a smaller dimensional space cannot be injective because $\d (\n T) > 0$.
        \item A map to a large dimensional space cannot be surjective because $\d (\r T) \leq \d V < \d W$.
        \item $V$ and $W$ are FDVS of equal dimension, then $$T \text{ is injective } \Longleftrightarrow T \text{ is surjective } \Longleftrightarrow \d \r T = \d V.$$
    \end{itemize}
    \item Solving systems of linear equations is a direct application of this.
    \begin{itemize}
        \item A homogeneous system of linear equations with more variables than equations has nonzero solutions. Here we can view the system as a linear map $T$ that maps the vector of unknown variables $x \in \F^n$ to $0 \in \F^m$ with $n > m$. The nullity of $T$ is greater than 0 as a result.
        \item Similarly, the $T$ associated with an inhomogeneous equation with more equations than variables is not surjective, and thus some choices of constants on the right side will give no solutions to the system.
    \end{itemize}
\end{itemize}

\subsection{Matrices}
\begin{itemize}
    \item Consider the $n$-dimensional $V$ with basis $\bv$ and $m$-dimensional $W$ with basis $w_1,\dots,w_m$. For $T \in \LVW$, we can find for every $v_k$, a list of $A_{i,k}$'s such that $Tv_k = A_{1,k}w_1 +\cdots+ A_{m,k}w_m = \sum_{j=1}^m A_{j,k}w_j$. As a result, we have $n$ lists of scalar $a$'s, each of length $m$, and we can put them into an $m \times n$ matrix $A$, with entries $a_{i,j}$ in the $i$-th row and $j$-th column. $\mathcal{M}(T,(\bv),(\bw))$ uniquely determines the linear map $T \in \LVW$.
    \begin{itemize}
        \item Assume
        \begin{align*}
            Tv_1 & = A'_{1,1} w_1 + \dots + A'_{1,m} w_m, \\
            & \vdotswithin{=} \\
            Tv_n & = A'_{n,1} w_1 + \dots + A'_{n,m} w_m,
        \end{align*}
        from which we get the $m \times n$ matrix $A'$. Its transpose $n \times m$ $A$ is our defined matrix representation of $T$. The reason for taking transpose will become clear later.
    \end{itemize}
    \item We define matrix addition entry-wise, because under this definition, matrix representation of linear maps preserves addition, i.e. $\M(S+T) = \M(S) + \M(T)$ for $S,T \in \LVW$, assuming $S+T$, $S$, and $T$ share the same ordered bases.
    \item Similarly entry-wise scalar multiplication with matrices give us $\M(\lambda T) = \lambda \M(T)$.
    \item Given the definition of matrix addition and scalar multiplication, $\F^{m,n}$, the set of matrices with $m$ rows and $n$ columns over $\F$, is a vector space of dimension $mn$.
    \item The reason why we represent linear maps from $n$-dimensional VS to $m$-dimensional VS is that we want to preserve the ordering of $``S"$ and $``T"$ after matrix representation, i.e., $\M(ST) = \M(S)\M(T)$ instead of $\M(T)\M(S)$, under the usual definition of matrix multiplication: $$[AC]_{j,k} = \sum_{r = 1}^n A_{j,r}C_{r,k}.$$
    \begin{itemize}
        \item Remember that matrix multiplication is not commutative, but is associative and distributive.
    \end{itemize}
    \item The book defines $A_{j,\cdot}$ and $A_{\cdot,k}$ as the $j$-th row and the $k$-th column of matrix $A$, and thus $[AC]_{j,k} = A_{j,\cdot}C_{\cdot,k}$. Also, $k$-th column of matrix product equals matrix times the $k$-th column of the second matrix: $$[AC]_{\cdot,k} = AC_{\cdot,k},$$ which is an alternative to understanding matrix multiplication. Obviously there is a row equivalent version that says $j$-th row of the matrix product equals $j$-th row times the second matrix.
    \item When we are considering matrix $A$ times a column vector $c$, we can see $Ac$ as the linear combination $c_1A_{\cdot,1} + \cdots + c_nA_{\cdot,n}$.
\end{itemize}

\subsection{Invertibility and Isomorphic Vector Spaces}
\begin{itemize}
    \item $T \in \LVW$ is \textit{invertible} if there exists $S$ such that $ST = I_V$ and $TS = I_W$. We denote this inverse $S$ by $T^{-1}$. One can show that the inverse must be linear and thus belongs to $\mathcal{L}(W,V)$. By the associativity of linear maps, the inverse is unique.
    \item Invertibility is equivalent to bijectivity between vector spaces. Linear maps are functions, so beyond showing that the inverse is linear, the proof is exactly the same.
    \item An \textit{isomorphism} is an invertible linear map; if an isomorphism exists between two vector spaces, then the vector spaces are \textit{isomorphic}.
    \item Two FDVS over $\F$ are isomorphic iff they have the same dimension. Injection gives us $\text{nullity } = 0$ and surjection gives us $\text{rank } T = \d W$. The rank-nullity theorem tells us $\d V = \d W$. On the other hand to prove isomorphism, we show $T(a_1v_1+\cdots+a_nv_n) = a_1w_1+\cdots+a_nw_n$ is both injective and surjective, which is not hard.
    \begin{itemize}
        \item The theorem above implies $n$-dimensional VS is always isomorphic to $\F^n$.
    \end{itemize}
    \item Now we show the isomorphism between a linear map and its corresponding matrix. When the orders of the bases $\bv$ and $w_1,w_2,\dots,w_m$ are fixed, then $\M$ is a function from $\LVW$ to $\F^{m,n}$. As we have noted earlier $\M$ is linear. Thus, it suffices to show that $\M$ is bijective.
    \begin{itemize}
        \item For all $A \in \F^{m,n}$, let $$Tv_k = \sum_{j = 1}^m A_{j,k}w_j.$$ Then this is the \textit{unique} (recall the theorem we emphasized in \S3.1) $T$ such that $A = \M(T)$.
        \item A linear map and its matrix representation are homomorphic not merely in regards to linearity, but also in regards to the preservation of the order of the product $\M(ST) = \M(S)\M(T)$.
    \end{itemize}
    \item By the theorems on isomorphism above, $\d \LVW = mn = (\d V) (\d W)$.
    \item We now shift our focus from the matrix representing an entire linear map to the matrix/column vector representing a vector in the same $V$. We fix the basis $\bv$, then for any $v \in V$, we define
    \begin{equation*}
        \M(v) = 
        \begin{bmatrix}
            a_1 \\ \vdots \\ a_n
        \end{bmatrix},
    \end{equation*}
    where $a_1,\dots,a_n$ are given by $v = a_1v_1+\cdots+a_nv_n$. Here $\M$ is an isomorphism from $V$ to $F^{n(, 1)}$.
    \begin{itemize}
        \item Following this definition, $\M(T)_{\cdot,k} = \M(Tv_k)$.
        \item More importantly, the linear map of a vector can now be completely described by matrix-multiplication. Symbolically, we want to prove $\M(Tv) = \M(T)\M(v)$ for all $v \in V$. Since $v = a_1v_1 + \cdots +a_nv_n$ and $\M$ is linear,
        \begin{align*}
            \M(Tv) & = a_1\M(Tv_1) + \cdots + a_n \M(Tv_n) \\ & = a_1\M(T)_{\cdot, 1} + \cdots + a_n\M(T)_{\cdot, n} \\ & = \M(T)\M(v).
        \end{align*}
        \item How do we interpret this? Consider the following diagram under the appropriate bases for $V$ and $W$.
        \begin{equation*}
            \begin{tikzcd}
            V \arrow[r,"T"] \arrow[d,"\simeq"]& W \arrow[d,"\simeq"] \\
            \F^{n,1} \arrow[r,"\M(T)"] \arrow[u,"\M"] & \F^{m,1} \arrow[u,"\M"]
            \end{tikzcd}
        \end{equation*}
        For a vector $v \in V$, its image $\M(v)$ under the isomorphism $\M$ of $V$ belongs to $\F^{n,1}$. Matrix multiplication with $\M(T)$ gives us $\M(Tv) \in \F^{m,1}$, which we identify with $Tv$ under the isomorphism $\M$ of $W$. This is why a linear map can be described entirely by its matrix representation.
        \item If $V = \F^n$ and $W = \F^m$ and we are using the canonical bases, then the linear map is equivalent to matrix multiplication. Think about the Jacobian matrix representation of the total derivative in the Euclidean space. Left-multiplication by a matrix corresponds to the linear \textit{transformation} of a vector.
    \end{itemize}
    \item A \textit{linear operator} is a linear map from a VS to itself. The notation $\LV$ stands for $\mathcal{L}(V,V)$.
    \item Suppose $V$ is FDVS and $T \in \LV$, then (a) $T$ is invertible iff (b) $T$ is injective iff (c) $T$ is surjective. (This is not true for infinite-dimensional vector space.) This is true for functions $f:S \to S$ (where $S$ is finite) in general, but for linear maps the proof is likely to be rather simple:
        
    (a)$\implies$(b) we proved earlier.
        
    (b)$\implies$(c) by checking the rank-nullity theorem. Injection means $\operatorname{nullity} T = 0$, which implies $\operatorname{rank} T = \d V$.
        
    (c)$\implies$(a) by showing $T$ is injective, as $\operatorname{nullity} T = \d V - \operatorname{rank} T$, which is 0.
\end{itemize}

\subsection{Products and Quotients of Vector Spaces}
\begin{itemize}
    \item The \textit{product} $V_1 \times \dots \times V_n$ of vector spaces $V_1,\dots,V_n$ over $\F$ is the set $$\{(v_1,\dots,v_n) \mid v_1 \in V_1,\dots, v_n \in V_n\}.$$ The addition and scalar multiplication on the product is defined in the usual entry-wise way. It is routine to prove that this product is a vector space over $\F$ as well.
    \item $\d (V_1\times \dots \times V_n) = \d V_1 + \dots + \d V_n$. Why? Choose a basis for each $V_i$, then put every basis vector of $V_i$ into the $i$-th entry of $(v_1,\dots,v_n)$ and let other entries be 0. These vectors are linearly independent and span the product and thus is a basis of the product.
    \item For subspaces $U_1,\dots,U_n$ of $V$, define the linear map $\Gamma: U_1 \times \dots \times U_n \to U_1 + \dots + U_n$ by $$\Gamma(u_1,\dots,u_n) = u_1+\dots+u_n.$$ (One can easily check that this map is indeed linear.) Then $U_1+\dots+U_n$ is a direct sum iff $\Gamma$ is injective (or invertible, since $\Gamma$ is surjective by its definition.) To prove this, we just need to show $\n T = {0}$ iff the only way to write $0 = u_1+\dots+u_n$ is to let all $u$'s be 0. This is quite clear.
    \item The last two theorems lead to the following one. A sum of subspaces $U_i$'s of $V$ is a direct sum iff dimensions add up. This is because two isomorphic vector spaces must have the same dimension. In both directions we would encounter $$\d (U_1+\dots+U_n) = \d (U_1 \times \dots \times U_n) = \d U_1 + \dots + \d U_n.$$
    Construct the linear map $\Gamma$ above from the product of subspaces makes the proof really clean. Recall that we have proved the special case $n=2$ already.
    \item An \textit{affine subset} of $V$ is a subset of $V$ of the form $v+U = \{v+u \mid u \in U\}$ for some vector $v \in V$ and some subspace $U$ of $V$. Here the affine subset $v+U$ is said to be \textit{parallel} to $U$.
    \begin{itemize}
        \item $v+U$ is a subspace of $V$ iff $v \in U$. If $v \in U$, then $v+U = U$, subspace of $V$. If $v \notin U$, then $0 \notin v+U$, and thus $v+U$ cannot be a subspace of $V$.
    \end{itemize}
    \item Under the same setting, the quotient space $V/U$ is the set of all affine subsets of $V$ parallel to $U$, i.e., $V/U = \{v+U \mid v \in V\}$.
    \item For subspace $U$ of $V$ and $v,w \in U$, then
        \begin{equation*}
            \text{(a) } v-w \in U \iff \text{(b) } v+U = w+U \iff \text{(c) } (v+U) \cap (w+U) \not= \emptyset.
        \end{equation*}
    (a)$\implies$(b) as $v+u = w+((v-w)+u) \in w+U$ (and the same for the other direction). (b)$\implies$(c) because $v+U$ is not empty, while (c)$\implies$(a) because $v+u_1=w+u_2$ for some $u_1$ and $u_2$, and we then move $v-w$ to the left side.
    \begin{itemize}
        \item The theorem above tells us that two affine subset to $U$ are either the same or  disjoint.
    \end{itemize}
    \item The addition and scalar multiplication on $V/U$ are defined by  $$(v+U)+(w+U) = (v+w)+U \quad \text{and} \quad \lambda(v+U) = \lambda v + U.$$ We should pay particular attention to this definition because \textbf{an affine subset do not have a unique representation} (for $v \not= v'$, it is possible that $v+U = v'+U$ still)! Therefore, one has to show that  different representations $v+U$ and $w+U$ of the same affine subsets of $U$ still gives us a \textbf{unique} $(v+w)+U$, so that the addition function above is well-defined (and similar for scalar multiplication). Suppose $v+U = v'+U$ and $w+U = w'+U$, then essentially we want to prove $(v+w)+U=(v'+w')+U$. By assumption $v-v'$ and $w-w' \in U$, $(v+w)-(v'+w') \in U$, and we reach the conclusion (and similar for scalar multiplication).
    \item $V/U$ with the two operations above is a vector space. Showing this is routine, but one has to remember that $V/U$ is not a subspace of $V$. Despite all the elements of the quotient space is in the whole space, the additive identity is now $0+U=U$, and the additive inverse of $v+U$ is $-v+U$. (The quotient space is discussed with respect to a fixed subspace.) Therefore, we have to check the 8 VS properties.
    \begin{itemize}
        \item Also, by the previous theorem, for any $v \in U$, $v+U$ equals the 0 vector $(0+)U$ and is thus not a basis vector for $V/U$.
    \end{itemize}
    \item To find the dimension of $V/U$, we appeal to the trick of constructing a linear map again. (One should check its linearity) The \textit{quotient map} $\pi: V \to V/U$ is given by $$\pi(v) = v+U.$$
    For FDVS $V$ and its subspace $U$, $\d V/U = \d V - \d U$. $\pi$ is a map onto $V/U$ obviously, and $\n \pi = U$. Therefore the formula follows.
    \item Now we define the induced map $\tilde{T}: V/(\n T) \to W$ of $T: V \to W$ by $$\tilde{T}(v+\n T) = Tv.$$ $u + \n T = v + \n T$ implies $u - v \in \n T$, and thus $Tu-Tv = 0$. Therefore the induced map is well defined. Checking the linearity of $\tilde{T}$ is routine. Note that $T = \tilde{T} \circ \pi$.
    \begin{equation*}
            \begin{tikzcd}
                V \arrow[rr,"T"] \arrow[dr,"\pi"] && W \\
                & {V/\n T} \arrow[ur,"\tilde{T}"]
            \end{tikzcd}
        \end{equation*}
    \item The definition leads to the isomorphism theorem that exists for different algebraic structures. For $T \in \LVW$, $\tilde{T}$ is injective and $\r \tilde{T} = \r T$. The latter simply follows from the definition, while the first requires us to prove $\n \tilde{T} = \{0+\n T\}$, the identity. $\tilde{T}(v+\n T) = Tv = 0$ gives us $v-0 \in \n T$. Hence, $v+\n T = 0+\n T$, and the conclusion follows.
        \begin{itemize}
            \item The two claims combined tell us $V/(\n T) \simeq \r T$ under the isomorphism $\tilde{T}$.
        \end{itemize}
    \end{itemize}
    
\subsection{Duality}
\begin{itemize}
    \item A \textit{linear functional} on $V$ is a linear map belonging to $\mathcal{L}(V,\F)$. The \textit{dual space} of $V$, denoted usually by $V^*$ or $V'$, is the vector space $\mathcal{L}(V,\F)$.
    \item We proved earlier that $\d \LVW = (\d V)(\d W)$. Therefore, $\d V' = \d V$ for finite-dimensional $V$.
    \item The \textit{dual basis} of $v_1,\dots,v_n$, basis of $V$, is a set of vectors $\phi_1,\dots,\phi_n$ in $V'$, where each $\phi_i$ is a linear functional defined by
    \begin{equation*}
        \phi_i(v_j) = \delta_{ij} = \left\{
            \begin{array}{rl}
                1 \quad & \text{if } i = j, \\
                0 \quad & \text{if } i \not= j.
            \end{array}
        \right.
    \end{equation*}
    These $\phi_i$'s are designed to form a basis of $V'$. It suffices to show that $\phi_1,\dots,\phi_n$ is linearly independent in $V'$. Consider $$a_1\phi_1+\dots+a_n\phi_n=0.$$ Since $(a_1\phi_1+\dots+a_n\phi_n)(v_j)=a_j$, all $a_j$'s are 0.
    \begin{itemize}
        \item Remember that the dual basis vector $\phi_i$ of $e_1,\dots,e_n$ in $\F^n$ has $$\phi_i(x_1,\dots,x_n)=\phi_i(x_1e_1+\dots+x_ne_n)=x_i.$$ Namely, the $i$-th dual basis vector maps $(x_1,\dots,x_n)$ to its $i$-th coordinate.
    \end{itemize}
    \item For $v_1,\dots,v_n$ basis of $V$ and its associated dual basis $\phi_1,\dots,\phi_n$ of $V'$, 
    \begin{itemize}
        \item first, every $\psi \in V'$ can be written as $\sum_{i=1}^n \psi(v_i)\phi_i$. Since $(\sum_{i=1}^n \psi(v_i)\phi_i)(v_j) = \psi(v_j)$, the sum and $\psi$ agrees on all basis vectors and they are equal.
        \item second, every $v \in V$ can be written as $\sum_{i=1}^n \phi_i(v)v_i$. For this claim one needs to prove that for $v = a_1v_1 + \dots +a_nv_n$, every $a_j = \phi_j(v)$. By linearity, $\phi_j(v) = \sum_{i=1}^n a_i \phi_j(v_i) = a_j$.
    \end{itemize}
    \item The \textit{dual map} $T' \in \mathcal{L}(W',V')$ of $T \in \LVW$ is given by $T'(\phi)=\phi \circ T$ for $\phi \in W'$. We have to be very clear about what this definition means. Here $T'$ maps a function to a function.
    \begin{equation*}
        \begin{tikzcd}
            W \arrow[dd, "\phi"'] &  & V 
            \arrow[dd, "\substack{\phi \circ T \\ \text{through} \\ W}"] \\
            {} \arrow[rr, "T'", Rightarrow] &  & {} \\
            \F &  & \F
        \end{tikzcd}
    \end{equation*}
    $T'$ is a composition of linear maps and is thus a linear map from $V'$ to $\F$. To show that $T'$ is linear from $W'$ to $V'$, one simply follows the definition.
    \begin{itemize}
        \item The following algebraic properties hold for dual maps:
        \begin{itemize}
            \item $(S+T)'=S'+T'$,
            \item $(\lambda T)'=\lambda T'$,
            \item $(ST)'=T'S'$
        \end{itemize}
        under the appropriate $S,T,$ and $\lambda$. The first two are routine, and the third one is manipulation of function compositions.
    \end{itemize}
    \item We define \textit{annihilator} $U^0$ of $U \subseteq V$ as $\{\phi \in V' \mid \phi(U)=0\}$, the subset of all linear functionals on $V$ whose image is 0.
    \item There is a good example (3.104) in the book about annihilators. For the dual basis $\phi_1,\dots,\phi_5$ in $(\F^5)'$ that corresponds to $e_1,\dots,e_5$ in $\F^5$, $(\s(e_1,e_2))^0 = \s(\phi_3,\phi_4,\phi_5)$, since $\phi_i$ takes out the $i$-th coordinate.
    \item As expected $U^0$ is a subspace of $V'$. The zero linear functional takes $U \subseteq V$ to $0$. $(\phi+\psi)(U) = 0$ and $\lambda\phi(U) = 0$ are omitted as usual.
    \item For FDVS $V$ and its subspace $U$, $$\d U + \d U^0 = \d V.$$ The book mentions two proofs. The first proof takes the usual ``choose a basis and extend it" approach. The second proof is more obscure: consider the inclusion map $i \in \mathcal{L}(U,V)$ such that $i(u) = u$ for all $u \in U$, then $i' \in \mathcal{L}(V',U')$, which gives us $$\d \n i' + \d \r i' = \d V' = \d V.$$ Note that $i'(\phi) = \phi \circ i$ for $\phi \in V'$, giving us $\n i' = U^0$. The nullspace of $i'$ are the set of $\phi$'s such that $\phi \circ i = 0$, and therefore $\phi$ should map the entire $U$ to $0$.
    
    Now we show $U = \r i'$. A linear map defined on a subspace can always be extended to the whole space, and thus we may extend any $\phi \in U'$ to $\psi \in V'$. The dual map $i'$ maps $\psi$ back to $\phi$, so $\phi \in U' \implies \phi \in \r i'$, showing us what we desire.
    \item The nullspace and range of the dual map $T'$ can now be characterized in terms of the concepts we have introduced.
    \begin{itemize}
        \item For $T \in \LVW$, $\n T' = (\r T')^0$: $$\phi \in \n T' \Longleftrightarrow (\phi \circ T)(v) = \phi(Tv) = 0 \Longleftrightarrow \phi \in (\r T)^0.$$
        \item If $V,W$ are finite-dimensional, then we have
        \begin{align*}
            \d \n T' & = \d (\r T)^0 \\
            & = \d W - \d \r T \\
            & = \d W - (\d V - \d \n T) \\
            & = \d \n T + \d W - \d V.
        \end{align*}
        \item $\d \r T' = \d \r T$ easily follows from the rank-nullity theorem and the two bullet points above.
        \item Also we can prove $\r T' = (\n T)^0$. Showing $\r T' \subseteq (\n T)^0$ is standard (just assume $T'$ maps $\phi$ to $\psi \in \r T'$), while the other  direction is quite tricky. The book assumes finite-dimension here and showed the two spaces have the same dimension: $$\d \r T' = \d \r T = \d V - \d \n T = \d (\n T)^0.$$
        We have to make a side note here. When we proved the first bullet point, we did not assume finite-dimension of $V$ and $W$, so it is natural to consider if our proof of $\r T' \supseteq (\n T)^0$ also does not require the finite-dimension assumption either.
        
        We may formulate our question this way: for $T \in \LVW$ and $\phi \in V'$ with $\phi(\n T) = 0$, can we always find a $\psi \in W'$ such that $\phi = \psi \circ T$? This turns out to be true if we assume the axiom of choice from set theory, which has important corollaries in the theory of infinite-dimensional vector spaces. We will not get into details here, but it is interesting to know about this.
    \end{itemize}
    \item The corollary of the previous claims are the following:
    $$T \text{ is surjective} \iff T' \text{ is injective} \quad \text{and} \quad T \text{ is injective} \iff T' \text{ is surjective}.$$ Proof of this is quite standard, and we show the first one here only: $$T \in \LVW \text{ surjective} \iff \r T = W \iff (\r T)^0 = {0} \iff \n T' = {0} \iff T' \text{ injective}.$$
    
    \item The \textit{row rank} and \textit{column rank} of matrix $A$ are defined to be the dimension of the span of the rows of $A$ in $\F^{1,n}$ and of the columns of $A$ in $\F^{m,1}$, respectively. As we know from lower-division matrix algebra, they are the same and represent the number of pivots in a reduced row echelon form of a matrix. We now prove this fact with the tools we have.
    
    \item $\M(T') = (\M(T))^t$. Consider the dual basis $\psi_j$ ($1 \leq j \leq m$) of $W'$ and $\phi_j$ ($1 \leq k \leq n$) of $V'$. Then $$T'(\psi_j) = \sum_{r=1}^n C_{r,j}\phi_r,$$ which we evaluate at $v_k$: $$(\psi_j \circ T)(v_k) = \sum_{r=1}^n C_{r,j}\phi_r(v_k) = C_{k,j}.$$ On the other hand, we have $$(\psi_j)(Tv_k) = \psi_j(\sum_{r=1}^mA_{r,k}w_r) = \sum_{r=1}^m A_{r,k}\psi_j(w_r) = A_{j,k}.$$
    
    \item First, $\d \r T = \text{column rank of } \M(T)$. From bases $\bv$ and $w_1,\dots,w_m$ of $V$ and $W$, we have $\s(\{Tv_1,\dots,Tv_n\})$ is isomorphic to $\s(\{\M(Tv_1),\dots,\M(Tv_n)\})$ under the isomorphism $\M$. Thus, the two VS have the same dimensions, with $\d \s(\{\M(Tv_1),\dots,\M(Tv_n)\}) = $ column rank of $\M(T)$. Recall that $\s(\{Tv_1,\dots,Tv_n\}) = \r T$, thus $\d \r T = $ column rank of $\M(T)$, as desired.
    
    \item For $A \in \F^{m,n}$, define $T \in \LVW$ as the left-multiplication by $A$ map (i.e., $Tx = Ax$). Then $A = \M(T)$ with respect to the canonical bases. It follows that
    \begin{align*}
        \text{column rank of } A & = \text{column rank of } \M(T) \\
        & = \d \r T = \d \r T' \text{ (which we proved a moment ago)} \\
        & = \text{column rank of } \M(T') = \text{column rank of } A^t
        = \text{row rank of } A.
    \end{align*}
    Therefore, \textit{rank} is a property of a finite matrix.
\end{itemize}

\section{Polynomials}
Remark. We used facts about polynomials in high school algebra. These facts were intuitively ``understood" by us back then, but we never had the chance to rigorously prove them. Although the theory of polynomials is definitely something one should know at this stage, we will skim through this chapter in our notes. Please refer to Chapter 4 of our textbook or Appendix E of \textit{Linear Algebra} by Friedberg, Insel, and Spence for the detailed development of major results.

\begin{itemize}
    \item The division algorithm for polynomials is similar to the one in number theory. For $p,s \in \mathcal{P}(\F)$ with $s \not= 0$, then there exist a pair of unique polynomials $q,r \in \mathcal{P}(\F)$ such that $p=sq+r$ and $\deg r < \deg s$.
    \begin{itemize}
        \item With the theory of linear maps this becomes easy to prove.
    \end{itemize}
    \item Each zero of a polynomial corresponds to a degree-1 factor, i.e., for $p \in \PF$ and $\lambda \in \F$, $p(\lambda) = 0$ iff $\exists q \in \PF$ such that for all $z \in \F$, $p(z) = (z-\lambda)q(z)$. To prove the $\implies$ direction, division algorithm gives us $p(z) = (z-\lambda)q(z) + r$ (here $r$ is a constant) as $(z-\lambda)$ is of degree 1. Thus, $p(\lambda) = 0$ gives us $r=0$.
    
    \item A polynomial $p \in \PF$ has at most as many zeros as its degree ($\geq 0$). $\deg = 0$ and 1 can be checked, and we proceed with induction on higher orders. If $p$ has no zeros, then done. If $p$ has one zero, let $$p(z) = (z-\lambda)q(z),$$ where $p$ is of degree $m+1$ and $q$ is of degree $m$ (with at most $m$ zeros).
    
    \item Recall the remark on polynomials we made in Chapter 2. Now we prove that polynomial functions over an infinite field $\F$ have a unique representation. Suppose $f,g \in \PF$ and $f(z)=g(z)$ for all $z \in \F$, we want to show that $f=g$.
    
    Define $h = f-g$, and then we have $\forall z\in\F, h(z) = 0$. Suppose $h$ is a nonzero polynomial of degree $n$, then by the division algorithm $h$ has at most $n$ zeros, yet $\F$ is an infinite field, contradiction! Thus $h = f-g$ is the zero polynomial, and $f = g$ as a result.
    
    \begin{itemize}
        \item In the case of $\F = \C$ or $\R$, the book gives another proof of the contraposition that if one coefficient out of $$h(z) = a_0+a_1z+\dots+a_mz^m$$ is not zero, then $h(z) \not= 0$ for some $z \in \F$. Assume $m \not= 0$ and pick $$z = \frac{|a_0| + \dots + |a_{m-1}|}{|a_m|}+1 \geq 1.$$ Thus, $z^j \leq z^{m-1}$ for all $j \leq m-1$. By triangle inequality and $|a_m|z > |a_0|+ \dots + |a_{m-1}|$, $$|a_0+a_1z+\dots+a_{m-1}z^{m-1}| \leq (|a_0|+\dots+|a_{m-1}|)z^{m-1}< |a_m z^m|.$$ Therefore $a_0+a_1z+\dots+a_{m-1}z^{m-1} \not= -a_m z^m$.
    \end{itemize}
    
    \item The proof of the fundamental theorem of algebra is omitted here for the obvious reason. Every nonconstant polynomial on $\C$ has a zero, which directly implies that a polynomial $p$ of degree $m$ has $m$ nondistinct roots, which we sketch through now.
    \begin{itemize}
        \item Every nonconstant polynomial over $\C$ has a unique factorization (up to the order of factors) of the form $$p(z) = c(z - \lambda_1)\cdots(z-\lambda_m),$$ where $c, \lambda_1,\cdots,\lambda_m \in \C$.
    \end{itemize}
    \begin{itemize}
        \item To prove this, by the fundamental theorem of algebra we can factorize $p(z)$ into $(z-\lambda)q(z)$, and again factorize $q(z)$, and so on. We have proved the existence of such factorization.
        \item Now we proceed to prove the uniqueness. $c$ is clearly unique as the highest coefficient of $p(z)$. Consider $(z - \lambda_1)\cdots(z-\lambda_m)=(z - \tau_1)\cdots(z-\tau_m)$. We want to show that the $\lambda$'s and $\tau$'s are the same up to their ordering. Let $z = \lambda_1$, one of the $\lambda_1 - \tau_i$'s on the RHS must be 0. WLOG we assume $\lambda_1 = \tau_1$, and $(z - \lambda_2)\cdots(z-\lambda_m)=(z - \tau_2)\cdots(z-\tau_m)$ holds for all $z \in \F$ except for $\lambda_1$. Actually $$(z - \lambda_2)\cdots(z-\lambda_m) - (z - \tau_2)\cdots(z-\tau_m) = 0$$ must hold for all $z \in \F$ because we would have a nonzero polynomial with infinite many zeros. Thus, we can proceed to show $\lambda_2 = \tau_2$, and so on.
    \end{itemize}
    \item For polynomials $p \in \mathcal{P}(\C)$ with real coefficients, the complex conjugate of a root is still a root. This follows from $\lambda^m = \conj{\lambda}^m$.
    
    \item For nonconstant $p \in \mathcal{P}(\R)$, $p$ has a unique factorization into linear and quadratic polynomials with the highest coefficients being 1, i.e., $$p(x)=c(x-\lambda_1)\cdots(x-\lambda_m)(x^2+b_1x+c)\cdots(x^2+b_Mx+c_M),$$ where $c,$ the $\lambda$'s, the $b$'s, and the $c$'s are all reals and each quadratic factors have negative discriminant.
    \begin{itemize}
        \item 
    \end{itemize}
\end{itemize}


\section{Eigenvalues, Eigenvectors, and Invariant Subspaces}
\subsection{Invariant Subspaces}
\begin{itemize}
    \item For $T \in \LV$, the subspace $U$ of $V$ is \textit{$T$-invariant} if $T(U) \subseteq U$, or alternatively, $T|_U$ is an operator on $U$.
    \item It can be easily verified that $\{0\}$, $V$, $\n V$, and $\r T$ are invariant subspaces of $V$ under $T$.
    \item We now focus on invariant subspaces of dimension 1. Any 1-dimensional subspace $U$ has some vector $u \in V$ as the basis (actually any vector in $U$ can be the basis). To check if $T$ is invariant under $U$, we only need to check if the basis vectors are mapped into $U$ (so that the $\r T \subseteq U$). Since $Tu \in U$ iff $Tu = \lambda u$ for some $u \in U$, a subspace with dimension 1 is invariant iff a vector $u$ in the subspace has $Tu = \lambda u$.
    \item For $T \in \LV$, $\lambda \in \F$ is an \textit{eigenvalue} of $T$ if there exists a nonzero $v \in V$ such that $Tv = \lambda v$. The $v$ here is called the \textit{eigenvector}.
    \item To move $\lambda v$ to the left side, we consider the linear operator $\lambda I$ that maps $v$ to $\lambda v$. Therefore, we have $(T-\lambda I) v = 0$. Since $v \not= 0$, $T-\lambda I$ is not injective/surjective/invertible (since it is an operator on $V$).
    \item Eigenvectors corresponding to distinct eigenvalues are linearly independent. The usual proof of this starts with 2 distinct eigenvalues and proceeds with induction, but the book uses the well-ordering principle instead. Suppose the $\bv$ corresponding to eigenvectors $\lambda_1,\dots,\lambda_n$ are linearly dependent, then at $k \leq n$ we have $v_k \in \s(v_1,\dots,v_{k-1})$ with $v_1,\dots,v_{k-1}$ being linearly dependent. Thus, 
    \begin{equation}
        v_k = a_1 v_1 + \dots + a_{k-1}v_{k-1}, \tag{$\star$}
    \end{equation}
    and by applying $T$ to both sides, we have $$\lambda_k v_k = a_1 \lambda_1 v_1 + \dots + a_{k-1} \lambda_{k-1}v_{k-1}.$$ If instead, we multiply ($\star$) by $\lambda_k$ on both sides, then $$\lambda_k v_k = a_1 \lambda_k v_1 + \dots + a_{k-1} \lambda_k v_{k-1}.$$ Subtracting the two equations above give us $$0 = a_1 (\lambda_1 - \lambda_k) + \dots + a_{k-1} (\lambda_{k-1}-\lambda_k) v_{k-1}.$$ Since $v_1, \dots, v_{k-1}$ are linearly independent, all the coefficients are 0 and thus all the $a$'s are the same (because the $\lambda$'s are distinct). However, this implies $v_k = 0$, which contradicts with $v_k$ being an eigenvector. Therefore, the set of eigenvectors are linearly independent.
    \item The direct corollary of this is that the number of eigenvalues of $T \in \LV$ cannot exceed the $\d V$.
    \item Here we restate the $T|_U \in \mathcal{L}(U)$ as the \textit{restriction operator} of $T$. The \textit{quotient operator} $T/U \in \mathcal{L}(V/U)$ is given by $(T/U)(v+U) = Tv+U$ for any $v \in V$. Of course for the quotient operator we need to verify it is well-defined for different representation of the same quotient space, which is very similar to what we have done before and is thus omitted. These two operators provide nice information about the original $T$.
\end{itemize}

\subsection{Eigenvectors and Upper-Triangular
Matrices}
\begin{itemize}
    \item Linear operators allow idempotent operations. Given $T \in \LV$, we define $T^m$ ($m \in \Z^+$) as $T$ composite with itself $m$ times, $T^0$ as the identity operator $I_V$, and $T^{-m}$ as $(T^{-1})^m$, given $T^{-1}$ exists.
    \begin{itemize}
        \item Obviously $T^mT^n = T^{m+n}$ and $(T^m)^n = T^{mn}$. (If $T$ is invertible then we can $m,n$ are allowed to be all integers.)
    \end{itemize}
    \item $p(T)$ is the polynomial $p$ taking $T$ as its indeterminate (although $T$ is not $\F$). For $p(z) = a_0 + a_1 z + \dots + a_n z^n$ over $\F$, we define $p(T) = a_0 I + a_1 T + \dots a_n T^n$.
    \begin{itemize}
        \item If we fix an operator $T$, the evaluation map $\PF$ to $\LV$ given by $p \mapsto p(T)$ is linear.
    \end{itemize}
    \item The product of $pq \in \PF$ of polynomials $p,q \in \PF$ is defined by $$(pq)(z) = p(z)q(z)$$ for all $z \in \F$. If we write out the polynomials, one can easily see that $(pq)(z) = p(z)q(z) = q(z)p(z) = (qp)(z)$. 
    \begin{itemize}
        \item It follows directly that $(pq)(T) = p(T)q(T)$ and $(pq)(T) = (qp)(T)$, because we only replace the symbol $z$ by $T$.
    \end{itemize}
    \item It turns out that every operator on a finite-dimensional and nonzero complex vector space necessarily has an eigenvalue. (This is not necessarily true on real vector spaces) The proof employs the fundamental theorem of algebra and is quite tricky. Choose a nonzero $v \in V$ ($\d V=n$) and consider the list $v, Tv, \dots, T^n v$, which must be linearly dependent. Thus, we have $$0 = a_0 v + a_1 Tv + \dots + a_n T^n v,$$ where the $a_i$'s are not all $0$. In particular, $a_1, \dots, a_n$ cannot all be 0 since $v \not= 0$ will then give us $a_0 = 0$. Thus, we have a nonconstant polynomial $a_0+a_1 z+\dots+a_m z^m$ ($m \leq n$), which can be factorized into $$c(z - \lambda_1)\cdots(z - \lambda_m)$$ over $\C$. For $$0=c(T - \lambda_1 I)\cdots(T - \lambda_m I)v,$$ the nonzero vector $w = (T - \lambda_{j+1} I) \cdots (T - \lambda_m I)v$ will be be taken to 0 by $(T - \lambda_j I)$. $(T - \lambda_j I)$ is thus not injective and $\lambda_j$ is our eigenvalue.
    
    \item The matrix of an operator is a square matrix. A square matrix with all entries being 0 below the diagonal is called an \textit{upper-triangular matrix}, which is crucial in the theory of eigenvalues.
    \item How are eigenvalues and vectors, invariant subspaces, and upper-triangular matrices connected? Suppose $T \in \LV$ and $v_1,\dots,v_n$ is a basis of $V$, then
    \begin{enumerate}[label=(\alph*)]
        \item $\M(T)$ is upper triangular.
        \item $Tv_j \in \s(v_1,\dots,v_j)$ for each $j$ from 1 to $n$.
        \item $\s(v_1,\dots,v_j)$ is invariant under $T$ for each $j$ from 1 to $n$.
    \end{enumerate}
    The first two are equivalent follow straight from the definition. (b)$\implies$(c) because for every $i$ between 1 and $j$, $Tv_i \in \s(v_1,\dots,v_j)$. (c)$\implies$(b) is obvious as well.
    \item For FDVS $V$ over $\C$ and $T \in \LV$, then $\M(T)$ is upper triangular with respect to some basis of $V$.
    
    \item Upper-triangular matrices help us determine whether $T$ is invertible. If $T \in \LV$ has an upper-triangular $\M(T)$ with respect to some basis of $V$, then $T$ is invertible iff all entries on the diagonal of $\M(T)$ are nonzero.
    
    \item By the previous theorem, we can determine all the eigenvalues of $T$ if we are fortunate to find a basis that corresponds to an upper-triangular $\M(T)$. $\M(T - \lambda I)$ is of the form
    \begin{equation*}
        \begin{bmatrix}
        \lambda_1 - \lambda & & & \ast \\
         & \lambda_2 - \lambda & & \\
         & & \ddots & \\
         0 & & & \lambda_n - \lambda
    \end{bmatrix}.
    \end{equation*}
    One of the $\lambda_i - \lambda$'s is 0 iff $T - \lambda I$ is invertible iff $\lambda$ is an eigenvalue. Therefore, all the entries $\lambda_i$'s on the diagonal are exactly all the eigenvalues.
\end{itemize}
\subsection{Eigenspaces and Diagonal Matrices}
\begin{itemize}
    \item A \textit{diagonal matrix} has all its entries off the diagonal 0.
    \item The \textit{eigenspace} of $T \in \LV$ corresponding to $\lambda \in \F$ is given by $$E(\lambda,T) = \n (T-\lambda I) = \{v \in V \mid Tv = \lambda v\}.$$
    The eigenspace is basically the vector subspace of $V$ with all eigenvectors corresponding to $v$, along with the 0 vector.
    \item The sum of eigenspaces is a direct sum. We consider $u_1+\cdots+u_m = 0$, where each eigenvector $u_i$ is from the eigenspace $E(\lambda_i,T)$. Because eigenvectors corresponding to distinct eigenvalues are linearly independent, we get what we want. Furthermore, since $$E(\lambda_1,T)+\dots+E(\lambda_m,T)$$ is a direct sum, we have $$\d E(\lambda_1,T) + \cdots + \d E(\lambda_m,T) = \d (E(\lambda_1,T) \oplus \dots \oplus E(\lambda_m,T)) \leq \d V.$$ Recall the theorem in \S3.E that says a sum is a direct sum iff the dimensions add up.
    \item An operator $T$ is \textit{diagonalizable} if the the operator has a diagonal matrix with respect to some basis of $T$.
    \item For $T$ on FDVS $V$ with dimension $n$ and distinct eigenvalues $\lambda_1, \dots, \lambda_m$, diagonalizability of $T$ is equivalent to the following four conditions:
    \begin{enumerate}[label=(\alph*)]
        \item $V$ has a basis consisting of eigenvectors of $T$;
        \item $V$ can be decomposed into $n$ 1-dimension subspaces $U_1$ to $U_n$, where $V = U_1 \oplus \cdots \oplus U_n$;
        \item $V = E(\lambda_1,T) \oplus \cdots \oplus E(\lambda_m,T)$;
        \item $\d V = \d E(\lambda_1,T) + \cdots + \d E(\lambda_m,T) = \d (E(\lambda_1,T) \oplus \dots \oplus E(\lambda_m,T))$
    \end{enumerate}
    The proof to this theorem is a little involved but not hard at all. One should get an intuitive idea of what each equivalent condition means.
    \begin{itemize}
    \item We consider the whole list of eigenvalues $\lambda_1, \dots, \lambda_n$ with repetition ($n \geq m$). Looking at the matrix
    \begin{equation*}
        \begin{bmatrix}
        \lambda_1 & & 0 \\
         & \ddots & \\
         0 & & \lambda_n
    \end{bmatrix}
    \end{equation*}
    for a moment would tell us diagonalizability and (a) are equivalent.
    \item (a)$\implies$(b): Let each $U_j = \s(v_j)$, each of which are invariant under $T$. Since $v_1,\dots,v_n$ is a basis of $V$, any $v \in V$ can be written uniquely as a linear combination of $v_1,\dots,v_n$, and thus a unique sum of elements from each $U_j$. Therefore, $V$ is a direct sum of the $U_j$'s. Trace this proof backward to show (b)$\implies$(a).
    \item We now show (a)$\iff$(c)$\iff$(d). (c)$\iff$(d) follows from the theorem in \S3.5.
    \begin{itemize}
        \item (a)$\implies$(c) because $V$ has a basis of $n$ eigenvectors, each of which must belong to any of the $m$ distinct eigenspaces $E(\lambda_j,T)$. Therefore, $V$ is the sum of all these eigenspaces (and thus the direct sum of them).
        \item To show (c) \& (d)$\implies$(a), choose a basis $B_j$ from each $E(\lambda_j,T)$. Therefore, we get a list of vectors $v_1, \dots, v_n$ of length $n = \d V$ by (d). To show the list is linearly independent, consider $0 = u_1+\dots+u_n$, where each $u_j$ is from $E(\lambda_j,T)$. All the $u_j$'s must be 0 because $V$ is a direct sum of the eigenspaces. If $0 = u_j$, since $u_j$ can be expressed as a linear combination of the basis vectors in $B_j$, the coefficients in the linear combination must be 0. Therefore our constructed list of eigenvectors $v_1, \dots, v_n$ is indeed a basis of $V$.
    \end{itemize}
    \end{itemize}
    \item If $T \in \LV$ has $\d V$ number of distinct eigenvalues, then $T$ is necessarily diagonalizable. Having $\d V$ number of distinct eigenvalues tells us we have a basis of eigenvectors of $T$. With respect to this basis $\M(T)$ is a diagonal matrix.
\end{itemize}

\section{Inner Product Spaces}
\subsection{Inner Products and Norms}
\begin{itemize}
    \item For $x=(x_1,\dots,x_n) \text{ and } y=(y_1,\dots,y_n) \in \R^n$ (the real $n$-dimensional space), the \textit{dot product} $x \cdot y \coloneqq x_1y_1+\dots+x_ny_n$ is a scalar in $\R$. The norm $\|x\|$ on $\R^n$ is usually given by $\sqrt{x_1^2+\dots+x_n^2}$, which is the square root of the dot product of $x$ with itself.
    \item The usual norm $\|z\| \coloneqq  \sqrt{|z_1|^2+\dots+|z_n|^2}$ on $\C^n$ (the real $n$-dimensional space) is also a scalar in $\R$.
    \item Generally on both real and complex $n$-dimensional spaces, we define the \textit{Euclidean inner product} by $$\inp{(w_1,\dots,w_n)}{(z_1,\dots,z_n)} =  w_1\conj{z_1}+\dots+w_n\conj{z_n}.$$ This generalizes the definition of the dot product on $\R^n$. Moreover, we have $$\inp{(z_1,\dots,z_n)}{(z_1,\dots,z_n)} = z_1\conj{z_1}+\dots+z_n\conj{z_n} = |z_1|^2 + \dots + |z_n|^2 = \|z\|^2,$$ so that $\|z\|$ becomes the square root of the inner product of $z$ with itself.
    
    We regard $\R^n$ and $\C^n$ by default as the $n$-dimensional \textit{real and complex Euclidean spaces} endowed with the Euclidean inner product. 
    
    \item Observation on our definition of the Euclidean inner product suggests how we could possibly define a general abstract inner product on arbitrary real or complex vector spaces. The \textit{inner product} is a function $\inp{\cdot}{\cdot}: V \times V \to \F$ ($\R$ or $\C$) with the following properties:
    \begin{itemize}
        \item Positive definiteness: $\inp{v}{v} \geq 0$ (\textbf{real} and nonnegative), which achieves ``$=$" iff $v = 0$;
        \item Linearity in the first slot: $\inp{\lambda u+v}{w} = \lambda \inp{u}{w} + \inp{v}{w}$; (This means $\phi_w(v) \coloneqq \inp{v}{w}$ with respect to a fixed $w$ gives us a linear functional on $V$.)
        \item Conjugate symmetry: $\inp{u}{v} = \conj{\inp{v}{u}}$.
    \end{itemize}
    The properties that directly ensue include:
    \begin{itemize}
        \item Conjugate symmetry gives us ``conjugate linearity" (anti-linearity) in the second slot: $$\inp{u}{\lambda v+w} = \conj{\lambda} \inp{u}{v} + \inp{u}{w};$$
        \item For fixed $w \in V$, $f: v \mapsto \inp{v}{w}$ is a linear map from $V$ to $\F$ because of linearity in the first slot;
        \item $\inp{0}{v}$ and $\inp{v}{0} = 0$ because linear maps take 0 to 0.
    \end{itemize}
    \item The \textit{norm} $\|v\| \coloneqq \sqrt{\inp{v}{v}}.$ It has properties
    \begin{itemize}
        \item $\|v\| = 0$ iff $v = 0$, and 
        \item $\|\lambda v\| = |\lambda| \|v\|$.
    \end{itemize}
    \item Two vectors are \textit{orthogonal} if their inner product is 0. Therefore, 0 is orthogonal to any vector and 0 is the only vector that is orthogonal to itself.
    \item Introducing these concepts provide us with a list of useful equations and inequalities about \textit{inner product spaces} (vector spaces endowed with an inner product) that we know as facts in the Euclidean spaces.
    \begin{itemize}
        \item The Pythagorean theorem now holds for orthogonal vectors $u,v \in V$: $$\|u+v\|^2 = \|u\|^2 + \|v\|^2.$$ Expanding the LHS, we have $\|u+v\|^2 = \inp{u+v}{u+v} = \inp{u}{u} + \inp{u}{v} + \inp{v}{u} + \inp{v}{v}$, where the two terms in the middle are 0.
        The converse of this theorem holds in real inner product spaces because then we would have $\inp{u}{v}+\inp{v}{u} = 2\inp{u}{v} = 0$, showing that the two vectors are orthogonal.
        \item The method of orthogonal decomposition is extended to inner product spaces. Suppose $u,v \in V$ with $v$ being nonzero, we wish write $u$ as the sum of scalar multiple of $v$ and another vector $w$ that is orthogonal to $v$.
        
        Basically we want to choose a scalar $c \in \F$ such that $$0 = \inp{u-cv}{v} = \inp{u}{v} - c\|v\|^2.$$ Let $c = \frac{\inp{u}{v}}{\|v\|^2}$ and we have $w = u - cv$ and we have our orthogonal decomposition $$\inp{w}{v} = 0 \quad \text{and} \quad u = cv+w.$$
        \item The well-known Cauchy-Schwarz inequality states that $|\inp{u}{v}| \leq \|u\|\|v\|$, which reaches equality iff one of $u,v$ is a scalar multiple of the other. This can be proved using the orthogonal decomposition theorem we have above, but we choose to use the more standard method. When $v = 0$, the result is immediate. By the equality condition given, for $v \not= 0$ we may consider $$0 \leq \|u - cv\|^2 = \inp{u}{u} - \conj{c}\inp{u}{v} - c\inp{v}{u} + c\conj{c}\inp{v}{v}.$$ Set $c = \nm{\inp{u}{v}/\inp{v}{v}}$, then the RHS of the ``$=$" becomes $ \nm{u}^2 - \frac{|\inp{u}{v}|^2}{\nm{v}^2}$, as desired. The equality is achieved if and only if $u = cv$ or $v = 0$.
        
        Two special cases should be kept in mind.
        \begin{itemize}
            \item On real numbers, we have $|x_1y_1+\dots+x_ny_n|^2 \leq (x_1+\dots+x_n)^2(y_1+\dots+y_n)^2$.
            \item One can define an inner product on function spaces such as the vector space $V$ of continuous functions from $[a,b]$ to $\F$. One can define the inner product on $V$ as such: $$\inp{f}{g} = \int_a^b f(x)\conj{g(x)} dx.$$ This is an important inner product that will be useful in later courses. If $\F = \R$, then we simply have $$\inp{f}{g} = \int_a^b f(x)g(x) dx.$$ By the Cauchy-Schwarz inequality, we then have (over $\R$) $$\left|\int_a^b f(x)g(x)dx\right|^2 \leq \left(\int_a^b (f(x))^2 dx \right) \left(\int_a^b (g(x))^2 dx \right).$$
        \end{itemize}
        \item The triangle inequality states that $\|u+v\| \leq \|u\|+\|v\|$, which reaches equality iff one of $u,v$ is a nonnegative real multiple of the other.
        To prove, expanding the LHS gives us
        \begin{align*}
            \inp{u}{u}+\inp{v}{v}+\inp{u}{v}+\conj{\inp{u}{v}} & = \nm{u}^2+\nm{v}^2+2 \Re{\inp{u}{v}} \\ & \leq \nm{u}^2+\nm{v}^2+2|\inp{u}{v}| \\ & \leq \nm{u}^2+\nm{v}^2+2\nm{u}\nm{v} = (\nm{u}+\nm{v})^2.
        \end{align*}
        Taking the square root gives us the result. To have equality in the third row of our derivation, we need $|\inp{u}{v}| = \nm{u}\nm{v}$, which implies the equality in the second row because the norm is a real number. Therefore, the triangle inequality achieves equality iff $|\inp{u}{v}| = \nm{u}\nm{v}$.
        
        Note that $|\inp{u}{v}| = \nm{u}\nm{v}$ implies $u=cv$ or $v=0$ by the Cauchy-Schwarz inequality. Also, it is easy to show $u=cv$ or $v=0$ implies $|\inp{u}{v}| = \nm{u}\nm{v}$. Thus, the triangle inequality achieves equality iff one of $u,v$ is a scalar multiple of the other.
        \item The parallelogram equality says that $\|u+v\|^2+\|u-v\|^2 = 2(\|u\|^2+ \|v\|^2)$. To prove, expand the LHS and after some cancellations, we get the RHS.
    \end{itemize}
\end{itemize}

\subsection{Orthonormal Bases}
\begin{itemize}
    \item A list (set) of vectors is \textit{orthonormal} if each vector is of norm 1 and is orthogonal to all other vectors in the list. That is to say for an orthonormal list of vectors $e_1,\dots,e_n \in V$,
    \begin{equation*}
    \inp{e_i}{e_j} = 
        \left\{
            \begin{array}{rl}
                1 \quad & \text{if } i = j, \\
                0 \quad & \text{if } i \not= j.
            \end{array}
        \right.
    \end{equation*}
    \begin{itemize}
        \item Note that the canonical basis in $\F^n$ is orthonormal.
    \end{itemize}
    \item Given an orthonormal list $e_1,\dots,e_n$, by repeated use of the Pythagorean theorem, we get $$\nm{a_1 e_1 + \cdots + a_n e_n}^2 = |a_1|^2 + \cdots + |a_n|^2$$ for all scalars $a_1, \dots, a_m$.
    \item It follows from the theorem above that if we set $a_1 e_1 + \cdots + a_n e_n = 0$, then all the $a_i$'s must be 0, showing that an orthonormal list of vectors $e_1,\dots,e_n$ are linearly independent. Therefore, an orthonormal list of length $\d V$ is an \textit{orthonormal basis}, i.e., an orthonormal list (set) that is also a basis.
    \item Now any $v$ can be written as a unique linear combination of the orthonormal basis vectors $e_1,\dots,e_n$. For the coefficient of each $e_j$, it is just $a_j = \inp{v}{e_j}$.
    
    Consider $v = a_1 e_1 + \dots + a_n e_n$ and take an inner product with $e_j$ on both sides. Then, $\inp{v}{a_j} = a_j$, as desired. Note that $\nm{v}^2 = |\inp{v}{e_1}|^2 + \dots + |\inp{v}{e_n}|^2$.
    \begin{itemize}
        \item There is an important corollary to the theorem above. For an orthonormal basis of $V$ $e_1,\dots,e_n$, every $$Te_k = \inp{Te_k}{e_1}e_1 + \dots + \inp{Te_k}{e_n}e_n.$$ Therefore, we have $\M(T)_{j,k} = \inp{Te_k}{e_j}$ for the matrix representation of $T$ with respect to $e_1,\dots,e_n$.
    \end{itemize}
    \item The Gram-Schmidt process can now be proved. For a linearly independent list $v_1,\dots,v_n$ in $V$, define $e_1 = \frac{v_1}{\nm{v_1}}$, and then for $2 \leq j \leq n$, define $e_j$ inductively by $$e_j = \frac{v_j - \inp{v_j}{e_1}e_1 - \dots - \inp{v_j}{e_{j-1}}e_{j-1}}{\nm{v_j - \inp{v_j}{e_1}e_1 - \dots - \inp{v_j}{e_{j-1}}e_{j-1}}}.$$ Then for any $j \leq m$, $e_1, \dots, e_j$ is still orthonormal and has the same span as $v_1,\dots,v_j$. 
    
    
    This algorithm for constructing an orthonormal basis from a given basis leads to two useful corollaries.
    \begin{itemize}
        \item Every FD inner product space has an orthonormal basis. Proof is trivial.
        \item If $V$ is FD, then every orthonormal list of vectors can be extended to an orthonormal basis. We start with $e_1,\dots,e_m$ and extend the list to a basis $e_1,\dots,e_m,v_{m+1},\dots,v_n$ of $V$. It is easily observed that if we apply the Gram-Schmidt process from $e_1$ to $v_n$, then $e_1$ to $e_m$ will remain themselves, yet $v_{m+1},\dots,v_n$ will be orthonormalized to $f_{m+1},\dots,f_n$. Thus the $e$'s and the $f$'s become the basis extended from the original list of $e$'s.
    \end{itemize}
    \item If $T \in \LV$ has an upper-triangular matrix with respect to some basis of $V$, then it has an upper-triangular matrix with respect to some orthonormal basis of $V$.
    
    (In the case of $\F = \C$, since the condition always holds, $T$ must have an upper-triangular matrix with respect to some orthonormal basis of $V$. This is known as Schur's theorem.)
    
    Recall having upper-triangular $\M(T)$ is equivalent to $\s(v_1,\dots,v_j)$ being invariant under for each $1 \leq j \leq n$. This is ensured by the Gram-Schmidt process.
    
    
    \item We briefly mentioned that an inner product with the second slot fixed is a linear functional. In fact the Riesz representation theorem states that for FDVS $v$ and any $\phi \in V'$, there exists a \textbf{unique} $u \in V$ such that $\phi_u(v) = \inp{v}{u}$ for every $v \in V$.
    
    To prove, choose an arbitrary orthonormal basis $e_1,\dots,e_n$. Then for any $v$,
    \begin{align*}
        \phi(v) & = \phi(\inp{v}{e_1}e_1+\cdots+\inp{v}{e_n}e_n) \\ & = \inp{v}{e_1}\phi(e_1)+\cdots+\inp{v}{e_n}\phi{e_n} \\ & = \inp{v}{\conj{\phi(e_1)}e_1+\cdots+\conj{\phi(e_n)}e_n}.
    \end{align*}
    Therefore $\conj{\phi(e_1)}e_1+\cdots+\conj{\phi(e_n)}e_n$ is our desired $u$. To show uniqueness, suppose $\phi(v) = \inp{v}{u_1} = \inp{v}{u_2}$, then $\inp{v}{u_1 - u_2} = 0$ for all $v$. And by setting $v = u_1 - u_2$ we have $u_1 = u_2$.
    \begin{itemize}
        \item By uniqueness, whatever what orthonormal basis $e_1,\dots,e_n$ we choose, the expression $\conj{\phi(e_1)}e_1+\cdots+\conj{\phi(e_n)}e_n$ remains the same.
    \end{itemize}
\end{itemize}
\subsection{Orthogonal Complement and Minimization Problems}


\section{Operators on Inner Product Spaces}
\subsection{Self-Adjoint and Normal Operators}

\subsection{The Spectral Theorem}

\subsection{Positive Operators and Isometries}

\subsection{Polar Decomposition and Singular Value Decomposition}


\section{Operators on Complex Vector Spaces}
\subsection{Generalized Eigenvectors and Nilpotent Operators}

\subsection{Decomposition of an Operator}

\subsection{Characteristic and Minimal Polynomials}

\subsection{Jordan Form}


\section{Operators on Real Vector Spaces}
\subsection{Complexification}

\subsection{Operators on Real Inner Product Spaces}


\section{Trace and Determinant}
\subsection{Trace}

\subsection{Determinant}


\end{document}