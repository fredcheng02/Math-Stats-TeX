\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsthm,amssymb,tikz-cd}
\usepackage{enumitem}
\usepackage{fullpage}
\usepackage{setspace}
\usepackage{mathtools}
\usepackage[bookmarks]{hyperref}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\s}{\text{span}}
\newcommand{\n}{\text{null }}
\newcommand{\LV}{\mathcal{L}(V)}
\newcommand{\LVW}{\mathcal{L}(V,W)}
\newcommand{\M}{\mathcal{M}}
\newcommand{\bv}{v_1,\dots,v_n}
\newcommand{\bw}{w_1,\dots,w_n}
\renewcommand{\r}{\text{range }}
\renewcommand{\d}{\text{dim }}
\renewcommand{\phi}{\varphi}

\onehalfspacing
\setlength{\parskip}{0em}

\begin{document}

\begin{center}
    {\Large MATH 110 based on \textit{Linear Algebra Done Right}} \vspace{0.5em}
    \\ Cheng, Feng
    \vspace{-0.5em}
\end{center}

\section{Vector Spaces}
\begin{itemize}
    \item A vector space $V$ is an abelian group under addition that has the distributive and associative properties in scalar multiplication and the multiplicative identity. (Here addition and scalar multiplication are closed operations in $V$.)
    \item $\F^S$ denotes the set of functions from $S$ to $\F$. It is a vector space over $\F$ under the sum and scalar multiplication of functions defined in the usual pointwise way.
    \item A vector space holds the cancellation law and has a unique additive identity and a unique additive inverse because it is a group. $0v = 0$, $a0 = 0$, and $(-1)v = -v$ hold by the distributive property.
    \item A subset $U$ of the whole vector space $V$ that is still a vector space is called a vector subspace. Usually we use the three criteria i) $0_V \in U$ (or $U$ is nonempty), ii) closed under addition in $U$, and iii) closed under scalar multiplication in $U$ to verify. To show the criteria are equivalent to the definition, in one direction we show that $0_V$ is the (unique) identity element of $U$ and in the other direction we only need to check the additive inverse property.
    \item The sum $U_1 + U_2 +\cdots+ U_m$, where $U_i$'s are subsets of $V$, is the set of all possible sums of the respective elements from each $U_i$.
    \item When the $U_i$'s are subspaces of $V$, then the sum is the smallest vector space containing all $U_i$'s.
    \item When the $U_i$'s are subspaces of $V$, and every element of the sum $U_1 + U_2 +\cdots+ U_m$ can be written in only one way as $u_1+u_2+\cdots+u_m$, we call the sum a \textit{direct sum}, denoted by $\oplus$ replacing $+$.
    \item The equivalent criterion for direct sum is that the only way to express $0$ is to write it as the sum of $0$'s from each $U_i$. One direction is trivial, while the other direction employs the common trick of subtracting two different expressions of the same vector $v \in V$.
    \item If we are only discussing two subspaces $U$ and $W$ of $V$, then $U+W$ is a direct sum iff their intersection is the $\{0\}$. The proof of this, in both directions, considers $0 = v + (-v)$. The fact $W$ always contain an element and its inverse will give us the answer.
    \item Arbitrary intersection of subspaces of $V$ is a subspace of $V$, and the union of two subspaces is still a subspace iff one is contained in the other.
\end{itemize}

\section{Finite-Dimensional Vector Spaces}

\textit{Remark}. Note that this section in the book builds on the notion of a list of vectors. FDVS is defined in terms of spanning list (set), and the concept of a basis and the dimension of a vector space is pushed backward. In our exposition, we rearrange the progression of these ideas and reconstruct some proofs.

\begin{itemize}
    \item The \textit{span} of a set $S$ of vectors in $V$ is defined to be all possible linear combinations of these vectors. In particular, span($\emptyset$) $\coloneqq \{0\}$.
    \item Similar to the fact that the sum $U_1 + U_2 +\cdots+ U_m$ is the smallest vector space containing all $U_i$'s, span($S$) is the smallest subspace of $V$ containing all the vectors in $S$. The proof of the two are almost exactly the same.
    \item A polynomial $p$ in an indeterminate $z$ with coefficients in $\F$ is the \textbf{expression} $a_n z^n + \cdots + a_1 z + a_0$ with all $a_i$'s in $\F$. The \textit{degree} of a polynomial is the largest exponent of $z$ with nonzero coefficient, and if all the coefficients are 0, we customarily define its degree to be $-\infty$ in the book, or $-1$ elsewhere.
    \begin{itemize}
        \item Since we are dealing with $z$ and coefficients in $\F = \R$ or $\C$ in this book, a polynomial here can be alternatively defined as the \textbf{function} $p: \F \rightarrow \F$ with $p(z) = a_n z^n + \cdots + a_1 z + a_0$. The reason behind is that a polynomial function with coefficients from an infinite field can be written into only one polynomial expression with its unique coefficients. See Chapter 4 of this book or Appendix E, Theorem 10 of \textit{Linear Algebra} by Friedberg, Insel, and Spence. It is the consequence of the fact that a polynomial of degree $n \geq 1$ has at most $n$ distinct zeros. We will proceed with this definition of polynomials.
        \item We denote the set of all polynomials with coefficients in $\F$ by $\mathcal{P}(\F)$. Under the usual coefficient-wise addition and scalar multiplication, it is obvious that $\mathcal{P}(\F)$ is a vector space over $\F$. In addition, the set of all polynomials with degree at most $m$, denoted by $\mathcal{P}_m(\F)$ is a vector subspace of $\mathcal{P}(\F)$.
    \end{itemize}
    \item Linear independence and dependence can be characterized in multiple ways. A set $S$ of vectors is linearly independent iff the only way to write 0 as $a_1v_1 + a_2v_2 + \cdots +a_mv_m$ is the trivial way. It is equivalent to saying that every linear combination has its only representation, or none of the vectors is in the span of the other vectors (can be written as a linear combination of them). Linear dependence is the negation of independence, but in particular we should remember that it means there exists at least one vector $v_i$ that is in the span of the rest of the vectors.
    \begin{itemize}
        \item Obviously removing such vector $v_i$ from $S$ will not change the span.
        \item In particular, for a linearly independent set $S = \{s_1, s_2, \dots , s_m\} \subsetneq$ a linearly dependent set $T = \{s_1, s_2, \dots , s_m, t_1, t_2, \dots , t_n\}$, one such vector is always in $T \backslash S$. This is because for the nontrivial way of writing $0 = a_1s_1 + \cdots +a_ms_m + b_1 t_1 + \cdots + b_n t_n$, if all $b$'s are 0, then all $a$'s will also be 0. Thus, some $b_i$ must be nonzero, and the corresponding $t_i$ can be removed without changing the span.
        \begin{itemize}
            \item The special case $n=1$ of the contraposition will be useful to us. If one new vector is added the linearly independent $S$ but does not belong to span($S$), then the resulting set is still linearly independent.
        \end{itemize}
        \item Note that a linearly independent set of vectors cannot have the $0$ vector.
    \end{itemize}
    \item We now have the essential tools to prove the following important theorem, that if a vector space $V$ can be spanned by $S = \{w_1, w_2, \dots,w_n\}$, then all linearly independent sets of vector $\{u_1,u_2,\dots,u_m\} \subseteq V$ must have $m \leq n$. The proof suggests adding a new $u_i$ into $S$ and then remove another $w_j$ from $S$ in each step. In the end all the $u$'s will be added and the corresponding number of $w$'s are removed from $S$. Therefore, $m \leq n$.
    \begin{itemize}
        \item The direct consequence of this theorem is the definition of dimension. But before that we need to introduce the notion of a basis and its existence.
    \end{itemize}
    \item A \textit{basis} of $V$ is a linearly independent spanning set of $V$. A set is a basis iff all vectors in $V$ can be written as a unique linear combination of the basis vectors. This follows directly from the definition.
    \item Most of time we are concerned with vector spaces with finite bases. The vector spaces that have a finite spanning set are called \textit{finite-dimensional vector spaces} (FDVS), and we will soon know why they bear this name. First of all, we can show that every finite spanning set of a FDVS $V$ can be reduced to a finite basis. Furthermore, every linearly independent set of a FDVS $V$ can be extended to a basis.
    \begin{itemize}
        \item To prove the first claim, we construct our basis $B$ from scratch and try to add all the elements of the spanning set $S = \{v_1, v_2, \dots, v_n\}$ one by one into the basis, as long as the element added is not in the span of the existing vectors in $B$. In this way, every time a new vector is added to $B$, $B$ will still be linearly independent, and this $B$ will be linearly independent in the end. Furthermore, span$(B) = $ span$(S) = V$ because all the $v$'s left out are in span($B$).
        \begin{itemize}
            \item It is also possible to start $B$ from $S$ and check from $v_1$ to $v_n$ whether each vector is in the span of the rest of the vectors in $B$. If it is, then we remove it from $B$; if it is not, then we keep it. In the end, span($B$) $=$ span($S$) and none of the vectors in $B$ is in the span of the others. However, this is top-down approach is harder in practice to actually find the basis.
            \item This claim shows that every FDVS has a finite basis.
        \end{itemize}
        \item To prove the second claim, similarly construct our basis from the given linearly independent set $L$. We pick a finite basis $B = \{b_1, b_2,\dots,b_n\}$ of $V$ and check whether the next $b$ may be added to $L$ (same to what we did above). In the end, all the $b$'s are in the span of $L$ and $L$ remains linearly independent.
    \end{itemize}
    
    \item Choose two finite bases of a vector space. The fact that they are both linearly independent and both span $V$ tells us that the two bases should have the same size, which we define as the \textit{dimension} of a vector space. Thus, the vector spaces with finite bases are called \textit{finite-dimensional}. From now on, $V$ is by default finite-dimensional.
    \begin{itemize}
        \item The fact that the bases of $V$ has a fixed size implies every spanning set or linearly independent set of this size $\d V$ is a basis itself (because the reduction/extension here is the trivial one).
    \end{itemize}
    \item The subspace dimension $\d U$ is always $\leq$ the whole space dimension $\d V$. To accomplish this we have to resort to the classic procedure: we add vectors from $U$ that are not in the span of the existing ones in $L$. Thus $L$ always remain linearly independent, and it should always be smaller in size than $B$, a spanning set of $U$. Our procedure will eventually terminate and no more vectors can be added to $L$, i.e., all the vectors in $V$ is in \s($L$). $L$ is our desired basis, and $|L| = \d U \leq \d V = |B|$.
    \begin{itemize}
        \item Note it is WRONG to assume that a finite basis of the whole space can always be reduced to a finite basis of the subspace!
        \item Corollary: for FDVS $V$ and its subspace $U$, if the two spaces have the same dimension, that they are the same space. This is an important result useful in many proofs.
    \end{itemize}
    \item There are two more things that deserves mentioning. For FDVS $V$ and its subspace $U$, we can always find subspace $W$ such that $V = U \oplus W$. The idea is to find a basis $B_U$ of $U$ and extend it to the basis $B_V$ of $V$. $\s (B_V \backslash B_U)$ is our desired W. We then only have to show $V = U+W$ and $U \cap W = \{0\}$, which is not difficult.
    \item The dimension of the sum of two vector spaces $U_1$ and $U_2$ can be expressed as follows: $$\d (U_1 + U_2) = \d U_1 + \d U_2 - \d (U_1 \cap U_2).$$
    \begin{itemize}
        \item Despite the fact that it looks like the inclusion-exclusion formula, $+$ and $\cup$ are essentially different, and this cannot be extended to higher orders. To prove the formula, start from a basis $\{u_1,u_2,\dots,u_m\}$ of $U_1 \cap U_2$ (intersection is a subspace). $U_1$ and $U_2$ then have their respective bases $B_1$ and $B_2$. We can show $B_1 \cup B_2$ is a basis of $U_1 + U_2$. See page 47 of the book for the full proof.
        \item Note that when the sum is a direct sum, then the formula is just $\d(U_1 \oplus U_2) = \d U_1 + \d U_2$.
    \end{itemize}
    
\end{itemize}

\section{Linear Maps}
\subsection{The Vector Space of Linear Maps}
\begin{itemize}
    \item A linear map(ping)/transformation is a homomorphism of vector spaces under addition and scalar multiplication. The linear structure is preserved after the transformation $T$. To use symbols, a linear map is a function $T: V \rightarrow W$ such that for all $u,v \in V$ and $\lambda \in \F$ (the field of $V$ and $W$),
    \begin{equation*}
        T(u+v) = Tu + Tv \quad \text{and} \quad
        T (\lambda u) = \lambda T(u).
    \end{equation*}
    \item The zero transformation maps every vector of $V$ to $0_W$. The identity transformation $I_V: V \rightarrow V$ maps each element of $V$ to itself in $V$.
    \item We define the addition and scalar multiplication of linear maps in the vector-wise way: $$(S+T)(v) = Sv + Tv \quad \text{and} \quad (\lambda T)(v) = \lambda T(v),$$ following which we can easily show that the set of linear maps $\mathcal{L}(V,W)$ from $V$ to $W$ is a vector space over $\F$. In addition to the linear properties, we define the \textit{product} $ST$ of linear maps same as $S \circ T$, given the appropriate domain and codomain. Why the product is essential will become clear later. The theory of functions tells us this product is associative [$T_1(T_2 T_3) = (T_1 T_2) T_3$] and the identity works as usual ($I_W T = T I_V = T$). The distributive properties ($(S_1 + S_2) T = S_1 T + S_2 T \text{ and } S(T_1 + T_2) = S T_1 + S T_2$) obviously hold as well.
    \item If $T(x_1,\dots,x_n) = (a_{11}x_1+\cdots+a_{1n}x_n,\dots,a_{m1}x_1+\cdots+a_{mn}x_n)$, then $T$ is obviously a linear map from $\F^n$ to $\F^m$. For a linear map $T \in \mathcal{L}(\F^n,\F^m)$, it must be of this form, and all the $a_{ij}$'s are determined by the mapping of the $n$ canonical basis vectors of $\F^n$.
    \item The uniqueness of the linear map that takes basis vectors $v_1, v_2, \dots, v_n$ to corresponding vectors $w_1, w_2, \dots, w_n$ is the most fundamental theorem in this section. It is our first step toward showing that linear maps and matrices are isomorphic. To prove the theorem, for arbitrary $a_i$'s, let $$T: \sum_{i=1}^n a_i v_i \mapsto \sum_{i=1}^n a_i w_i.$$
    \item Here are a few properties that are useful in linear maps. The proofs are quite routine.
    \begin{itemize}
        \item $T(0_V) = 0_W;$
        \item $T$ is linear iff $T(cx+y) = cT(x) + T(y)$ for all $c,x,y.$
    \end{itemize}
\end{itemize}

\subsection{Null Spaces and Ranges}
\begin{itemize}
    \item The \textit{nullspace}/\textit{kernel} $\n T$ is the preimage of $0_W$. The \textit{range}/\textit{image} $\r T$ is defined the same as the range of a function. The nullspace and the range are respectively subspaces of $V$ and $W$ (by checking the three criterion). If the nullspace (resp. range) is finite-dimensional, then its dimension is the \textit{nullity} (resp. \textit{rank}) of $T$.
    \begin{itemize}
        \item Since $\d U = \d W$ for subspace $U$ of $W$ implies $U = W$, $\text{rank }T = \d W$ implies surjection.
        \item A simple theorem to always keep in mind is that the mapping of the basis vectors of $V$ spans $\r T$.
    \end{itemize}
    \item We are now endowed with all the tools to prove the \textbf{rank-nullity theorem}: for a FDVS $V$ and $T$ from $V$ to $W$, $\r T$ is finite-dimensional with $$\d V = \d \n T + \d \r T.$$
    \begin{itemize}
        \item As always, we need to construct a basis to work with first. Let ${u_1, u_2, \dots, u_n}$ be a basis of $\n T$, and thus can be extended to ${u_1,\dots,u_m,v_1,\dots,v_n}$ a basis of $V$. We prove $T(\{v_1, v_2, \dots, v_n\})$ is a basis for $\r T$. The fact that the $u$'s are in the nullspace shows that $T(v_1), \dots,T(v_n)$ spans $V$. To show the elements are linearly independent, consider that $$a_1 T(v_1)+\cdots+a_n T(v_n) = 0$$ implies $a_1v_1+\dots+a_nv_n \in \n T$. Yet we also have $b_1u_1+\dots+b_nu_n \in \n T$, so we can set the two linear combinations equal. Because the $u$'s and $v$'s are altogether linearly independent, the coefficients $a$'s (and $b$'s) are all 0.
    \end{itemize}
    \item The rank-nullity theorem gives rise to a number of equivalent characterizations regarding injection and surjection. First of all, there is a criterion that says $T$ is injective iff $\n T = {0}$ (proof follows straight from the definition of nullspace). The results below follow.
    \begin{itemize}
        \item A map to a smaller dimensional space cannot be injective because $\d (\n T) > 0$.
        \item A map to a large dimensional space cannot be surjective because $\d (\r T) \leq \d V < \d W$.
        \item $V$ and $W$ are FDVS of equal dimension, then $$T \text{ is injective } \Longleftrightarrow T \text{ is surjective } \Longleftrightarrow \text{rank } T = \d V.$$
    \end{itemize}
    \item Solving systems of linear equations is a direct application of this.
    \begin{itemize}
        \item A homogeneous system of linear equations with more variables than equations has nonzero solutions. Here we can view the system as a linear map $T$ that maps the vector of unknown variables $x \in \F^n$ to $0 \in \F^m$ with $n > m$. The nullity of $T$ is greater than 0 as a result.
        \item Similarly, the $T$ associated with an inhomogeneous equation with more equations than variables is not surjective, and thus some choices of constants on the right side will give no solutions to the system.
    \end{itemize}
\end{itemize}

\subsection{Matrices}
\begin{itemize}
    \item Consider the $n$-dimensional $V$ with basis $\bv$ and $m$-dimensional $W$ with basis $w_1,\dots,w_m$. For $T \in \LVW$, we can find for every $v_k$, a list of $A_{i,k}$'s such that $Tv_k = A_{1,k}w_1 +\cdots+ A_{m,k}w_m = \sum_{j=1}^m A_{j,k}w_j$. As a result, we have $n$ lists of scalar $a$'s, each of length $m$, and we can put them into an $m \times n$ matrix $A$, with entries $a_{i,j}$ in the $i$-th row and $j$-th column. $\mathcal{M}(T,(\bv),(\bw))$ uniquely determines the linear map $T \in \LVW$.
    \begin{itemize}
        \item Assume
        \begin{align*}
            Tv_1 & = A'_{1,1} w_1 + \dots + A'_{1,m} w_m, \\
            & \vdotswithin{=} \\
            Tv_n & = A'_{n,1} w_1 + \dots + A'_{n,m} w_m,
        \end{align*}
        from which we get the $m \times n$ matrix $A'$. Its transpose $n \times m$ $A$ is our defined matrix representation of $T$. The reason for taking transpose will become clear later.
    \end{itemize}
    \item We define matrix addition entry-wise, because under this definition, matrix representation of linear maps preserves addition, i.e. $\M(S+T) = \M(S) + \M(T)$ for $S,T \in \LVW$, assuming $S+T$, $S$, and $T$ share the same ordered bases.
    \item Similarly entry-wise scalar multiplication with matrices give us $\M(\lambda T) = \lambda \M(T)$.
    \item Given the definition of matrix addition and scalar multiplication, $\F^{m,n}$, the set of matrices with $m$ rows and $n$ columns over $\F$, is a vector space of dimension $mn$.
    \item The reason why we represent linear maps from $n$-dimensional VS to $m$-dimensional VS is that we want to preserve the ordering of $``S"$ and $``T"$ after matrix representation, i.e., $\M(ST) = \M(S)\M(T)$ instead of $\M(T)\M(S)$, under the usual definition of matrix multiplication: $$(AC)_{j,k} = \sum_{r = 1}^n A_{j,r}C_{r,k}.$$
    \begin{itemize}
        \item Remember that matrix multiplication is not commutative, but is associative and distributive.
    \end{itemize}
    \item The book defines $A_{j,\cdot}$ and $A_{\cdot,k}$ as the $j$-th row and the $k$-th column of matrix $A$, and thus $(AC)_{j,k} = A_{j,\cdot}C_{\cdot,k}$. Also, $k$-th column of matrix product equals matrix times the $k$-th column of the second matrix: $$(AC)_{\cdot,k} = AC_{\cdot,k},$$ which is an alternative to understanding matrix multiplication. Obviously there is a row equivalent version that says $j$-th row of the matrix product equals $j$-th row times the second matrix.
    \item When we are considering matrix $A$ times a column vector $c$, we can see $Ac$ as the linear combination $c_1A_{\cdot,1} + \cdots + c_nA_{\cdot,n}$.
\end{itemize}

\subsection{Invertibility and Isomorphic Vector Spaces}
\begin{itemize}
    \item $T \in \LVW$ is \textit{invertible} if there exists $S$ such that $ST = I_V$ and $TS = I_W$. We denote this inverse $S$ by $T^{-1}$. One can show that the inverse must be linear and thus belongs to $\mathcal{L}(W,V)$. By the associativity of linear maps, the inverse is unique.
    \item Invertibility is equivalent to bijectivity between vector spaces. Linear maps are functions, so beyond showing that the inverse is linear, the proof is exactly the same.
    \item An \textit{isomorphism} is an invertible linear map; if an isomorphism exists between two vector spaces, then the vector spaces are \textit{isomorphic}.
    \item Two FDVS over $\F$ are isomorphic iff they have the same dimension. Injection gives us $\text{nullity } = 0$ and surjection gives us $\text{rank } T = \d W$. The rank-nullity theorem tells us $\d V = \d W$. On the other hand to prove isomorphism, we show $T(a_1v_1+\cdots+a_nv_n) = a_1w_1+\cdots+a_nw_n$ is both injective and surjective, which is not hard.
    \begin{itemize}
        \item The theorem above implies $n$-dimensional VS is always isomorphic to $\F^n$.
    \end{itemize}
    \item Now we show the isomorphism between a linear map and its corresponding matrix. When the orders of the bases $\bv$ and $w_1,w_2,\dots,w_m$ are fixed, then $\M$ is a function from $\LVW$ to $\F^{m,n}$. As we have noted earlier $\M$ is linear. Thus, it suffices to show that $\M$ is bijective.
    \begin{itemize}
        \item For all $A \in \F^{m,n}$, let $$Tv_k = \sum_{j = 1}^m A_{j,k}w_j.$$ Then this is the \textit{unique} (recall the theorem we emphasized in \S3.1) $T$ such that $A = \M(T)$.
        \item A linear map and its matrix representation are homomorphic not merely in regards to linearity, but also in regards to the preservation of the order of the product $\M(ST) = \M(S)\M(T)$.
    \end{itemize}
    \item By the theorems on isomorphism above, $\d \LVW = mn = (\d V) (\d W)$.
    \item We now shift our focus from the matrix representing an entire linear map to the matrix/column vector representing a vector in the same $V$. We fix the basis $\bv$, then for any $v \in V$, we define
    \begin{equation*}
        \M(v) = 
        \begin{bmatrix}
            a_1 \\ \vdots \\ a_n
        \end{bmatrix},
    \end{equation*}
    where $a_1,\dots,a_n$ are given by $v = a_1v_1+\cdots+a_nv_n$. Here $\M$ is an isomorphism from $V$ to $F^{n(, 1)}$.
    \begin{itemize}
        \item Following this definition, $\M(T)_{\cdot,k} = \M(Tv_k)$.
        \item More importantly, the linear map of a vector can now be completely described by matrix-multiplication. Symbolically, we want to prove $\M(Tv) = \M(T)\M(v)$ for all $v \in V$. Since $v = a_1v_1 + \cdots +a_nv_n$ and $\M$ is linear,
        \begin{align*}
            \M(Tv) & = a_1\M(Tv_1) + \cdots + a_n \M(Tv_n) \\ & = a_1\M(T)_{\cdot, 1} + \cdots + a_n\M(T)_{\cdot, n} \\ & = \M(T)\M(v).
        \end{align*}
        \item How do we interpret this? Consider the following diagram under the appropriate bases for $V$ and $W$.
        \begin{equation*}
            \begin{tikzcd}
            V \arrow[r,"T"] \arrow[d,"\simeq"]& W \arrow[d,"\simeq"] \\
            \F^{n,1} \arrow[r,"\M(T)"] \arrow[u,"\M"] & \F^{m,1} \arrow[u,"\M"]
            \end{tikzcd}
        \end{equation*}
        For a vector $v \in V$, its image $\M(v)$ under the isomorphism $\M$ of $V$ belongs to $\F^{n,1}$. Matrix multiplication with $\M(T)$ gives us $\M(Tv) \in \F^{m,1}$, which we identify with $Tv$ under the isomorphism $\M$ of $W$. This is why a linear map can be described entirely by its matrix representation.
        \item If $V = \F^n$ and $W = \F^m$ and we are using the canonical bases, then the linear map is equivalent to matrix multiplication. Think about the Jacobian matrix representation of the total derivative in the Euclidean space.
    \end{itemize}
    \item A \textit{linear operator} is a linear map from a VS to itself. The notation $\LV$ stands for $\mathcal{L}(V,V)$.
    \item Suppose $V$ is FDVS and $T \in \LV$, then (a) $T$ is invertible iff (b) $T$ is injective iff (c) $T$ is surjective. (This is not true for infinite-dimensional vector space.)
        
    (a) $\implies$ (b) we proved earlier.
        
    (b) $\implies$ (c) by checking the rank-nullity theorem. Injection means nullity $T = 0$, which implies rank $T = \d V$.
        
    (c) $\implies$ (a) by showing $T$ is injective, as nullity $T = \d V - $ rank $T$, which is 0.
\end{itemize}

\subsection{Products and Quotients of Vector Spaces}
\begin{itemize}
    \item The \textit{product} $V_1 \times \dots \times V_n$ of vector spaces $V_1,\dots,V_n$ over $\F$ is the set $$\{(v_1,\dots,v_n) \mid v_1 \in V_1,\dots, v_n \in V_n\}.$$ The addition and scalar multiplication on the product is defined in the usual entry-wise way. It is routine to prove that this product is a vector space over $\F$ as well.
    \item $\d (V_1\times \dots \times V_n) = \d V_1 + \dots + \d V_n$. Why? Choose a basis for each $V_i$, then put every basis vector of $V_i$ into the $i$-th entry of $(v_1,\dots,v_n)$ and let other entries be 0. These vectors are linearly independent and span the product and thus is a basis of the product.
    \item For subspaces $U_1,\dots,U_n$ of $V$, define the linear map $\Gamma: U_1 \times \dots \times U_n \to U_1 + \dots + U_n$ by $$\Gamma(u_1,\dots,u_n) = u_1+\dots+u_n.$$ (One can easily check that this map is indeed linear.) Then $U_1+\dots+U_n$ is a direct sum iff $\Gamma$ is injective (or invertible, since $\Gamma$ is surjective by its definition.) To prove this, we just need to show $\n T = {0}$ iff the only way to write $0 = u_1+\dots+u_n$ is to let all $u$'s be 0. This is quite clear.
    \item The last two theorems lead to the following one. A sum of subspaces $U$'s of $V$ is a direct sum iff dimensions add up. This is because two isomorphic vector spaces must have the same dimension. Therefore, $$\d (U_1+\dots+U_n) = \d (U_1 \times \dots \times U_n) = \d U_1 + \dots + \d U_n.$$
    Construct the linear map $\Gamma$ above from the product of subspaces makes the proof really clean. Recall that we have proved the special case $n=2$ already.
    \item An \textit{affine subset} of $V$ is a subset of $V$ of the form $v+U = \{v+u \mid u \in U\}$ for some vector $v \in V$ and some subspace $U$ of $V$. Here the affine subset $v+U$ is said to be \textit{parallel} to $U$.
    \begin{itemize}
        \item $v+U$ is a subspace of $V$ iff $v \in U$. If $v \in U$, then $v+U = U$, subspace of $V$. If $v \notin U$, then $0 \notin v+U$, and thus $v+U$ cannot be a subspace of $V$.
    \end{itemize}
    \item Under the same setting, the quotient space $V/U$ is the set of all affine subsets of $V$ parallel to $U$, i.e., $V/U = \{v+U \mid v \in V\}$.
    \item For subspace $U$ of $V$ and $v,w \in U$, then
        \begin{equation*}
            \text{(a) } v-w \in U \iff \text{(b) } v+U = w+U \iff \text{(c) } (v+U) \cap (w+U) \not= \emptyset.
        \end{equation*}
    (a) $\implies$ (b) as $v+u = w+((v-w)+u) \in w+U$ (and the same for the other direction). (b) $\implies$ (c) because $v+U$ is not empty, while (c) $\implies$ (a) because $v+u_1=w+u_2$ for some $u_1$ and $u_2$, and we then move $v-w$ to the left side.
    \item The addition and scalar multiplication on $V/U$ are defined by  $$(v+U)+(w+U) = (v+w)+U \quad \text{and} \quad \lambda(v+U) = \lambda v + U.$$ We should pay particular attention to this definition because \textbf{an affine subset do not have a unique representation} (for $v \not= v'$, it is possible that $v+U = v'+U$ still)! Therefore, one has to show that  different representations $v+U$ and $w+U$ of the same affine subsets of $U$ still gives us a \textbf{unique} $(v+w)+U$, so that the addition function above is well-defined (and similar for scalar multiplication). Suppose $v+U = v'+U$ and $w+U = w'+U$, then essentially we want to prove $(v+w)+U=(v'+w')+U$. By assumption $v-v'$ and $w-w' \in U$, $(v+w)-(v'+w') \in U$, and we reach the conclusion (and similar for scalar multiplication).
    \item $V/U$ with the two operations above is a vector space. Showing this is routine, but one has to remember that $V/U$ is not a subspace of $V$. Despite all the elements of the quotient space is in the whole space, the additive identity is now $0+U=U$, and the additive inverse of $v+U$ is $-v+U$. (The quotient space is discussed with respect to a fixed subspace.) Therefore, we have to check the 8 VS properties.
    \item To find the dimension of $V/U$, we appeal to the trick of constructing a linear map again. (One should check its linearity) The \textit{quotient map} $\pi: V \to V/U$ is given by $$\pi(v) = v+U.$$
    For FDVS $V$ and its subspace $U$, $\d V/U = \d V - \d U$. $\pi$ is a map onto $V/U$ obviously, and $\n \pi = U$. Therefore the formula follows.
    \item Now we define the induced map $\tilde{T}: V/(\n T) \to W$ of $T: V \to W$ by $$\tilde{T}(v+\n T) = Tv.$$ $u + \n T = v + \n T$ implies $u - v \in \n T$, and thus $Tu-Tv = 0$. Thus the induced map is well defined. Checking the linearity of $\tilde{T}$ is routine. Note that $T = \tilde{T} \circ \pi$.
    \begin{equation*}
            \begin{tikzcd}
                V \arrow[rr,"T"] \arrow[dr,"\pi"] && W \\
                & {V/\n T} \arrow[ur,"\tilde{T}"]
            \end{tikzcd}
        \end{equation*}
    \item The definition leads to an important theorem that exists for different algebraic structures. For $T \in \LVW$, $\tilde{T}$ is injective and $\r \tilde{T} = \r T$. The latter simply follows from the definition, while the first requires us to prove $\n T = \{0+\n T\}$, the identity. $\tilde{T}(v+\n T) = Tv = 0$ gives us $v-0 \in \n T$. Hence, $v+\n T = 0+\n T$, and the conclusion follows.
        \begin{itemize}
            \item The two claims combined tell us $V/(\n T) \simeq \r T$ under $\tilde{T}$.
        \end{itemize}
    \end{itemize}
    
\subsection{Duality}
\begin{itemize}
    \item A \textit{linear functional} on $V$ is a linear map belonging to $\mathcal{L}(V,\F)$. The \textit{dual space} of $V$, denoted usually by $V^*$ or $V'$, is the vector space $\mathcal{L}(V,\F)$.
    \item We proved earlier that $\d \LVW = (\d V)(\d W)$. Therefore, $\d V' = \d V$.
    \item The \textit{dual basis} of $v_1,\dots,v_n$, basis of $V$, is a set of vectors $\phi_1,\dots,\phi_n$ in $V'$, where each $\phi_i$ is a linear functional defined by
    \begin{equation*}
        \phi_i(v_j) = \left\{
            \begin{array}{rl}
                1 \quad & \text{if } i = j, \\
                0 \quad & \text{if } i \not= j.
            \end{array}
        \right.
    \end{equation*}
    As it turns out, these $\phi_i$'s indeed form a basis of $V'$. 
    
    \item
\end{itemize}

\section{Polynomials}

\section{Eigenvalues, Eigenvectors, and Invariant Subspaces}

\section{Inner Product Spaces}

\section{Operators on Inner Product Spaces}

\section{Operators on Complex Vector Spaces}

\section{Operators on Real Vector Spaces}

\section{Trace and Determinant}

\end{document}