\documentclass[11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{setspace,microtype}
\usepackage{framed,xcolor}
\usepackage[pdfusetitle,bookmarksnumbered,colorlinks]{hyperref} %load this at the end

\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\R}{\mathbf{R}}
\newcommand{\where}{\,|\,}
\newcommand{\blank}{\,\cdot\,}
\newcommand{\inp}[2]{\langle #1, #2 \rangle}
\newcommand{\nm}[1]{\lVert #1 \rVert}
\newcommand{\abs}[1]{\lvert #1 \rvert}
\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\trp}{\mathrm T}
\newcommand{\trace}{\operatorname{trace}}

\renewcommand{\Pr}{\operatorname{P}}
\newcommand{\E}{\mathop{}\!\mathbf{E}}
\newcommand{\I}[1]{\mathbf{1}_{\{#1\}}}
\newcommand{\SE}{\operatorname{SE}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Corr}{\operatorname{Corr}}
\newcommand{\argmax}{\operatorname*{arg\,max}}
\newcommand{\argmin}{\operatorname*{arg\,min}}
\newcommand{\convd}{\xrightarrow[]{\mathrm D}}
\newcommand{\convp}{\xrightarrow[]{\mathrm P}}
\newcommand{\convas}{\xrightarrow[]{\mathrm{a.s.}}}
\newcommand{\eqD}{\stackrel{\mathrm D}{=}}

\newcommand{\df}[1]{\textit{\textsf{#1}}} % definition
% \newcommand{\codevar}[1]{\ensuremath{\mathtt{\textcolor{brown}{#1}}}} % variable in code

\usepackage{thmtools}
\declaretheoremstyle[headfont=\scshape]{plain}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\newtheorem*{thm*}{Theorem}
\newtheorem*{lem*}{Lemma}
\declaretheoremstyle[headfont=\bfseries]{definition}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\declaretheoremstyle[headfont=\itshape]{remark}
\theoremstyle{remark}
\newtheorem*{rmk*}{Remark}
\theoremstyle{definition}
\newtheorem{sol}[thm]{Solution}
\newtheorem{mansolinner}{Solution}
\newenvironment{mansol}[1]{%
  \renewcommand\themansolinner{#1}%
  \mansolinner
}{\endmansolinner}

\usepackage{mathpazo}
\usepackage{eucal}
\usepackage[scaled=1.008]{inconsolata}
\usepackage[scaled=0.842]{berasans}

\onehalfspacing
\setlength{\parskip}{0em}
\setlist{listparindent=\parindent,parsep=0pt}

\makeatletter
\let\@subtitle\@empty % default value
\protected\def\subtitle#1{\gdef\@subtitle{#1}}
\def\@maketitle{%
  \newpage
  \begin{center}%
  \let \footnote \thanks
    {\Large \@title \par}%
    {\@subtitle \par}%
    \vskip 0.5em%
    {\lineskip .5em%
      \begin{tabular}[t]{c}%
        \@author
      \end{tabular}\par}%
    {\@date}%
    \vskip -0.5em%
  \end{center}%
  \par}
\makeatother

\title{HDP Selected Exercises}
\author{Cheng, Feng}
\date{}

\begin{document}
\maketitle

\section{Preliminaries on random variables}
\begin{mansol}{1.2.2}
    We already know that for $X \geq 0$, \[\E X = \int_{0}^\infty \Pr(X > t) \, dt.\] In general for a real number $x$, \[
    x = \I{x \geq 0} \cdot \int_0^\infty \I{t < x} \,dt - \I{x < 0} \cdot \int_{-\infty}^0 \I{x < t} \,dt.
\]
When $x<0$, $\I{t<x} = 0$ for all $t\geq 0$; when $x \geq 0$, $\I{x<t} = 0$ for all $t \leq 0$. Hence \[x = \int_0^\infty \I{t < x} \,dt - \int_{-\infty}^0 \I{x < t} \,dt,
\]
and by interchanging $\int$ and $\E$ (via Fubini-Tonelli's theorem), we have \[ \E X = \int_{0}^\infty \Pr(X > t) \, dt - \int_{-\infty}^0 \Pr(X < t) \,dt. \qedhere\]
\end{mansol}

\begin{mansol}{1.2.3}
By the change of variables formula, since $g(t) = t^p$ is a strictly increasing function that maps $[0,\infty)$ onto $[0,\infty)$,
\begin{align*}
\text{RHS} & = \int_0^\infty \Pr(\abs X  > t) (t^p)'\,dt \\
& = \int_0^\infty \Pr(\abs X > \sqrt[p]{u})\,du \\ & = \int_0^\infty \Pr(\abs X ^p > u) \,du = \text{LHS}.
\end{align*}
\end{mansol}

\begin{mansol}{1.3.3}
Jensen's inequality tells us $(\E X)^2 \leq \E X^2$ [true regardless of whether $X$ has finite second moment or not]. Now consider \[
\Bigl(\E \Big\lvert\frac{1}{N} \sum_{i=1}^N X_i - \mu\Big\rvert \Bigr)^2 = \bigl[\E\bigl(\ol{X} - \E\ol{X}\bigr)\bigr]^2,
\] which is
\[
\leq \E \bigl(\ol X - \E \ol X \bigr)^2 = \Var(\ol X) = \frac{\sigma^2}{N}.
\]
Taking the square root gives us the desired result.
\end{mansol}

\section{Concentration of sums of independent random variables}

\begin{mansol}{2.1.4}
Let Z replace $g \sim N(0,1)$. By the law of unconscious statistician, 
\[
\E(Z^2 \I{Z > t}) = \int_t^\infty z^2 \frac{1}{\sqrt{2\pi}} e^{-z^2 / 2}\,dz 
= \frac{1}{\sqrt{2\pi}}\int_t^\infty z \cdot \bigl(z e^{-z^2 / 2}\bigr)\,dz.
\]
Integration by parts gives us \begin{align*}
\int_t^\infty z \cdot \bigl(z e^{-z^2 / 2}\bigr)\,dz & = \bigl[z \cdot \bigl(- e^{-z^2 / 2}\bigr)\bigr]_t^\infty - \int_t^\infty \bigl(- e^{-z^2/2}\bigr)\,dz \\
& = t \bigl(e^{-t^2 / 2}\bigr) + \int_t^\infty \bigl(e^{-z^2/2}\bigr)\,dz.
\end{align*}
Hence \[
\E(Z^2 \I{Z > t}) = t \cdot \frac{1}{\sqrt{2\pi}} e^{-t^2/2} + \Pr(Z > t),
\] and the inequality on the right is obvious by proposition 2.1.2.
\end{mansol}

\begin{mansol}{2.2.7}[Proof of 2.2.6 Hoeffding's inequality for general bounded R.V.]\ \\
We follow what we have done in the special case for Rademacher distribution (2.2.2): for any chosen $\lambda > 0$, 
\begin{align}
\Pr \biggl(\sum_{i=1}^N (X_i - \E X_i) \geq t \biggr) & = \Pr \biggl(\exp\Bigl(\lambda \sum_{i=1}^N (X_i - \E X_i)\Bigr) \geq e^{\lambda t}\biggr) \nonumber \\ 
& \leq \frac{\E \exp\Bigl(\lambda \sum_{i=1}^N (X_i - \E X_i)\Bigr)}{e^{\lambda t}} \nonumber \\
& = e^{-\lambda t} \prod_{i=1}^N \E \exp\bigl(\lambda (X_i - \E X_i)\bigr). \label{eq:1}
\end{align}
We know that $\E (X_i - \E X_i) = 0$. To show the exact bound given in the problem, we have to show that

\begin{centering}
for a zero-mean random variable $X \in [a,b]$, for any $\lambda \in \R$, \begin{equation}
\E \exp(\lambda X) \leq \exp\biggl(\frac{1}{8} \lambda^2 (b - a)^2\biggr). \label{eq:2}
\end{equation}
\end{centering}

This is called the \df{Hoeffding's lemma}. A proof of the inequality with 2 instead of 8 is in example 2.4 of HDS\footnote{\textit{High-Dimensional Statistics} by Wainwright. The proof of the original inequality is outlined in exercise 2.4 of the same book, and is given in full length in \textit{Concentration Inequalities} by Boucheron et al. This proof uses the \df{change of measure} technique.}. The technique used is called the \df{symmetrization argument}, and there is another proof of the inequality with 4 instead of 8 using the same technique at \href{https://marcromani.github.io/2021-05-01-hoeffding-lemma/}{this link}. For the most elementary proof, see \href{https://en.wikipedia.org/wiki/Hoeffding\%27s_lemma}{Wikipedia}, which is given below.

\begin{proof}
Since $e^{\lambda x}$ is convex, we have for $x \in [a,b]$ \[
e^{\lambda x} \leq \frac{b-x}{b-a}e^{\lambda a} + \frac{x-a}{b-a}e^{\lambda b}.
\]
Taking expectation on both sides, and we have \begin{equation}
\E \exp(\lambda X) \leq \frac{b}{b-a} e^{\lambda a} - \frac{a}{b-a} e^{\lambda b} = e^{L(\lambda(b-a))}, \label{eq:3}
\end{equation}
where for all $h \in \R$, $L(h) = - \gamma h + \log(1 - \gamma + \gamma e^h)$, with $\gamma = -\frac{a}{b - a} > 0$ [so that the log is well-defined].

Notice that $L(0) = 0$, and $L'(0) = -\gamma + \frac{\gamma e^h}{1 - \gamma + \gamma e^h}\big|_{h = 0} = 0$. \begin{align*}
L''(h) & = \biggl(\frac{\gamma e^h}{1 - \gamma + \gamma e^h}\biggr)' \\
& = \frac{\gamma e^h}{1 - \gamma + \gamma e^h} - \frac{\gamma^2 e^{2h}}{(1 - \gamma + \gamma e^h)^2} \\
& = t - t^2 \leq 1/4\quad \text{for any }t\in \R, 
\end{align*}
where we let $t = \frac{\gamma e^h}{1 - \gamma + \gamma e^h}$. Now we appeal to Taylor's theorem: for $h \neq 0$, there exists a $\xi$ between $h$ and 0 such that \[L(h) = 0 + 0\cdot h + \frac{L''(\xi)}{2} \cdot h^2 \leq \frac{1}{8} h^2.\] Now let $h = \lambda(b-a)$ and plug it back into \eqref{eq:3}, and we have shown \eqref{eq:2}.
\end{proof}

Now we return to \eqref{eq:1}. For any $t > 0$, \begin{align*}
e^{-\lambda t} \prod_{i=1}^N \E \exp\bigl(\lambda (X_i - \E X_i)\bigr) & \leq e^{-\lambda t} \prod_{i=1}^N \exp\biggl(\frac{N}{8} \lambda^2 \bigl((M_i - \E X_i)-(m_i - \E X_i)\bigr)^2\biggr) \\ 
& = \exp \biggl(\frac{\sum_{i = 1}^N (M_i - m_i)^2}{8}\lambda^2 - t \lambda \biggr)
\end{align*}
The expression in the large parentheses is minimized at $\lambda = \frac{4 t}{\sum(M_i - m_i)^2}$ by $- \frac{2t^2}{\sum(M_i - m_i)^2}$. We can thus conclude that \[
\Pr \biggl(\sum_{i=1}^N (X_i - \E X_i) \geq t \biggr) \leq \exp \biggl(- \frac{2t^2}{\sum_{i=1}^N (M_i - m_i)^2}\biggr),
\]
as desired.
\iffalse
Before this we need to show: for a random variable $Y \in [m,M]$,
\[\Var(Y) \leq \frac 1 4 (M-m)^2.\]

\begin{proof}
We know 
\[\Var(Y) = \E (Y - \E Y)^2 \leq \E \biggl(Y - \frac{M+m}{2}\biggr)^2\]
because $\E Y$ is the minimizer of the MSE. Notice that \begin{align*}
\E \biggl(Y - \frac{M+m}{2}\biggr)^2 & = \frac 1 4 \E [(Y - M) + (Y - m)]^2 \\
& \leq \frac 1 4 \E[(Y - m) - (Y - M)]^2
\end{align*}
because $Y - M \leq 0$. Hence $\Var(Y) \leq \frac 1 4 (M-m)^2.$
\end{proof}

Now we return to proving \eqref{eq:1}. Let $X'$ be an independent copy of $X$. Because here $\E X = \E X' = 0$, \begin{align*}
\E_X \exp(\lambda X) & = \E_X \exp\bigl(\lambda (X - \E_{X'}X')\bigr) \\
& \leq \E_X \E_{X'} \exp\bigl(\lambda (X - X') \bigr),
\end{align*}
by Jensen's inequality. Let $U = X - X'$. $U$ is a zero-mean symmetric\footnote{Clearly $\E U = \E X - \E X' = 0$. Since $X$ and $X'$ are i.i.d., $X$ and $X'$ are exchangeable, i.e., $(X,X') \eqD (X',X)$. Applying the function $g(a,b) = a - b$ to the two random vectors shows that $X - X' \eqD X' - X$, i.e., $U$ is symmetric.} random variable, and is always in $[m - M, M - m]$. Now by Taylor's theorem, for all $u \in [m - M, M - m]$, 
\[
e^{\lambda u} = 1 + (\lambda u) + \frac{\lambda^2 u^2}{2!} + \frac{\lambda^3 u^3}{3!}e^{\lambda \theta}
\]
for some $\theta$ between 0 and $u$. We want to take the expectation of the expression above. By symmetry we know that $\E U^3 = 0 = \E U$. Meanwhile, $\E U^2 = \Var(U) = 2 \Var(X)$. It follows that 
\begin{align*}
\E e^{\lambda U} & = 1 + \frac{\lambda^2}{2} \E U^2\\
& = 1 + \lambda^2
\end{align*}
\fi
\end{mansol}

\begin{mansol}{2.2.8}[Boosting]
Let $X_i = \I{\text{wrong $i$-th classification}}$. Then $\E X_i = \frac{1}{2} - \delta$. Basically we want to bound $\Pr \bigl(\sum_{i=1}^N X_i \geq \frac{N}{2}\bigr)$ above by $\epsilon$. We try to use Hoeffding's inequality 2.2.6:
\begin{align*}
\Pr \biggl(\sum_{i=1}^N X_i \geq \frac{N}{2}\biggr) & = \Pr \biggl(\sum_{i=1}^N X_i - N\Bigl(\frac{1}{2} - \delta \Bigr)\geq N\delta \biggr) \\
& \leq \exp\biggl(-\frac{2 N^2\delta^2}{N}\biggr).
\end{align*}
We may just let $\exp\bigl(-2N \delta^2\bigr) \leq \epsilon$; equivalently this means $N \geq -\frac{1}{2\delta^2} \log(\epsilon)$.
\end{mansol}

\begin{mansol}{2.2.9}
\begin{enumerate}[label=(\alph*)]
    \item Easy; by Chebyshev's inequality, let $N \geq 4 \sigma^2 / \epsilon^2$.
    \item Let $\hat{\mu}_1,\hat{\mu}_2,\ldots,\hat{\mu}_K$ be $K$ weak estimates from part (a). The sample size $N$ is now $\geq 4K \sigma^2/\epsilon^2$.

    Consider $\hat \mu = \operatorname{median}\{\hat \mu_i\}_{i=1}^K$, and let $Y_i = \I{\abs{\mu_i - \mu} > \epsilon}$ for $i$ between 1 and $K$. Then $\hat \mu - \mu > \epsilon$ (resp.\ $< -\epsilon$) implies that more than half of $\mu_i - \mu$ is $> \epsilon$ (resp.\ $< -\epsilon$). To be precise, let $K$ be an odd number, then \[\abs{\hat\mu - \mu} > \epsilon \implies \sum_{i=1}^K Y_i \geq K/2.\] It follows that \[
        \Pr(\abs{\hat \mu - \mu} > \epsilon) \leq \Pr\Bigl(\sum_{i=1}^K Y_i \geq K/2\Bigr) \leq \Pr\Bigl(\sum_{i=1}^K (Y_i - \E Y_i) \geq K/4\Bigr),
    \]
    where the second inequality follows from \[\E Y_i = \Pr(\abs{\mu_i - \mu} > \epsilon) \leq 1/4\] given in part (a). Now by Hoeffding's inequality for bounded random variables (theorem 2.2.6) we have \[
    \Pr\Bigl(\sum_{i=1}^K (Y_i - \E Y_i) \geq K/4\Bigr) \leq \exp(- K / 8).
    \]
    Therefore to let $\Pr(\abs{\hat \mu - \mu} > \epsilon) \leq \delta$, we may just let $\exp(- K / 8) \leq \delta$, or $K \geq 8 \log(\delta^{-1})$. This show that the median estimates from a sample of total size $\geq 32 \log(\delta^{-1}) \sigma^2/\epsilon^2$ gives an $\epsilon$-accurate estimate with probability at least $1-\delta$.
\end{enumerate}
\end{mansol}

\begin{mansol}{2.2.10}
\begin{enumerate}[label=(\alph*)]
\item Easy; by $\int_0^\infty e^{-tx} dx = 1/t$.
\item Easy; just follow the hint.
\end{enumerate}
\end{mansol}

\begin{mansol}{2.3.2}
For $\lambda < 0$, \[\Pr(S_N \leq t) = \Pr\bigl(e^{\lambda S_N} \geq e^{\lambda t}\bigr) \leq e^{-\lambda t} \E \exp(\lambda S_N).\]
Now follow the proof of theorem 2.3.1 and choose $\lambda = \log(t/\mu) < 0$ this time as well. \textit{Note that here $t > 0$.}
\end{mansol}

\begin{mansol}{2.3.3}
Let us see $X \sim \text{Poisson}(\lambda)$ as the limit in distribution of a sum $S_N$ of Bernoulli random variables specified by the Poisson limit theorem. Take limits on the two sides of theorem 2.3.1 and we get \[\Pr(X \geq t) = \lim_{N\to \infty} \Pr(S_N \geq t) \leq \lim_{N\to \infty} e^{-\mu} \Bigl(\frac{e\mu}{t}\Bigr)^t = e^{-\lambda}\Bigl(\frac{e\lambda}{t}\Bigr)^t.\]
\end{mansol}

\begin{mansol}{2.3.5}
    An important log inequality (via Padé approximation) says that 
    \begin{align}
        \frac{2x}{2+x} & \leq \log(1+x) \leq \frac{x}{2}\cdot\frac{2+x}{1+x}\quad \text{for }x\geq 0,\text{ and} \label{eq:4}\\
        \frac{2x}{2+x} & \geq \log(1+x) \geq \frac{x}{2}\cdot\frac{2+x}{1+x}\quad \text{for }-1 < x\leq 0. \label{eq:5}
    \end{align}
    Note the expressions in \eqref{eq:4} and \eqref{eq:5} above all have Taylor expansions $x - \frac{x^2}{2} + O(x^3)$ within their radii of convergence. Also, rewriting $\log(1+x)$ in \eqref{eq:4} by $-\log\bigl(1 - \frac{x}{1+x}\bigr)$ gives us \eqref{eq:5}. Just to verify the inequalities, check the derivatives of the expressions as usual.

    From $\Pr(S_N - \mu \geq \delta \mu) \leq \Bigl(\frac{e^\delta}{(1+\delta)^{1+\delta}}\Bigr)^\mu$, we want to show that $\frac{e^\delta}{(1+\delta)^{1+\delta}} \leq e^{-c_1\delta^2}$ for some $c_1 > 0$. Take the logarithm of the LHS gives \[
    \delta - (1+\delta)\log(1+\delta), \text{ which is } \leq \delta - (1+\delta)\cdot\frac{2\delta}{2+\delta} = -\frac{\delta^2}{2+\delta}
    \]
    Since we let $\delta \leq 1$, we can let $c_1 = 1/3$.

    By exercise 2.3.2 we know $\Pr(S_N - \mu \leq -\delta \mu) \leq \Bigl(\frac{e^{-\delta}}{(1-\delta)^{1-\delta}}\Bigr)^\mu$ given $\delta \in (0,1)$, and when $\delta = 1$ the inequality is trivally true. We want to find some $c_2 > 0$ such that \[
    -\delta - (1-\delta)\log(1-\delta) \leq -c_2 \delta^2.
    \]
    For $-\delta \in [-1,0)$, \[
    \text{LHS} \leq -\delta - (1 -\delta)\cdot \frac{-2\delta}{2-\delta} = -\frac{\delta^2}{2-\delta}.
    \]
    By $-\delta < 0$ we may choose $c_2 = 1/2$. 

    Hence for $\delta \in (0,1]$, \[
    \Pr(\abs{S_N - \mu} \geq \delta \mu) \leq e^{-\frac{1}{2}\mu \delta^2} + e^{-\frac{1}{3}\mu \delta^2} \leq 2 e^{-\frac{1}{2}\mu \delta^2}.
    \]
\end{mansol}

\begin{mansol}{2.5.1}
    \begin{align*}
        \E \abs{X}^p & = \int_{x = -\infty}^{\infty} \abs{x}^p \frac{1}{\sqrt{2 \pi}}e^{-\frac{1}{2}x^2}\,dx \\
        & = \sqrt{\frac{2}{\pi}}\int_{x=0}^\infty x^p e^{-\frac{1}{2}x^2}\,dx \\
        & = \sqrt{\frac{2}{\pi}}\int_{t=0}^\infty (\sqrt{2t})^p e^{-t}\,d{\sqrt{2t}} \\
        & = \frac{2^{\frac{p+1}{2}}}{\sqrt{\pi}} \int_{t=0}^\infty t^{p/2} e^{-t} \cdot \frac{1}{\sqrt{2}} t^{-\frac{1}{2}}\,dt \\
        & = 2^{p/2}\frac{\int_{t=0}^\infty t^{\frac{p-1}{2}} e^{-t}\,dt}{\sqrt{\pi}} = \bigl(\sqrt{2}\bigr)^p\frac{\Gamma\bigl(\frac{p+1}{2}\bigr)}{\Gamma\bigl(\frac{1}{2}\bigr)}.
    \end{align*}
    Now take the $p$-th root on both sides to get $\nm{X}_{L^p} = \sqrt{2} \biggl[\frac{\Gamma\bigl(\frac{p+1}{2}\bigr)}{\Gamma\bigl(\frac{1}{2}\bigr)}\biggr]^{1/p}$. Note that when $p$ is even, \[\Gamma\Bigl(\frac{p+1}{2}\Bigr) = \frac{p-1}{2}\frac{p-3}{2}\cdots\frac{1}{2}\cdot\Gamma\Bigl(\frac{1}{2}\Bigr),\] and when $p$ is odd, $\Gamma\bigl(\frac{p+1}{2}\bigr) = \bigl(\frac{p-1}{2}\bigr)!$. Both cases show that \[\biggl[\frac{\Gamma\bigl(\frac{p+1}{2}\bigr)}{\Gamma\bigl(\frac{1}{2}\bigr)}\biggr]^{1/p} = O\bigl(((p-1)!!)^{1/p}\bigr) = 
    O\bigl(\bigl(\sqrt{p!}\bigr)^{1/p}\bigr) = O\bigl(\sqrt{p}\bigr),\] and thus $\nm{X}_{L^p} = O(\sqrt{p})$.
\end{mansol}

\begin{mansol}{2.5.4}
    Expand $\E \exp(\lambda X)$ to get \begin{align*}
        \lambda \E X & = \E \exp(\lambda X) - 1 - \sum_{k=2}^\infty \frac{(\E X^k)\lambda^k}{k!} \\
        & \leq \exp(K_5^2 \lambda^2) - 1 - \sum_{k=2}^\infty \frac{(\E X^k)\lambda^k}{k!}
    \end{align*}
    Now consider $\lambda \uparrow 0$ and $\downarrow 0$ respectively. By the uniform convergence and continuity of convergent power series we know that $\lim_{\lambda \to 0}\sum_{k=2}^\infty \frac{(\E X^k)\lambda^k}{k!} = 0$. Hence $\lambda \E X \leq 0$ for both $\lambda \uparrow 0$ and $\downarrow 0$. This shows that $\E X = 0$.
\end{mansol}

\begin{mansol}{2.5.5}
    \begin{enumerate}[label=(\alph*)]
        \item $\E \exp(\lambda^2 X^2) = \frac{1}{\sqrt{2\pi}}\int_{x = -\infty}^\infty \exp\bigl(\bigl(\lambda^2 -\frac 1 2\bigr) x^2\bigr)\,dx$. We have to let $\lambda^2 < 1/2$, i.e., $\lambda \in \bigl(\frac{-1}{\sqrt{2}}, \frac{1}{\sqrt{2}}\bigr)$, so that the improper integral converges and can be evaluated as a Gaussian integral.
        \item The \df{essential supremum} $\nm{X}_\infty$ is defined to be $\sup\{M \in \R : \mu(\{\omega : \abs{X(\omega)} > M\}) > 0\}$. Consider \begin{align*}
            \Pr (\abs{X} > M) & = \Pr\bigl(\exp(\lambda^2 X^2) > \exp(\lambda^2 M^2)\bigr) \\
            & \leq \frac{\E \exp(\lambda^2 X^2)}{\exp(\lambda^2 M^2)} \\
            & \leq \exp\bigl(\lambda^2(K^2 - M^2)\bigr).
        \end{align*}
        Choose $M > K$ to make the inequality nontrivial. Since the inequality above holds for all real $\lambda$, take $\lambda \to \infty$ and we see $\Pr(\abs{X} > M) = 0$ for any $M > K$. This shows that $X$ is a bounded random variable.
    \end{enumerate}
\end{mansol}

\begin{mansol}{2.5.7}
    First, \[X = 0 \implies \nm{X}_{\psi_2} = \inf\{t:t>0\} = 0.\] Now assuming $\nm{X}_{\psi_2} = 0$, if we assume that $X$ is not almost surely 0, then $\E X^2 \geq (\E X)^2 > 0$. By Jensen's inequality, given $0<t\leq 1$, \[
    \bigl(e^{\E X^2}\bigr)^{1/t^2} \leq \bigl(\E e^{X^2}\bigr)^{1/t^2} \leq \E \exp(X^2 / t^2).
    \]
    Since $e^{\E X^2} > 1$, we can choose some $t > 0$ such that $\bigl(e^{\E X^2}\bigr)^{1/t^2} > 2$. By the definition of infimum we know that for some $0 < \delta < t$, \[
         \bigl(e^{\E X^2}\bigr)^{1/t^2} \leq \bigl(e^{\E X^2}\bigr)^{1/\delta^2} \leq \E \exp(X^2 / \delta^2) \leq 2,
    \]
    which leads to contradiction. Hence $X = 0$ a.s.

    Homogeneity is clear from definition, and now we prove the triangle inequality. Let $g(x) = \exp(x^2)$, which as a composition of convex functions is convex. Now let $t_X = \nm{X}_{\psi_2} + \epsilon/2$, and $t_Y = \nm{Y}_{\psi_2} + \epsilon/2$ for any $\epsilon > 0$, and we have \[
        g \biggl(\frac{X+Y}{t_X + t_Y}\biggr) \leq \frac{t_X}{t_X + t_Y} g\biggl(\frac{X}{t_X}\biggr) + \frac{t_Y}{t_X + t_Y} g\biggl(\frac{Y}{t_Y}\biggr)
    \]
    Now take expectation on both sides [we can certainly ignore the case $t_X$ or $t_Y = 0$]. It follows that \[\E g \biggl(\frac{X+Y}{t_X + t_Y}\biggr) \leq 2.\]
    This tells us that $\nm{X}_{\psi_2} + \nm{Y}_{\psi_2} + \epsilon = t_X + t_Y \in \{t>0 : \E g(\frac{X+Y}{t}) \leq 2\}$. Therefore \[
        \nm{X+Y}_{\psi_2} \text{ exists and is } \leq \nm{X}_{\psi_2} + \nm{Y}_{\psi_2} + \epsilon.
    \]
    Since $\epsilon$ is arbitrary, the triangle inequality follows.
\end{mansol}

\begin{rmk*}
    When the infimum exists, it is attained by the monotone convergence theorem. Therefore we can directly plug the subgaussian norm in when proving the triangular inequality.
\end{rmk*}

\begin{mansol}{2.5.10}
    
\end{mansol}

\begin{mansol}{2.5.11}

\end{mansol}

\begin{mansol}{2.6.5}
    The first inequality is just Lyaponouv's inequality.

    To prove the second inequality, just use (2.15): 
    \begin{align*}
        \Big\lVert\sum_i a_iX_i\Big\rVert_{L^p} & \lesssim \Big\lVert\sum_i a_iX_i\Big\rVert_{\psi_2} \sqrt{p} \\
        & \lesssim \Bigl(\sum_i \nm{a_i X_i}_{\psi_2}^2\Bigr)^{1/2} \sqrt{p} \\
        & = \Bigl(\sum_i a_i^2\nm{ X_i}_{\psi_2}^2\Bigr)^{1/2} \sqrt{p} \\
        & \leq K \sqrt{p} \nm{a}_2,
    \end{align*}
    where $K \coloneqq \max_i \nm{X_i}_{\psi_2}$.

    
\end{mansol}

\begin{mansol}{2.8.5 \& 2.8.6}[Bernstein's inequality for bounded distributions]\ \\
    Just use the hint to solve 2.8.5. For the actually inequality, when approaching \begin{align*}
    \Pr \Bigl(\sum_{i=1}^N X_i \geq t\Bigr) & \leq e^{-\lambda t } \prod_{i=1}^N \E \exp(\lambda X_i) \\ 
    & \leq \exp\bigl(g(\lambda) \sigma^2 - \lambda t\bigr),
    \end{align*}
    we simply let $\lambda t = 2 g(\lambda)\sigma^2$. This is $g(\lambda) > 0$ under the constraint $\abs \lambda < 3/K$, so that \[g(\lambda) \sigma^2 - \lambda t < 0.\] Assuming $\lambda > 0$, we get $\lambda^* = \frac{t}{\sigma^2 + t\frac{K}{3}}$. Now plug this back into $\exp\bigl(g(\lambda) \sigma^2 - \lambda t\bigr)$ to give the desired upper bound to $\Pr \bigl(\sum_{i=1}^N X_i \geq t\bigr)$.
\end{mansol}

\newpage
\section{Random vectors in high dimensions}
\begin{mansol}{3.3.3}
    \begin{enumerate}[label=(\alph*)]
        \item By the property of normal distribution we may WLOG prove the following: for $g \sim N(0,I_n)$ and $u \in \R^n$ with $\nm{u}_2 = 1$, $\inp{g}{u}\sim N(0,1)$.
        
        Note that we can always find an orthogonal matrix $U$ such that $e_1 = Uu$. By rotational invariance we then have $Ug \sim N(0,I_n)$. By the definition of multivariate normal we know that $\inp{Ug}{e_1} \sim N(0,1)$. It follows that $\inp{g}{U^\trp e_1} = \inp{g}{u} \sim N(0,1)$, as desired.
        \item We normalize each of the $X_i$'s to $Y_i$ by dividing $\sigma_i$. Then $Y \coloneqq 
        \begin{bmatrix}
            Y_1 \\ Y_2 \\ \vdots \\ Y_n
        \end{bmatrix} \sim N(0,I_n)$. Now choose $u$ to be $\begin{bmatrix}
            \sigma_1 \\ \sigma_2 \\ \vdots \\ \sigma_n
        \end{bmatrix}$ and use (a).
        \item By (b) it is easy to see that every entry of $Gu$ follows $N(0,1)$. This shows that $Gu$ is jointly normal. Every pair of coordinates of $Gu$ has covariances 0, and thus $Gu \sim N(0,I_m)$.
    \end{enumerate}
\end{mansol}

\begin{mansol}{3.3.6}
We showed in (c) above that both are $N(0,I_m)$ random vectors. We only need to show that the covariance between each entry of $Gu$ and each entry of $Gv$ is 0. This is because the joint distribution of two normal distributions is still normal, and we just need uncorrelatedness to prove independence (which is true for random vectors as well; see this \href{https://stats.stackexchange.com/questions/375217/definition-of-independence-of-two-random-vectors-and-how-to-show-it-in-the-joint}{stackexchange post} for the proof). Now we need to consider two cases: $\Cov(G_{1,\cdot}u,G_{1,\cdot}v)$ and $\Cov(G_{1,\cdot}u,G_{2,\cdot}v)$.

In the first case, \begin{align*}
\Cov(G_{1,\cdot}u,G_{1,\cdot}v) & = \Cov(G_{11}u_1 + \cdots + G_{1n}u_n, G_{11}v_1 + \cdots + G_{1n}v_n) \\ 
& = \sum_{i=1}^n u_i v_i \Cov(G_{1i},G_{1i}) = \inp{u}{v} = 0;
\end{align*}
and in the second case, \[
\Cov(G_{1,\cdot}u,G_{2,\cdot}v) = \sum_{i=1}^n u_i v_i \Cov(G_{1i},G_{2i}) = 0
\] because all $\inp{G_{1i}}{G_{2i}} = 0$. This finishes the proof.
\end{mansol}

\begin{mansol}{3.3.9}
    We want to prove \[\sum_{i=1}^N \inp{u_i}{x}^2 = A\nm{x}_2^2 \text{ for all }x \in \R^n \iff \sum_{i=1}^N u_i u_i^\trp = A I_n.\]

    Clearly $\sum_{i=1}^N u_i u_i^\trp$, a sum of symmetric matrices is symmetric, and hence the two symmetric matrices $\sum_{i=1}^N u_i u_i^\trp$ and $AI_n$ are equal iff for all $x \in \R^n$, \[
    \Big\la\sum_{i=1}^N u_i u_i^\trp x, x\Big\ra = \inp{A I_n x}{x}.
    \]
    Note that the LHS \[
     \Big\la\sum_{i=1}^N u_i u_i^\trp x, x\Big\ra = \sum_{i=1}^N \inp{u_i^\trp X}{u_i^\trp X} = \sum_{i=1}^N \inp{u_i}{x}^2,
    \]
    while the RHS \[
    \inp{A I_n x}{x} = A\inp{x}{x} = A \nm{x}_2^2.
    \]
    The proof is now finished.
\end{mansol}

\begin{mansol}{3.4.7}
Let $Y \sim \operatorname{Uniform}(\sqrt{n} S^{n-1})$, $U \sim \operatorname{Uniform}([0,1])$ and define $X = U^{1/n}Y \sim \operatorname{Uniform}(B(0;\sqrt{n}))$. Let $X_1 = \inp{X}{e_1}$ and $Y_1 = \inp{Y}{e_1}$. We want to bound $\Pr(\abs{X_1} > t)$ for all $t \geq 0$, by proposition 2.5.2(i). Note that
\[
\Pr(\abs{X_1} > t) = \Pr\bigl(\big\lvert U^{1/n} \big\rvert \abs{Y_1} > t\bigr) \leq \Pr(\abs{Y_1} > t).
\]
We already know from theorem 3.4.6 that the $\Pr(\abs{Y_1} > t)$ satisfies the subgaussian condition 2.5.2(i), and the proof is completed.
\end{mansol}



\newpage
\section{Random matrices}
\begin{mansol}{4.1.6}
By definition, \[\nm{A^\trp A - I_n} = \max_{\nm{x} = 1} \abs{\inp{(A^\trp A - I_n) x}{x}}.\]
Simplification gives $\max_{\nm{x} = 1} \big\lvert\nm{Ax}_2^2 - 1\big\rvert$. Since $1 - \delta \leq \nm{Ax}_2 \leq 1 + \delta$ for $\nm{x} = 1$, we have \[
\delta^2 - 2 \delta \leq \nm{Ax}_2^2 - 1 \leq \delta^2 + 2 \delta
\]
Hence under $\nm{x} = 1$, $\big\lvert\nm{Ax}_2^2 - 1\big\rvert \leq \max\{\delta^2 - 2\delta, \delta^2 + 2 \delta\} \leq 3 \max\{\delta, \delta^2\}$. Thus \[\nm{A^\trp A - I_n} \leq 3 \max\{\delta,\delta^2\}.\]
\end{mansol}

\begin{mansol}{4.2.5}\label{ep-sep-equiv}
\begin{enumerate}[label=(\alph*)]
\item We want to show that two points $x,y \in T$ are separated by $> \epsilon$ iff $\ol{B}(x,\epsilon/2)$ and $\ol{B}(y,\epsilon/2)$ are disjoint.

($\implies$) If the two closed balls are not disjoint, choose $z \in \ol{B}(x,\epsilon/2) \cap \ol{B}(y,\epsilon/2)$. Then \[(\epsilon/2)\cdot 2 \geq \nm{x - z} + \nm{z - y} \geq \nm{x - y} > \epsilon.\] Contradiction.

($\impliedby$) If $\nm{x-y} \leq \epsilon$, consider $\frac{x+y}{2}$ such that $\bigl\lVert x - \frac{x+y}{2} \bigr\rVert = \bigl\lVert y - \frac{x+y}{2} \bigr\rVert = \frac{1}{2}\nm{x-y}$. This shows that $\frac{x+y}{2}$ belongs to both $\ol{B}(x,\epsilon/2)$ and $\ol{B}(y,\epsilon/2)$. \emph{Be aware of the role normed vector space plays here.}

\item We want to give an example where $x,y$ have $\nm{x-y} \leq \epsilon$ while the two closed balls are disjoint. A discrete metric meets the requirement.

Consider the metric space $T = \{0,1,2\}$ with the indicator metric $d(x,y) = \I{x\neq y}$. For $K = \{0,1\}$, we have $\nm{x-y} \leq 1$. However, $\ol{B}(0,1/2) = \{0\}$ and $\ol{B}(0,1/2) = \{1\}$, which are disjoint.

In this example $\mathcal{P}(K,d,1) = 1$, while the largest number of disjoint balls is 2.
\end{enumerate}
\end{mansol}

\begin{mansol}{4.2.9}
The first inequality is obvious. To prove the second one, let $\{\ol{B}(x_i;\epsilon/2)\}$ be an exterior $\epsilon/2$-net with the smallest possible cardinality. Since this is the smallest net, every $\ol{B}(x_i;\epsilon/2)$ intersects $K$. Choose $y_i \in K$ from each of these closed balls. It follows that $\{\ol{B}(y_i;\epsilon)\}$ is a net over $K$. To see this, fix $i$. For every $a \in \ol{B}(x_i;\epsilon/2)$, \[
d(a,y_i) \leq d(a,x_i) + d(x_i,y_i) = \epsilon/2 + \epsilon/2 = \epsilon.
\]
\end{mansol}

\begin{mansol}{4.2.16}
Fix an $x \in K = \{0,1\}^n$. By the definition of hamming distance, $\ol{B}(x;m)$ consists of all binary strings that differ from $x$ by at most $m$. It follows that \[
\abs{\ol{B}(x;m)} = \sum_{k=0}^m \binom{n}{k}.
\]
By the definition of $m$-net, \begin{align*}
\abs{K} = 2^n & \leq \abs{\ol{B}(x;m)} \cdot \mathcal{N}(K,d_H,m) \\
& = \sum_{k=0}^m \binom{n}{k} \cdot \mathcal{N}(K,d_H,m),
\end{align*} as desired.

To show the upper bound to $\mathcal{P}(K,d_H,m)$, for $m$-separated balls we know \[\big \lvert \ol{B}(x;\floor{m/2}) \big \rvert = \sum_{k=0}^{\floor{m/2}}\binom{n}{k}.\] By \ref{ep-sep-equiv} we know $\mathcal{P}$ is at most the number of disjoint $\floor{m/2}$-closed balls. Hence \[\mathcal{P} \leq \frac{2^n}{\sum_{k=0}^{\floor{m/2}}\binom{n}{k}}.\]
\end{mansol}

\begin{mansol}{4.4.2}
    WLOG we normalize $x$, i.e., now $x \in S^{n-1}$. Then we want to show that \[\sup_{y \in \mathcal{N}} \inp{x}{y} \leq 1 \leq \frac{1}{1-\epsilon} \sup_{y\in \mathcal{N}} \inp{x}{y}.\]
    The first inequality follows straight from the Cauchy-Schwarz inequality. To prove \[1-\epsilon \leq \sup_{y \in \mathcal{N}} \inp{x}{y},\] by the definition of $\epsilon$-separated set we know for any $x \in S^{n-1}$, there exists $y$ such that \[
    \nm{x-y}_2 \leq \epsilon.
    \]
    Now from \[\inp{x - y}{x - y} = \nm{x}^2 + \nm{y}^2 -2\inp{x}{y} = 2 - 2\inp{x}{y} \leq \epsilon^2\] we see that \[1 - \epsilon \leq 1 - \frac{\epsilon^2}{2} \leq \inp{x}{y}.\]
\end{mansol}

\begin{mansol}{4.4.3}
    \begin{enumerate}[label=(\alph*)]
        \item The first inequality is obvious. To prove the upper bound, choose $x_0 \in S^{n-1}$ and $y_0 \in S^{m-1}$ such that $\nm{A} = \inp{Ax_0}{y_0}$. Then there exists $x\in\mathcal{N}$ and $y \in \mathcal{M}$ such that $\nm{x - x_0}_2 \leq \epsilon$ and $\nm{y - y_0}_2 \leq \epsilon$.

        Then we have \begin{align*}
            & \inp{Ax}{y} - \inp{Ax_0}{y_0} \\
            ={} & \inp{A(x-x_0)}{y} + \inp{Ax_0}{y-y_0} \\
            \leq{} & \nm{A}\nm{x-x_0}_2\nm{y}_2 + \nm{A}\nm{x_0}_2\nm{y-y_0}_2 \\
            \leq{} & 2 \epsilon \nm{A},
        \end{align*}
        and hence the desired inequality.
        \item Same argument, instead we are using $\nm{A} = \max_{x \in S^{n-1}} \inp{Ax}{x}$.
    \end{enumerate}
\end{mansol}

\begin{mansol}{4.4.6}
    For the nonnegative $\nm{A}$, we have \begin{align*}
        \E \nm{A} & = CK \int_0^\infty \Pr(\nm{A} > CKt)\,dt \\
        & = CK \int_{-\sqrt{m}-\sqrt{n}}^\infty \Pr\bigl(\nm{A} > CK(t+ \sqrt{m} + \sqrt{n})\bigr)\,dt \\
        & \leq CK\Big[\sqrt{m} + \sqrt{n} + 2 \int_0^\infty \exp(-t^2)\,dt\Bigr] \\
        & = CK(\sqrt{m}+\sqrt{n} + \sqrt{\pi}) \leq C'(\sqrt{m} + \sqrt{n})
    \end{align*}
    for some absolute constant $C'$, as desired.
\end{mansol}

\begin{mansol}{4.5.3}[Weyl's inequality]
By the min-max definition, we know \begin{align*}
\lambda_i(S) & = \max_{\dim E = i} \min_{\nm{x} = 1} \inp{Sx}{x},\\
\lambda_i(T) & = \max_{\dim E = i} \min_{\nm{x} = 1} \inp{Tx}{x}.
\end{align*}
Also, using Rayleigh's quotient we know \[
\textstyle\nm{S-T} = \big\lvert\max_{\nm{x} = 1}\inp{(S-T)x}{x}\big\rvert \geq \text{ any } \inp{(S-T)x}{x} \text{ with }\nm{x} = 1.
\]
Hence \begin{align*}
\lambda_i(S) - \nm{S-T} & \leq \max_{\dim E = i} \min_{\nm{x} = 1} [\inp{Sx}{x} - \inp{(S-T)x}{x}] \\
& = \max_{\dim E = i} \min_{\nm{x} = 1} \inp{Tx}{x} = \lambda_i(T).
\end{align*}
Interchange $S$ and $T$ gives $\lambda_i(T) - \nm{T - S} \leq \lambda_i(S)$. These two combined gives $\abs{\lambda_i(S) - \lambda_i(T)} \leq \nm{S-T}$.
\end{mansol}

\newpage

\section{Concentration without independence}
\begin{mansol}{5.1.2 (b)} The gradient should be bounded here.

Because $f$ is differentiable, by the multivariate MVT, we have \begin{align*}
\abs{f(x_2) - f(x_1)} & = \inp{\nabla f(\lambda x_1 + (1-\lambda) x_2)}{x_2 - x_1} \text{ for some }\lambda \in (0,1) \\ 
& \leq \nm{\nabla f(\lambda x_1 + (1-\lambda) x_2)}_2 \nm{x_2 - x_1}_2, \text{ by Cauchy-Schwarz} \\ 
& \leq \sup_{x \in \R^n}\nm{\nabla f(x)}_2 \nm{x_2 - x_1}_2
\end{align*}
Thus $f$ is Lipschitz with the Lipschitz constant $\leq \sup_{x \in \R^n}\nm{\nabla f(x)}_2$.
\end{mansol}

\begin{mansol}{5.1.3}
\begin{enumerate}[label=(\alph*)]
\item skipped.
\item We know by the definition of operator norm that \[\nm{Ax_1 - Ax_2}_2 \leq \nm{A}\nm{x_1 - x_2}_2,\] where the equality is achieved when $x_1 - x_2$ is an eigenvector of $A^*A$ corresponding to the largest eigenvalue $s_1^2 = \nm{A}^2$. Hence $\nm{A}_\mathrm{Lip} = \nm{A}$.
\item There is a very important result from functional analysis that says any two norms on a finite-dimensional normed vector space are equivalent, in the sense that \[\nm{\blank}_a \asymp \nm{\blank}_b.\] The special case $\nm{x} \lesssim \nm{x}_2$ that we will use may be easily verified by breaking $x$ into a linear combination of orthonormal basis vectors.

Since \begin{align*}
\bigl\lvert\nm{x_1} - \nm{x_2}\bigr\rvert & \leq \nm{x_1 - x_2} \\
& \lesssim \nm{x_1 - x_2}_2,
\end{align*}
we know $\nm{\blank}$ is Lipschitz. Because the first inequality above can be achieved when $x_1$ and $x_2$ are colinear in the same direction, the Lipschitz constant is simply the smallest $L$ as described.
\end{enumerate}
\end{mansol}
\begin{mansol}{5.4.6}
It is standard to use the Taylor expansion $e^X$ for this problem. We proceed on an alternative track: write $X = \sum_{i=1}^n \lambda_i u_i u_i^\trp$, then $e^X = \sum_{i=1}^n e^{\lambda_i} u_i u_i^\trp$.

An important theorem states that commuting operators are simultaneously diagonalizable w.r.t.\ the same basis. Hence $e^Y = \sum_{j=1}^n e^{\mu_j}u_j u_j^\trp$. Now consider \[e^X e^Y = \Bigl(\sum_{i=1}^n e^{\lambda_i} u_i u_i^\trp\Bigr)\Bigl(\sum_{j=1}^n e^{\mu_j} u_j u_j^\trp\Bigr).\]

Note that if $i = j$, then since $\inp{u_i}{u_j} = 1$ \[
\bigl(e^{\lambda_i} u_i u_i^\trp\bigr)\bigl(e^{\mu_j} u_j u_j^\trp\bigr) = e^{\lambda_i + \mu_i}u_i u_i^\trp.
\]
Otherwise if $i \neq j$, then by $\inp{u_i}{u_j} = 0$ \[
\bigl(e^{\lambda_i} u_i u_i^\trp\bigr)\bigl(e^{\mu_j} u_j u_j^\trp\bigr) = 0.
\]
Hence $e^Xe^Y = \sum_{i=1}^n e^{\lambda_i + \mu_i}u_i u_i^\trp$. Since $X(X+Y) = (X+Y)X$, and $(X+Y)u_i = Xu_i + Yu_i = (\lambda_i + \mu_i)u_i$, \[\sum_{i=1}^n e^{\lambda_i + \mu_i}u_i u_i^\trp = e^{X+Y}.\]
\end{mansol}

\newpage
\section{Quadratic forms, symmetrization and contraction}
\begin{mansol}{6.3.4}
    \begin{enumerate}[label=(\alph*)]
        \item We know there is an orthonormal basis $e_1,\ldots,e_d$ of $E$. We may extend this list to $e_1,\ldots,e_n$, an orthonormal basis of $\R^n$. We then have \begin{align*}
        \operatorname{dist}(X,E)^2 & = \nm{\inp{X}{e_{d+1}}e_{d+1} + \cdots + \inp{X}{e_n}e_n}_2^2 \\ & = \inp{X}{e_{d+1}}^2 + \cdots + \inp{X}{e_n}^2.
        \end{align*}
    It suffice to prove that for $e \in \R^n$ with norm 1, $\E \inp{X}{e}^2 = 1$. This is easy: 
    \begin{align*}
        \E \inp{X}{e}^2 & = e^\trp \bigl(\E X X^\trp\bigr) e \\ & = \nm{e}_2^2 = 1.
    \end{align*}
    \item Let $\Pi$ be the projection matrix from $\R^n$ to $E$. Consider $B = I - \Pi$, then $\nm{BX}_2$ is just the distance. Recall $\nm{B}_{\mathrm F}$ is the sqrt of sum of squares of singular values of $B$. Since $B$ has $n - d$ singular values 1 and $d$ singular values 0, we now know $\sqrt{n-d} = \nm{B}_{\mathrm F}$.

    Now because $\nm{B} = \text{the largest singular value of }B = 1$, the proof is complete.
    \end{enumerate}
\end{mansol}

\begin{mansol}{6.5.2}
    Just follow the hints. There is a 
\end{mansol}

\newpage
\section{Random processes}
\begin{mansol}{7.1.13}
    First, \[\nm{X_{v_t} - X_{v_s}}_{L^2} = \Big\lVert\sum_{i=t}^s Z_i\Big\rVert_{L^2}.\] Since $\sum_{i=t}^s Z_i \sim N(0,t-s)$, the $\nm{v_t-v_s}_2^2 = t-s$ (where $v_t$ and $v_s$ are chosen in the canonical Gaussian process). This leads us to make the choice that $v_t = (\underbrace{1,\ldots,1}_{t},\underbrace{0,\ldots,0}_{N-t})$.
\end{mansol}

\begin{mansol}{}
    
\end{mansol}

\begin{mansol}{}
    
\end{mansol}

\end{document}